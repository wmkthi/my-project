Platform,Offerer,Course_Name,LO,Text,LO_sequence_id,Specialization,Course
coursera,deeplearning.ai,ai-for-everyone,week-1-introduction,"b'Welcome to AI for everyone. AI is changing the way\nwe work and live and this nontechnical course\nwill teach you how to navigate the rise of AI. Whether you want to\nknow what\'s behind the buzzwords or\nwhether you want to perhaps use AI yourself\neither in a personal context or in a corporation or\nother organization, this course will teach you how. If you want to understand\nhow AI is affecting society, and how you can navigate that, you also learn that\nfrom this course. In this first week, we\'ll start by cutting through\nthe hype and giving you a realistic view of what AI\nreally is. Let\'s get started. You\'ve probably\nseen news articles about how much value\nAI is creating. According to a study by\nMcKinsey Global Institute, AI is estimated to create an additional 13\ntrillion US dollars of value annually by the year 2030. Even though AI is\nalready creating tremendous amounts of value\ninto software industry, a lot of the value\nto be created in a future lies outside\nthe software industry. In sectors such as retail,\ntravel, transportation, automotive, materials,\nmanufacturing and so on. I should have\na hard time thinking of an industry that I don\'t think AI will have a huge impact on in\nthe next several years. My friends and I used a\nchallenge each other to name and industry where we don\'t think\nAI will have a huge impact. My best example was the\nhairdressing industry because we know how to use AI robotics\nto automate hairdressing. But, I once said this on stage and one of my friends who is a robotics professor was\nin the audience that day, and she actually stood up and she looked at me in\nthe eye and she said, ""You know Andrew,\nmost people\'s hairstyles, I couldn\'t get a robot to\ncut their hair that way."" But she looked at me and said, ""Your hairstyle Andrew,\nthat a robot can do."" There is a lot of\nexcitement but also a lot of unnecessary hype about AI. One of the reasons\nfor this is because AI is actually\ntwo separate ideas. Almost all the progress\nwe are seeing in the AI today is artificial\nnarrow intelligence. These are AIs that do one thing such as\na smart speaker or a self-driving car or AI\nto do web search or AI applications in farming\nor in a factory. These types of AI are one trick ponies but when you\nfind the appropriate trick, this can be incredibly valuable. Unfortunately, AI also refers to a second concept of AGI or artificial\ngeneral intelligence. That is the goal to build AI. They can do anything a human\ncan do or maybe even be superintelligence and\ndo even more things than any human can. I\'m seeing tons of\nprogress in ANI, artificial narrow intelligence\nand almost no progress to what AGI or artificial\ngeneral intelligence. Both of these are\nworthy goals and unfortunately the\nrapid progress in ANI which is incredibly valuable, that has caused\npeople to conclude that there\'s a lot of progress\nin AI, which is true. But that has caused people to falsely think that\nthere might be a lot of progress in AGI as well which is leading to\nsome irrational fears about evil clever\nrobots coming over to take over humanity\nanytime now. I think AGI is an exciting goal for\nresearchers to work on, but it\'ll take most for technological breakthroughs\nbefore we get there and it may be decades or hundreds of years or\neven thousands of years away. Given how far away AGI is, I think there is no need\nto unduly worry about it. In this week, you\nwill learn what ANI can do and how to apply\nthem to your problems. Later in this, course\nyou\'ll also see some case studies of how ANI, this one trick ponies\ncan be used to build really valuable applications such as smart speakers and\nself-driving cars. In this week, you will\nlearn why this AI. You may have heard of\nmachine learning and the next video will teach you\nwhat is machine learning. You also learn what is\ndata and what types of data are valuable but also what does the data are not valuable. You learn what it is that makes a company an AI company\nor an AI first company so that perhaps you can start thinking if there\nare ways to improve your company or other\norganizations ability to use AI and importantly, you also learned this week what machine learning\ncan and cannot do. In our society, newspapers as well as research papers\ntend to talk only about the success stories of machine learning and\nAI and we hardly ever see any failure stories because they just aren\'t as\ninteresting to report on. But for you to have\na realistic view of what AI and what machine learning\ncan or cannot do, I think is important that you see examples of both\nso that you can make more accurate judgements\nabout what you may and maybe should not try to\nuse these technologies for. Finally, a lot of\nthe recent rise of, machine learning has been driven through the rise\nof Deep Learning. Sometimes also called\nNeural Networks. In the final two optional\nvideos of this week, you can also see\nan intuitive explanation of deep learning so\nthat you will better understand what they\ncan do particularly for a set of narrow ANI tasks. So, that\'s what you learn this week and by\nthe end of this week, you have a sense of AI technologies and what\nthey can and cannot do. In the second week, you\'ll learn how\nthese AI technologies can be used to build valuable projects. You learn what it\nfeels like to build an AI project as what\nas what you should do to make sure you select projects that are\ntechnically feasible as well as valuable to you or your business or\nother organization. After learning what it\ntakes to build AI projects, in the third week you\'ll learn how to build AI in your company. In particular, if\nyou want to take a few steps toward making\nyour company good at AI, you see the AI transformation\nplaybook and learn how to build AI teams and also\nbuilt complex AI products. Finally, AI is having\na huge impact on society. In a fourth and final week, you\'ll learn about\nhow AI systems can be bias and how to diminish\nor eliminate such biases. You also learn how\nAI is affecting developing economies\nand how AI is affecting jobs and be better\nable to navigate this rise of AI for yourself\nand for your organization. By the end of this four recourse, you\'ll be more\nknowledgeable and better qualified than even the CEOs of most large companies in terms of your understanding of\nAI technology as well as your ability to help\nyourself or your company or other organization\nnavigate the rise of AI as I hope that\nafter this course, you\'ll be in a position to provide leadership to others as well as they navigate\nthese issues. Now, one of\nthe major technologies driving the recent rise of\nAI is Machine Learning. But what is Machine Learning? Let\'s take a look\nin the next video.'",1,0,1
coursera,deeplearning.ai,ai-for-everyone,machine-learning,"b'The rise of AI has\nbeen largely driven by one tool in AI called\nmachine learning. In this video, you\'ll learn what is machine learning,\nso that by the end, you hope we will start\nthinking how machine learning might be applied to your company\nor to your industry. The most commonly used type of machine learning\nis a type of AI that learns A to B, or input to output mappings. This is called\nsupervised learning. Let\'s see some examples. If the input A is an email and the output B one is\nemail spam or not, zero one. Then this is the core piece of AI used to build a spam filter. Or if the input is an audio clip, and the AI\'s job is to\noutput the text transcript, then this is speech recognition. More examples, if you want to input English and have it\noutput a different language, Chinese, Spanish, something else, then this is machine translation. Or the most lucrative form\nof supervised learning, of this type of machine learning maybe be online advertising, where all the large\nonline ad platforms have a piece of AI that inputs\nsome information about an ad, and some information about you, and tries to figure out, will you click on this ad or not? By showing you the ads you\'re\nmost likely to click on, this turns out to\nbe very lucrative. Maybe not the most\ninspiring application, but certainly having\na huge economic impact today. Or if you want to build\na self-driving car, one of the key pieces of AI is in the AI that takes\nas input an image, and some information from their radar, or\nfrom other sensors, and output the position\nof other cars, so your self-driving car\ncan avoid the other cars. Or in manufacturing. I\'ve actually done\na lot of work in manufacturing where you take as input a picture of something\nyou\'ve just manufactured, such as a picture of a cell phone coming\noff the assembly line. This is a picture of a phone, not a picture taken by a phone, and you want to output, is there a scratch,\nor is there a dent, or some other defects on this thing you\'ve\njust manufactured? And this is visual inspection\nwhich is helping manufacturers to reduce or prevent defects in the things\nthat they\'re making. This set of AI called\nsupervised learning, just learns input to output, or A to B mappings. On one hand, input to output, A to B it seems quite limiting. But when you find\na right application scenario, this can be incredibly valuable. Now, the idea of\nsupervised learning has been around for many decades. But it\'s really taken off in the last few years. Why is this? Well, my friends asked\nme, ""Hey Andrew, why is supervised\nlearning taking off now?"" There\'s a picture\nI draw for them. I want to show you\nthis picture now, and you may be able to\ndraw this picture for others that ask you\nthe same question as well. Let\'s say on\nthe horizontal axis you plot the amount of data\nyou have for a task. So, for speech recognition, this might be the amount of audio data and\ntranscripts you have. In lot of industries, the amount of data\nyou have access to has really grown over\nthe last couple of decades. Thanks to the rise\nof the Internet, the rise of computers. A lot of what used to\nbe say pieces of paper, are now instead recorded\non a digital computer. So, we\'ve just been getting\nmore and more data. Now, let\'s say on\nthe vertical axis you plot the performance of an AI system. It turns out that if you use\na traditional AI system, then the performance\nwould grow like this, that as you feed in more data is performance gets a bit better. But beyond a certain point it did not get that much better. So it\'s as if your\nspeech recognition system did not get that\nmuch more accurate, or your online advertising\nsystem didn\'t get that much more accurate that\'s\nshowing the most relevant ads, even as you show the more data. AI has really taken\noff recently due to the rise of neural networks\nand deep learning. I\'ll define these terms more\nprecise in later video, so don\'t worry too much\nabout what it means for now. But with modern AI, with neural networks\nand deep learning, what we saw was that, if you train\na small neural network, then the performance\nlooks like this, where as you feed them more data, performance keeps getting\nbetter for much longer. If you train a even slightly\nlarger neural network, say medium-sized neural net, then the performance\nmay look like that. If you train a very\nlarge neural network, then the performance just keeps on getting\nbetter and better. For applications like\nspeech recognition, online advertising,\nbuilding self-driving car, where having a high-performance, highly accurate, say speech recognition system\nis important, enable these AI systems\nget much better, and make speech\nrecognition products much more acceptable to users, much more valuable to\ncompanies and to users. Now, a few couple of\nimplications of this figure. If you want the best possible\nlevels of performance, your performance to be up here, to hit this level of performance, then you need two things: One is, it really helps to\nhave a lot of data. So that\'s why sometimes\nyou hear about big data. Having more data\nalmost always helps. The second thing is,\nyou want to be able to train a very large\nneural network. So, the rise of fast computers,\nincluding Moore\'s law, but also the rise of specialized processors such as graphics processing\nunits or GPUs, which you\'ll hear more\nabout in a later video, has enabled many companies, not just a giant tech companies, but many many other companies to be able to train\nlarge neural nets on a large enough amount\nof data in order to get very good performance and\ndrive business value. The most important idea in AI\nhas been machine learning, has basically\nsupervised learning, which means A to B, or input to output mappings. What enables it to work\nreally well is data. In the next video, let\'s take a look at what is the data and what data\nyou might already have? And how to think about\nfeeding this into AI systems. Let\'s go on to the next video.'",2,0,1
coursera,deeplearning.ai,ai-for-everyone,what-is-data,"b'You may have heard that data is really important for\nbuilding AI systems. But, what is data really?\nLet\'s take a look. Let\'s look at an\nexample of a table of data which we also\ncall a dataset. If you\'re trying to\nfigure out how to price houses that you\'re\ntrying to buy or sell, you might collect\na dataset like this, and this can be\njust a spreadsheet, like a MS excel\nspreadsheet of data where one column is\nthe size of the house, say in square feet\nor square meters, and the second column is\nthe price of the house. So, if you\'re trying to build a AI system or Machine\nLearning system to help you set prices for houses or figure out if a\nhouse is priced appropriately, you might decide that\nthe size of the house is A and the price of\nthe house is B, and have an AI system learn this input to output\nor A to B mapping. Now, rather than just pricing a house\nbased on their size, you might say, ""Well, let\'s also collect data on the number of bedrooms\nof this house."" In that case, A can be both\nof these first two columns, and B can be just the\nprice of the house. So, given that table of data, given the dataset, it\'s\nactually up to you, up to your business use case to decide what is A and what is B. Data is often unique\nto your business, and this is an example\nof a dataset that a rural state agency\nmight have that they tried to help price houses. It\'s up to you to decide\nwhat is A and what is B, and how to choose\nthese definitions of A and B to make it valuable\nfor your business. As another example, if\nyou have a certain budget and you want to decide what is the size of house\nyou can afford, then you might decide that the input A is how\nmuch does someone spend and B is just the size\nof the house in square feet, and that would be\na totally different choice of A and B that tells you, given a certain budget, what\'s the size of the house you should be maybe looking at. Here\'s another\nexample of a dataset. Let\'s say that you want to build a AI system to recognize\ncats in pictures. I\'m not sure why you\nmight want to do that, but maybe the fun mobile app, and you want to tag\nall the pictures of cats. So, you might collect\na dataset where the input A is a set of different images and the\noutput B are labels that says, ""First picture is a cat,\nthat\'s not a cat. That\'s a cat, that\'s\nnot a cat"" and have an AI input a\npicture A and output B is it the cats or\nnot so you can tag all the cat pictures on your photo feed or\nyour mobile app. In Machine Learning tradition, there\'s actually a lot of\ncats in Machine Learning. I think some of\nthis started when I was leaving the\nGoogle Brain team and we published the results with somewhat\ninfamous Google cat, where an AI system\nlearn to detect cats from watching\nYouTube videos. But since then, there\'s\nbeen a tradition of using cats as a running\nexample when talking about Machine Learning\nwith apologies to all the dog lovers out\nthere. I love dogs too. So, data is important. But how do you get data? How do you acquire data? Well, one way to get\ndata is manual labeling. For example, you might\ncollect a set of pictures like these over here, and then you might either\nyourself or have someone else go through these pictures\nand label each of them. So, the first one is a cat, second one is not a cat, third one is a cat,\nfourth one is not a cat. By manually labeling\neach of these images, you now have a dataset for\nbuilding a cat detector. To do that, you actually need\nmore than four pictures. You might need hundreds\nof thousands of pictures but manual labeling is a tried and true way of getting a dataset where you\nhave both A and B. Another way to get\na dataset is from observing user behaviors or\nother types of behaviors. So, for example,\nlet\'s say you run a website that sells\nthings online. So, an e-commerce or\nan electronic commerce website where you offer things to users at\ndifferent prices, and you can just observe if\nthey buy your product or not. So, just through the act of either buying or not\nbuying your product, you may be able to collected\na data set like this, where you can store the user ID, the time the user\nvisited your website, the price you offer\nthe product to the users as well as whether or\nnot they purchased it. So, just by using your website, users can generate\nthis data from you. This was an example of\nobserving user behaviors. We can also observe behaviors of other things\nsuch as machines. If you run a large machine in a factory and\nyou want to predict if a machine is about to\nfail or have a fault, then just by observing\nthe behavior of a machine, you can then record\na dataset like this. There\'s a machine ID, there\'s a temperature of the machine, there\'s a pressure\nwithin the machine, and then did the machine\nfail or not. If your application is\nprevent the maintenance, say you want to figure out if\na machine is about to fail, then you could for example, choose this as the input A and choose that as\nthe output B to try to figure out if a machine is about to fail in\nwhich case you might do preventative maintenance\non the machine. The third and very common way\nof acquiring data is to download it from a website or to get\nit from a partner. Thanks to the open internet,\nthere\'s just so many, there\'s as that you\ncan download for free, ranging from computer vision\nor image datasets, to self-driving car datasets, to speech recognition datasets, to medical imaging data\nsets to many many more. So, if your application\nneeds a type of data, you just download off the web keeping in mind\nlicensing and copyright, then that could be a great way to get started on the application. Finally, if you\'re\nworking with a partner, say you\'re working\nwith a factory, then they may already have collected a big\ndataset, machines, and temperatures, and pressure\ninto the machines fail not that they could give to you. Data is important, but\nthere\'s also little bit over-hyped and\nsometimes misused. Let me just describe\nto you two of the most common misuses or the bad ways of\nthinking about data. When I speak of seals\nof large companies, a few of them have\neven said to me, ""Hey Andrew, give me three years\nto build up my IT team, we\'re collecting so much data. Then after three years, I\'ll\nhave this perfect dataset, and then we\'ll do AI then."" It turns out that\'s\na really bad strategy. Instead, what I recommend\nto every company, is once you\'ve started\ncollecting some data, go ahead and start showing it or feeding it to an AI team. Because often,\nthe AI team can give feedback to your IT team on what types of data to\ncollect and what types of IT infrastructure\nto keep on building. For example, maybe\nan AI team can look at your factory data and\nsay, ""Hey. You know what? If you can collect data from this big manufacturing machine, not just once every ten minutes, but instead once\nevery one minute, then we could do\na much better job building a preventative maintenance\nsystems for you."" So, there\'s often\nthis interplay of this back and forth between\nIT and AI teams, and my advise is\nusually try to get feedback from AI earlier, because it can help you guide the development of\nyour IT infrastructure. Second, misuse of data. Unfortunately, I\'ve\nseen some CEOs read about the importance\nof the trend in use, and then say, ""Hey, I have so much data. Surely, an AI team can\nmake it valuable."" Unfortunately, this\ndoesn\'t always work out. More data is usually\nbetter than less data, but I wouldn\'t take it for\ngranted that just because you have many terabytes\nor gigabytes of data, that an AI team can actually\nmake that valuable. So, my advice is don\'t throw data in a AI team and assume\nit will be valuable. In fact, in one extreme case, I saw one company go and acquire a whole string of\nother companies in medicine, on the thesis, on the hypothesis that their data\nwould be very valuable. Now, a couple years later, as far as I know\nthe engineers have not yet figured out how to take all this data and actually\ncreate value out of it. So, sometimes it works\nand sometimes it doesn\'t. But, they will not over-invest in just acquiring\ndata for the sake of data until unless you\'re also getting an AI team\nto take a look at it. Because, they can help\nguide you to think through what is the data that is\nactually the most valuable. Finally, data is messy. You may have heard the phrase\ngarbage in garbage out, and if you have bad data, then the AI will learn\ninaccurate things. Here are some examples\nof data problems. Let\'s say you have this data\nsets of size of houses, number of bedrooms,\nand the price. You can have incorrect labels\nor just incorrect data. For example, this house is\nprobably not going to sell for $0.1 just for one dollar. Or, data can also have\nmissing values such as we have here a whole bunch\nof unknown values. So, your AI team\nwill need to figure out how to clean up the data or how to deal with\nthese incorrect labels and all missing values. There are also multiple\ntypes of data. For example, sometimes you hear about images, audio, and text. These are types of data\nthat humans find it very easy to interpret.\nThere\'s a term for this. This is called unstructured data, and there\'s a certain types of AI techniques that could work with images to recognize cats or audios to recognize\nspeech or texts, or understand that email is spam. Then, there are also datasets\nlike the one on the right. This is an example\nof structured data. That basically means\ndata that lives in a giant spreadsheet, and the techniques for dealing with unstructured data are little bit different than\nthe techniques for dealing with structured data. But AI techniques can work very well for both of\nthese types of data, unstructured data\nand structured data. In this video, you\nlearned what is data and you also saw how\nnot to misuse data, for example by over-investing in an IT infrastructure in the hope that it will be useful\nfor AI in the future, but we\'re actually checking\nthat they\'re really will be useful for the AI applications\nyou want to build. Finally, you saw data is messy. But a good AI team would be the help you deal with\nall of these problems. Now, AI has a\ncomplicated terminology when people throw\naround terms like AI, Machine Learning, Data Science. What I want to do in\nthe next video is share with you what these terms\nactually mean, so that you\'d be able\nto confidently and accurately talk about\nthese concepts with others. Let\'s go on to the next video.'",3,0,1
coursera,deeplearning.ai,ai-for-everyone,the-terminology-of-ai,"b'You might have heard\nterminology from AI, such as machine learning or data science or neural\nnetworks or deep learning. What do these terms mean? In this video, you\'ll see what is this terminology of the most\nimportant concepts of AI, so that you will speak with\nothers about it and start thinking how these things\ncould apply in your business. Let\'s get started.\nLet\'s say you have a housing dataset like this\nwith the size of the house, number of bedrooms,\nnumber of bathrooms, whether the house is newly\nrenovated as was the price. If you want to build a mobile app to help\npeople price houses, so this would be the input A, and this would be the output B. Then, this would be\na machine-learning system, and particular would be one of those machine\nlearning systems that learns inputs to outputs, or A to B mappings. So, machine learning\noften results in a running AI system. So, it\'s a piece of software\nthat anytime of day, anytime of night you\ncan automatically input A these properties of\nhouse and output B. So, if you have\nan AI system running, serving dozens or\nhundreds of thousands of millions of users, that\'s usually\na machine-learning system. In contrast, here\'s something\nelse you might want to do, which is to have a team analyze your dataset in order\nto gain insights. So, a team might come up\nwith a conclusion like, ""Hey, did you know if you have two houses of a similar size, they\'ve a similar square footage, if the house has three bedrooms, then they cost a lot more than\nthe house of two bedrooms, even if the square for\nthis is the same."" Or, ""Did you know that newly renovated homes\nhave a 15% premium, and this can help you\nmake decisions such as, given a similar square footage, do you want to build\na two bedroom or three bedroom size in\norder to maximize value? "" Or, ""Is it worth an investment to renovate\na home in the hope that the renovation\nincreases the price you can sell a house for?"" So, these would be examples\nof data science projects, where the output of a\ndata science project is a set of insights that can help you make\nbusiness decisions, such as what type of house to build or whether to\ninvest in renovation. The boundaries between\nthese two terms, machine learning and data science are actually little bit buzzy, and these terms are not used consistently even\nin industry today. But what I\'m giving here is maybe the most commonly used\ndefinitions of these terms, but you will not find universal adherence\nto these definitions. To formalize these two\nnotions a bit more, machine learning is\nthe field of study that gives computers\nthe ability to learn without being\nexplicitly programmed. This is a definition by\nArthur Samuel many decades ago. Arthur Samuel was one of the pioneers of machine learning, who was famous for building\na checkers playing program. They could play checkers, even better than he himself, the inventor could play the game. So, a machine learning\nproject will often results in a piece of\nsoftware that runs, that outputs B given A. In contrast, data science is the size of extracting knowledge\nand insights from data. So, the output of a data science project\nis often a slide deck, the PowerPoint presentation that summarizes conclusions\nfor executives to take business actions or that\nsummarizes conclusions for a product team to decide\nhow to improve a website. Let me give an example of\nmachine learning versus data science in the online\nadvertising industry. Today, to launch our platforms, all have a piece\nof AI that quickly tells them what\'s the ad you\nare most likely to click on. So, that\'s a machine\nlearning system. This turns out to be incredibly lucrative AI system to inputs enrich about you and about the ad and outputs where you\nclick on this or not. These systems are running 24-7. These are machine\nlearning systems that drive our gravity\nfor these companies, such as a piece of\nsoftware that runs. In contrast, I have also done data science projects in\nthe online advertising industry. If analyzing data tells\nyou, for example, that the travel industry is\nnot buying a lot of ads, but if you send more salespeople to sell ads to travel companies, you could convince them\nto use more advertising, then that would be an example of a data science project and the data science conclusion\nthe results and the executives deciding\nto ask a sales team to spend more time reaching\nout to the travel industry. So, even in one company, you may have different\nmachine learning and data science projects, both of which can be\nincredibly valuable. You have also heard\nof deep learning. So, what is deep learning? Let\'s say you want to\npredict housing prices, you want to price houses. So, you will have an input that tells you\nthe size of the house, number of bedrooms,\nnumber of bathrooms and whether it\'s newly renovated. One of the most effective ways\nto price houses, given this input A\nwould be to feed it to this thing here in order to\nhave it output the price. This big thing in the middle\nis called a neural network, and sometimes we also called an artificial neural network. That\'s to distinguish it from the neural network\nthat is in your brain. So, the human brain is\nmade up of neurons. So, when we say artificial\nneural network, that\'s just to emphasize that this is not the biological brain, but this is a piece of software. What a neural network does, or an artificial\nneural network does is takes this input A, which is all of\nthese four things, and then output B, which is the estimated\nprice of the house. Now, in a later optional\nvideo this week, I\'ll show you more, what this artificial\nneural network really is. But all of human\ncognition is made up of neurons in your brain\npassing electrical impulses, passing little\nmessages each other. When we draw a picture of an\nartificial neural network, there\'s a very loose\nanalogy to the brain. These little circles are\ncalled artificial neurons, or just neurons for short. That also passes\nneurons to each other. This big artificial\nneural network is just a big mathematical equation that tells it given the inputs A, how do you compute the price B. In case it seems like there\na lot of details here, don\'t worry about it. We\'ll talk more about\nthese details later. But the key takeaways are\nthat a neural network is a very effective technique for learning A to B or\ninput-output mappings. Today, the terms\nneural network and deep learning are used\nalmost interchangeably, they mean essentially\nthe same thing. Many decades ago, this type of software was called\na neural network. But in recent years, we found that deep learning was just a much better\nsounding brand, and so that for better or worse is a term that\'s been\ntaken off recently. So, what do neural networks or artificial neural networks\nhave to do with the brain? It turns out almost nothing. Neural networks were originally\ninspired by the brain, but the details of how\nthey work are almost completely unrelated to how\nbiological brains work. So, I choose very courses\ntoday about making any analogies between\nartificial neural networks and the biological brain, even though there was\nsome loose inspiration there. So, AI has many different tools. In this video, you learned about what are machine learning\nand data science, and also what is deep learning, and what\'s a neural network. You might also hear in the media other buzzwords like\nunsupervised learning, reinforcement learning,\ngraphical models, planning, knowledge\ngraph, and so on. You don\'t need to know what all of these other terms mean, but these are just\nother tools to getting AI systems to make computers\nact intelligently. I\'ll try to give you\na sense of what some of these terms mean in\nlater videos as well. But the most important tools\nthat I hope you know about are machine learning and data science as well as deep learning and\nneural networks, which are a very powerful way\nto do machine learning, and sometimes data science. If we were to draw a Venn diagram showing how all these concepts put together, this is what it may look like. AI is this huge set of tools for making computers\nbehave intelligently. Off AI, the biggest subset is prairie tools from\nmachine learning, but AI does have other tools\nthan machine learning, such as some of these buzzwords, are listed at the bottom. The part of machine learning that\'s most important these days is neural networks\nor deep learning, which is a very powerful set\nof tools for carrying out supervised learning or A to B mappings as well as\nsome other things. But there are also other\nmachine learning tools that are not just deep learning tools. So, how does data science\nfit into this picture? There is inconsistency in\nhow the terminology is used. Some people will tell you\ndata science is a subset of AI. Some people will tell you AI\nis a subset of data science. So, it depends on who you ask. But I would say that\ndata science is maybe a cross-cutting subset of all of these tools that uses many tools from AI machine learning\nand deep learning, but has some other separate\ntools as well that solves a very set of important problems in driving business insights. In this video, you saw\nwhat is machine learning, what is data science, and what is deep learning and\nneural networks. I hope this gives you a sense of the most common and important\nterminology using AI, and you can start\nthinking about how these things might\napply to your company. Now, what does it mean for\na company to be good at AI? Let\'s talk about that\nin the next video.'",4,0,1
coursera,deeplearning.ai,ai-for-everyone,what-makes-an-ai-company,"b'What makes a company good at AI? Perhaps even more importantly, what will it take for your company to become\ngreat at using AI? I had previously led\nthe Google brain team, and Baidu\'s AI group, which I respectively\nhelped Google and Baidu become great AI companies. So, what can you do\nfor your company? This is the lesson I\nhad learned to washing the rise of the Internet\nthat I think will be relevant to how all of us navigate the rise of\nAI. Let\'s take a look. A lesson we learned from the rise of\nthe Internet was that, if you take your\nfavorite shopping mall. So, my wife and I\nsometimes shop at Stanford shopping center and you build a website\nfor the Shopping mall. Maybe sell things on the website, that by itself does not turn the shopping mall into\nan internet company. In fact, a few years\nago I was speaking with the CEO of a large retail\ncompany who said to me, ""Hey Andrew, I have a website, I sell things in the website."" Amazon has a website, Amazon sells things on\nwebsite is the same thing. But of course it wasn\'t, in the shopping mall\nwith a website isn\'t the same thing as a first-class\ninternet company. So, what is it that defines\nan internet company if it isn\'t just\nwhether or not you sell things on the website? I think an Internet\ncompany is a company that does the thing that\ninternet let you do really well. For example, we engage\nand pervasive AB testing. Meaning we routinely threw up two different versions of website and see which one works\nbetter because we can. So, we learn much faster. Whereas in a traditional\nshopping mall, very difficult to have two shopping malls in\ntwo parallel universes and you can only\nmaybe change things around every quarter\nor every six months. Internet company is since\na very short iteration times. You can ship\na new product every week or maybe even every\nday because you can whereas a shopping mall\ncan be redesigned and we are protected only\nevery several months. Internet companies also tend to push decision-making down from the CEO to the engineers and to other specialized rules such\nthat the product managers. This is in contrast to\na traditional shopping mall. We can maybe have\nthe CEO just decide all the key decisions and then just everyone does\nwhat the CEO says. It turns out that traditional\nmodel doesn\'t work in the internet era because\nonly the engineers and other specialized roles like product managers\nknow enough about the technology and\nthe product and the users to make\ngreat decisions. So, these are some of the things that internet companies do in order to make sure they\ndo the things that the internet doesn\'t\ndo really well. This is a lesson we learned\nfrom the internet era. How about the AI era? I think that today, you can take any\ncompany and have it use a few neural networks or few deep learning algorithms. That by itself does not turn the accompany into an AI company. Instead, what makes\na great AI company, sometimes an AI first company is, are you doing the things that\nAI lets you do really well? For example, AI\ncompanies are very good at strategic\ndata acquisition. This is why many of the large consumer tech companies\nmay have three products that do not monetize\nand it allows them to acquire data that they\ncan monetize elsewhere. Serve less strategy teams where we would\ndeliberately launch products that do not make any money just for the sake\nof data acquisition. Thinking through how to get data is a key part of\nthe great AI companies. AI companies sends\na unified data warehouses. If you have 50\ndifferent databases or 50 different data\nwarehouses under the control of 50\ndifferent Vice-Presidents, then there will be impossible\nfor an engineer to get the data into one place\nso that they can connect the dots and\nspot the patterns. So, many great AI companies have preemptively invested\nin bringing the data together into\nsingle data warehouse to increase the odds that\nthe teams can connect the dots. Subject of course to\nprivacy guarantees and also to data regulations\nsuch as GDPR in Europe. AI companies are very good at spotting automation\nopportunities. We\'re very good at saying, Oh! Let\'s insert the supervised\nlearning algorithm and have an ATP mapping here so that we don\'t\nhave to have people do these tasks instead\nwe can automate It. AI companies also have many\nnew roles such as the MLE or Machine Learning Engineer\nand new ways of dividing up tasks among\ndifferent members of a team. So, for a company to\nbecome good at AI means, architecting for company\nto do the things that AI makes it possible\nto do really well. Now, for a company\nthat become good at AI does require a process. In fact, 10 years ago, Google and Baidu as\nwell as companies like Facebook and Microsoft\nthat I was not a part of, we\'re not great AI companies\nthe way they are today. So, how can a company\nbecome good at AI? It turns out that\nbecoming good at AI is not a mysterious\nmagical process. Instead there is\na systematic process through which many companies, almost any big company\ncan become good at AI. This is the five-step AI\ntransformation playbook that I recommend to\ncompanies that want to become effective at using AI. I\'ll give a brief overview\nof the playbook here and they\'re going to\ndetail in a later week. Step one is to execute pilot\nprojects to gain momentum. So, just to a few small projects to get a better sense of\nwhat AI can or cannot do and get a better sense of what doing an AI project feels like. This you could do\nin house or you can also do with an outsource team. But eventually, you\nthen need to do step two which is the building in house AI team and provide\nbroad AI training, not just to the engineers\nbut also to the managers, division leaders and executives and how they think about AI. After doing this or\nas you\'re doing this, you have a better sense\nof what AI is and then is important for\nmany companies to develop an AI strategy. Finally, to align internal\nand external communications so that all your stakeholders\nfrom employees, customers and investors\nare aligns with how your company is navigating\nthe rise of AI. AI has created\ntremendous value in the software industry and\nwill continue to do so. It will also create tremendous value outside\nthe software industry. If you can help your\ncompany become good at AI, I hope you can play\na leading role in creating a lot of this value. It is video you saw\nwhat is it that makes a company a good AI company and also briefly the\nAI transformation playbook which I\'ll go\ninto much greater detail on in a later week\nas a road-map for helping companies\nbecome great at AI. If you\'re interested,\nthere is also published online an AI transformation\nplaybook that goes into these five steps\nin greater detail for you see more of these in\nthe later weeks as well. Now, one of the challenges\nof doing AI projects such as the pilot projects in step one is understanding what AI\ncan and cannot do. In the next video, I want\nto show you and give you some examples of what\nAI can and cannot do, to help you better\nselect projects AI that there may be effective\nfor your company. That\'s gone to the next video.'",5,0,1
coursera,deeplearning.ai,ai-for-everyone,what-machine-learning-can-and-cannot-do,"b'In this video and the next video, I hope to help you\ndevelop intuition about what AI can and cannot do. In practice, before I commit\nto a specific AI project, I\'ll usually have either\nmyself or engineers do technical diligence\non the project to make sure that it is feasible. This means: looking at\nthe data, look at the input, and output A and B, and\njust thinking through if this is something\nAI can really do. What I\'ve seen unfortunately\nis that some CEOs can have an inflated expectation\nof AI and can ask engineers to do things that\ntoday\'s AI just cannot do. One of the challenges\nis that the media, as well as the\nacademic literature, tends to only report on positive results or\nsuccess stories using AI, and we see a string of success stories and\nno failure stories, people sometimes think\nAI can do everything. Unfortunately, that\'s\njust not true. So, what I want to do in\nthis and in the next video, is to show you a few\nexamples of what today\'s AI technology can do, but also what it cannot do, and I hope that\nthis will help you, hone your intuition\nabout what might be more or less promising projects to select for your company. Previously, you saw this list of AI applications from spam filtering to\nspeech recognition, to machine translation,\nand so on. One imperfect rule of\nthumb you can use to decide what supervised\nlearning may or may not be able to do is that, pretty much anything you could do with a second of thought, we can probably now or soon automate using\nsupervised learning, using this input-output mapping. So for example, in order to determine the position\nof other cars, that\'s something that you can\ndo with less than a second. In order to tell if\na phone is scratched, you can look at it and you can tell in less than a second. In order to understand or at least transcribe\nwhat was said, it doesn\'t take that\nmany seconds of thought. While this is an\nimperfect rule of thumb, it maybe gives you a way\nto quickly think of some examples of tasks\nthat AI systems can do. Whereas in contrast, something that AI today cannot do would be: to analyze a market and\nwrite a 50 page report, a human cannot write a 50 page mark of\nanalysis report in a second, and it\'s very difficult,\nat least I don\'t know. I don\'t think\nany team in the world today knows how to get an AI system to do\nmarket research and run an extended\nmarket report either. I\'ve found out one of\nthe best ways to hone intuition is to look\nat concrete examples. So, let\'s take a look\nat a specific example, relating to customer\nsupport automation. Let\'s see a random website\nthat sells things, so an e-commerce company, and you have a customer\nsupport division that gets an email like this, ""The toy arrived two days late, so I wasn\'t able to give it\nto my niece for her birthday. Can I return it?"" If what\nyou want is an AI system that looks at this and decides\nthis is a refund request, so let me route it to\nmy refund department, then I will say, you\nhave a good chance of building an AI system to do that. The AI system would\ntake as input, the customer text, what\nthe customer emails you, and it would output, is this a refund requests or\nis this a shipping problem, or is it the other request, in order to route this email to the most appropriate parts of your customer support center. So, the input A is the text and the output B is one of\nthese three outcomes, is it a refund or\na shipping problem, or shipping query, or is\nit a different requests. So, this is something\nthat AI today can do. Here\'s something that AI today cannot do which is if you want the AI to input an email\nand automatically generate, it responds like, ""Oh,\nsorry to hear that. I hope you\'re niece\nhad a good birthday. Yes, we can help\nwith, and so on."" So, for an AI to output a complicated piece of text\nlike this today is very difficult by today\'s standards\nof AI and in fact to even empathize about\nthe birthday of your niece, that is very difficult to do for every single possible type\nof email you might receive. Now, what would happen\nif you were to use a machine learning tool like a deep learning algorithm\nto try to do this anyway. So, let\'s say you tried to get an AI system to input\nthe user\'s email, and output a two to\nthe three paragraph, empathetic and\nappropriate response. Let\'s say that you have\na modest-sized dataset like a 1,000 examples of user emails\nand appropriate responses. It turns out if you run an AI system on\nthis type of data, on a small dataset\nlike 1,000 examples, this may be the\nperformance you get, which is if a user emails, ""My box was damaged,""\nthey\'ll say, ""Thank you for your\nemail,"" and it says, ""Where do I write a\nreview?"", ""Thank you email."" ""What\'s the return policy?"",\n""Thank you for your email."" But the problem with\nbuilding this type of AI is that with just\na 1,000 examples, there\'s just not enough data\nfor an AI system to learn how to write\nto the three paragraph, appropriate and\nempathetic responses. So, you may end up just generating the same very\nsimple response like, ""Thank you for your\nemail,"" no matter what the customer is sending you. Another thing that\ncould go wrong, another way for an AI system\nto fail is if it generates gibberish such as: ""When is my box\narriving,"" and it says, ""Thank, yes, now\nyour,"" gibberish. This is a hard enough\nproblem that even with 10,000 or a 100,000\nemail examples, I don\'t know if that would be enough data for an AI system\nto do this well. The rules for what\nAI can and cannot do are not hardened first and I usually end up having to ask engineering teams to sometimes\nspend a few weeks doing deep technical diligence to decide for myself if\na project is feasible. But to hone your\nintuitions to help you quickly filter feasible\nor not feasible projects, here are a couple of other rules of thumb about what makes a machine learning problem easier or more likely\nto be feasible. One, learning a simple concept is more likely to be feasible. Well, what does\na simple concept mean? There\'s no formal\ndefinition of that but it is something\nthat takes you less than a second\nof mental thought or a very small number of\nseconds of mental thought to come up with a conclusion\nthen that would lean to whether it\nbeing a simple concept. So, you\'re looking outside the window of\na self-driving car to spot the other cars that would be\na relatively simple concept. Whereas how to write\nan empathetic response, so a complicated user complaints, that would be less\nof a simple concept. Second, a machine learning\nproblem is more likely to be feasible if you have\nlots of data available. Here, our data means both the\ninput A and the output B, that you want the AI system\nto have in your A to B, input to output mapping. So for example, in the\ncustomer support application, the input A would be examples of emails from customers and B could be labeling each of\nthese customer emails as to whether it\'s a refund\nrequests or a shipping query, or some other problem, one of three outcomes. Then if you have thousands\nof emails with both A and B, then the odds of you building a machine learning system to do that would be pretty good. AI is the new electricity and it\'s transforming every industry, but it\'s also not magic and it can\'t do everything\nunder the sun. I hope that this video\nstarted to help you hone your intuitions about\nwhat it can and cannot do, and increase the odds\nof your selecting feasible and valuable projects for maybe your teams\nto try working on. In order to help you continue\ndeveloping your intuition, I would like to show you more examples of what\nAI can and cannot do. Let\'s go into the next video.'",6,0,1
coursera,deeplearning.ai,ai-for-everyone,more-examples-of-what-machine-learning-can-and-cannot-do,"b""One of the challenges of becoming good at recognizing\nwhat AI can and cannot do is that\nit does take seeing a few examples of concrete\nsuccesses and failures of AI. If you work on an average of say, one new AI project a year, then to see three examples\nwould take you three years of work experience and that's just a long time. What I hope to do, both in the previous video\nand in this video is to quickly show you a few examples of\nAI successes and failures, or what it can and cannot do so that in\na much shorter time, you can see multiple\nconcrete examples to help hone your intuition and\nselect valuable projects. So, let's take a look\nat a few more examples. Let's say you're building\na self-driving car, here's something that\nAI can do pretty well, which is to take a picture\nof what's in front of your car and maybe\njust using a camera, maybe using other senses as\nwell such as radar or lidar. Then to figure out, what is the position, or where are the other cars. So, this would be an AI\nwhere the input A, is a picture of what's\nin front of your car, or maybe both a\npicture as well as radar and other sensor readings. The output B is, where are the other cars? Today, the self-driving\ncar industry has figured out how to collect enough data and has\npretty good algorithms for doing this reasonably well. So, that's what\nthe AI today can do. Here's an example of something\nthat today's AI cannot do, or at least would be very\ndifficult using today's AI, which is to input\na picture and output the intention of whatever the human is trying to\ngesture at your car. So, here's a construction worker holding out a hand to\nask your car to stop. Here's a hitchhiker trying\nto wave a car over. Here is a bicyclist raising the left-hand to indicate\nthat they want to turn left. So, if you were to try to build a system to learn\nthe A to B mapping, where the input A is a short video of our human\ngesturing at your car, and the output B is, what's the intention or\nwhat does this person want, that today is very\ndifficult to do. Part of the problem is\nthat the number of ways people gesture at you\nis very, very large. Imagine all the hand\ngestures someone could conceivably use asking you\nto slow down or go, or stop. The number of ways\nthat people could gesture at you is just\nvery, very large. So, it's difficult to\ncollect enough data from enough thousands or\ntens of thousands of different people\ngesturing at you, and all of these\ndifferent ways to capture the richness\nof human gestures. So, learning from a video\nto what this person wants, it's actually a somewhat\ncomplicated concept. In fact, even people have\na hard time figuring out sometimes what someone\nwaving at your car wants. Then second, because this is a safety critical application, you would want an AI that is extremely accurate in\nterms of figuring out, does a construction worker\nwant you to stop, or does he or she\nwants you to go? And that makes it harder\nfor an AI system as well. So, today if you\ncollect just say, 10,000 pictures of other cars, many teams would build an\nAI system that at least has a basic capability\nat detecting other cars. In contrast, even if you collect pictures or\nvideos of 10,000 people, it's quite hard to track down 10,000 people waving at your car. Even with that data set, I think it's quite\nhard today to build an AI system to recognize\nhumans intentions from their gestures at\nthe very high level of accuracy needed in order to drive\nsafely around these people. So, that's why today, many self-driving car teams have some components for\ndetecting other cars, and they do rely on\nthat technology to drive safely. But very few self-driving\ncar teams are trying to count on the\nAI system to recognize a huge diversity of\nhuman gestures and counting just on that to\ndrive safely around people. Let's look at one more example. Say you want to build\nan AI system to look at X-ray images and\ndiagnose pneumonia. So, all of these\nare chest X-rays. So, the input A could\nbe the X-ray image and the output B can\nbe the diagnosis. Does this patient have\npneumonia or not? So, that's something\nthat AI can do. Something that AI cannot do would be to diagnose pneumonia from 10 images of\na medical textbook chapter explaining pneumonia. A human can look at\na small set of images, maybe just a few dozen images, and reads a few paragraphs from medical textbook and\nstart to get a sense. But actually don't know, given a medical textbook, what is A and what is B? Or how to really pose this as an AI problems like know how to write a piece\nof software to solve, if all you have is just 10\nimages and a few paragraphs of text that explain what pneumonia in a chest X-ray looks like. Whereas a young medical doctor might learn quite well reading a medical textbook\nat just looking at maybe dozens of images. In contrast, an AI system isn't really able\nto do that today. To summarize, here are some of the strengths and weaknesses\nof machine learning. Machine learning\ntends to work well when you're trying to\nlearn a simple concept, such as something that you\ncould do with less than a second of mental thought, and when there's lots\nof data available. Machine learning tends to work poorly when you're\ntrying to learn a complex concept from\nsmall amounts of data. A second underappreciated\nweakness of AI is that it tends to\ndo poorly when it's asked to perform on\nnew types of data that's different than the data it\nhas seen in your data set. Let me explain with an example. Say you built a supervised\nlearning system that uses A to B to learn to diagnose pneumonia\nfrom images like these. These are well pretty high\nquality chest X-ray images. But now, let's say you\ntake this AI system and apply it at a different hospital or different medical center, where maybe the X-ray technician\nsomehow strangely had the patients always\nlie at an angle or sometimes there\nare these defects. Not sure if you can see the\nlost structures in the image. These low other objects lying\non top of the patients. If the AI system has learned from data like\nthat on your left, maybe taken from\na high-quality medical center, and you take this\nAI system and apply it to a different medical center that generates images like\nthose on the right, then it's performance will\nbe quite poor as well. A good AI team would\nbe able to ameliorate, or to reduce some\nof these problems, but doing this is not that easy. This is one of\nthe things that AI is actually much weaker than humans. If a human has learned\nfrom images on the left, they're much more likely to\nbe able to adapt to images like those on the right\nas they figure out that the patient is\njust lying on an angle. But then AI system\ncan be much less robust than human doctors in generalizing or\nfiguring out what to do with new types of\ndata like these. I hope these examples\nare helping you hone your intuitions about\nwhat AI can and cannot do. In case the boundary between\nwhat it can or cannot do still seems fuzzy\nto you, don't worry. It is completely normal,\ncompletely okay. In fact even today, I still\ncan't look at a project and immediately tell is something\nthat's feasible or not. I often still need weeks or small numbers of weeks\nof technical diligence before forming strong conviction about whether something\nis feasible or not. But I hope that these examples can at least help\nyou start imagining some things in\nyour company that might be feasible and might be\nworth exploring more. The next two videos after\nthis are optional and are a non-technical description of what are neural networks\nand what is deep learning. Please feel free to watch those. Then next week, we'll go\nmuch more deeply into the process of what building an AI project\nwould look like. I look forward to\nseeing you next week.""",7,0,1
coursera,deeplearning.ai,ai-for-everyone,non-technical-explanation-of-deep-learning-part-1-optional,"b""The terms deep learning and neural network\nare used almost interchangeably in AI. And even though they're great for machine learning, there's also been a bit\nof hype and bit of mystique about them. This video will demystify deep learning,\nso that you have a sense of what deep\nlearning and neural networks really are. Let's use an example\nfrom demand prediction. Let's say you run a website\nthat sells t-shirts. And you want to know,\nbased on how you price the t-shirts, how many units you expect to sell,\nhow many t-shirts you expect to sell. You might then create a dataset like this, where the higher the price of the t-shirt,\nthe lower the demand. So you might fit a straight line to this\ndata, showing that as the price goes up, the demand goes down. Now demand can never go below zero, so maybe you say that the demand\nwill flatten out at zero, and beyond a certain point you expect\npretty much no one to buy any t-shirts. It turns out this blue line is maybe\nthe simplest possible neural network. You have as input the price, A, and you want it to output\nthe estimated demand, B. So the way you would draw this\nas a neural network is that the price will be input to\nthis little round thing there, and this little round thing\noutputs the estimated demand. In the terminology of AI,\nthis little round thing here is called a neuron, or sometimes it's\ncalled an artificial neuron, and all it does is compute this blue curve\nthat I've drawn here on the left. This is maybe the simplest possible neural\nnetwork with a single artificial neuron, that just inputs the price and\noutputs the estimated demand. If you think of this orange circle, this\nartificial neuron as a little Lego brick, all that a neural network is, if you take\na lot of these Lego bricks and stack them on top of each other until you get a big\npower, a big network of these neurons. Let's look at a more complex example. Suppose that instead of knowing only\nthe price of the t-shirts, you also have the shipping costs that the customers\nwill have to pay to get the t-shirts. May be you spend more or\nless on marketing in a given week, and you can also make the t-shirt\nout of a thick, heavy, expensive cotton or a much cheaper,\nmore lightweight material. These are some of the factors that\nyou think will affect the demand for your t-shirts. Let's see what a more complex\nneural network might look like. You know that your consumers\ncare a lot about affordability. So let's say you have one neuron,\nand let me draw this one in blue, whose job it is to estimate\nthe affordability of the t-shirts. And so affordability is mainly\na function of the price of the shirts and of the shipping cost. A second thing though affecting demand for\nyour t-shirts is awareness. How much are consumers aware that\nyou're selling this t-shirt? So the main thing that affects awareness,\nis going to be your marketing. So let me draw here a second\nartificial neuron that inputs your marketing budget,\nhow much you spend on marketing, and outputs how aware\nare consumers of your t-shirt. Finally, the perceived quality of\nyour product will also affect demand, and perceived quality would\nbe affected by marketing. The marketing tries to convince people\nthis is a high quality t-shirt, and sometimes the price of something\nalso affects perceived quality. So I'm going to draw here\na third artificial neuron that inputs price,\nmarketing and material, and tries to estimate the perceived\nquality of your t-shirts. Finally, now that the earlier neurons,\nthese three blue neurons, have figured out how affordable,\nhow much consumer awareness and what's the perceived quality,\nyou can then have one more neuron over here that takes as input these three\nfactors and outputs the estimated demand. So this is a neural network, and its job is to learn to map\nfrom these four inputs, that's the input A,\nto the output B, to demand. So it learns this input output or\nA to B mapping. This is a fairly small neural network\nwith just four artificial neurons. In practice, neural networks used today\nare much larger, with easily thousands, tens of thousands or even much\nlarger than that numbers of neurons. Now, there's just one final detail of\nthis description that I want to clean up, which is that in the way I've\ndescribed the neural network, it was as if you had to figure out that\nthe key factors are affordability, awareness and perceived quality. One of the wonderful things about\nusing neural networks is that to train a neural network, in other words,\nto build a machine learning system using neural network, all you have to do is\ngive it the input A and the output B. And it figures out all of\nthe things in the middle by itself. So to build a neural network,\nwhat you would do is feed it lots of data, or the input A, and have a neural\nnetwork that just looks like this, with a few blue neurons feeding\nto a yellow open neuron. And then you have to give it\ndata with the demand B as well. And it's the software's job to figure\nout what these blue neurons should be computing, so that it can\ncompletely automatically learn the most accurate possible function mapping\nfrom the input A to the output B. And it turns out that if you\ngive this enough data and train a neural network that is big enough, this can do an incredible good job\nmapping from inputs A to outputs B. So that's a neural network,\nis a group of artificial neurons each of which computes a relatively\nsimple function. But when you stack enough of\nthem together like Lego bricks, they can compute incredibly complicated\nfunctions that give you very accurate mappings from the input A to the output B. Now, in this video you saw an example\nof neural networks applied to demand prediction. Let's go on to the next video\nto see a more complex example of neural networks applied\nto face recognition.""",8,0,1
coursera,deeplearning.ai,ai-for-everyone,non-technical-explanation-of-deep-learning-part-2-optional,"b""In the last video, you saw how a neural\nnetwork can be applied to demand prediction, but how can the new\nnetwork look at the picture and figure out\nwhat's in the picture? Or listen to an audio clip and understand what is\nsaid in an audio clip? Let's take a look at\na more complex example of applying a neural network\nto face recognition. Say you want to build a system to recognize people from pictures, how can a piece of\nsoftware look at this picture and figure out the identity of the person in it? Let's zoom in to\na little square like that to better understand how\na computer sees pictures. Where you and I see a human eye, a computer instead sees that, and sees this grid of pixel brightness values\nthat tells it, for each of the pixels\nin the image, how bright is that pixel. If it were a black and\nwhite or grayscale image, then each pixel\nwould correspond to a single number telling you\nhow bright is that pixel. If it's a color image, then each pixel will\nactually have three numbers, corresponding to how\nbright are the red, green, and blue\nelements of that pixel. So, the neural networks\njob is to take as input a lot of numbers\nlike these and tell you the name of the person\nin the picture. In the last video, you saw how a neural network\ncan take as input four numbers corresponding\nto the price, shipping costs,\namounts of marketing, and cloth material of\na T-shirt and output demand. In this example, the neural\nnetwork just has to input a lot more numbers\ncorresponding to all of the pixel brightness values\nof this picture. If the resolution of this picture is 1000 pixels by 1000 pixels, then that's a million pixels. So, if it were a black and\nwhite or grayscale image, this neural network\nwas take as input a million numbers corresponding\nto the brightness of all one million pixels\nin this image or if was a color image it would take as input three million numbers\ncorresponding to the red, green, and blue values of each of these one million pixels\nin this image. Similar to before, you\nwill have many many of these artificial neurons\ncomputing various values, and it's not your job to figure out what these neurons\nshould compute. The neural network will\nfigured out by itself. Typically, when you\ngive it an image, the neurons in the earlier parts of the neural network\nwill learn to detect edges in pictures and then lobe and later learn to\ndetect parts of objects. So, they learn to detect eyes and noses and the shape of cheeks\nand the shape of mouths, and then the later neurons,\nfurther to the right, will learn to detect\ndifferent shapes of faces and it will finally, put all this together to output the identity of\nthe person in the image. Again, part of the magic\nof neural networks is that you don't really need to worry about what it is\ndoing in the middle. All you need to do\nis give it a lot of data of pictures like this, A, as well as the correct identity B and\nthe learning algorithm will figure out by\nitself what each of these neurons in the middle\nshould be computing. Congratulations on finishing all the videos for this week. You now know how machine\nlearning and data science work. I look forward to seeing\nyou in next week's videos, as well where you'll\nlearn how to build your own machine learning\nor data science project. See you next week.""",9,0,1
coursera,deeplearning.ai,ai-for-everyone,week-2-introduction,"b""Welcome back. Last week\nyou learned about the basics of AI and\nmachine learning technology. But how do you use\nthis technology in a project, either if you want\nto do a project in your proverbial garage or if you want to do your project\nin a bigger company, or maybe even have\nsomething that builds up to align with\nyour corporate strategy? Let's take a look.\nFirst, in this week you learn what is the workflow\nof an AI project. Different projects\nhave different steps. So, just as a birthday party has a sequence of\npredictable steps. First, you figure\nout the guest list, and you find a venue, then\nyou order the birthday cake, and send invites, and so on. So, too does an AI project have a sequence of\npredictable steps. So, through this, you learn what is the workflow\nof an AI project, you learn what it\nfeels like to be working on an AI project. Second, how do you\nselect an AI project? Seems there are a lot\nof things you could do. The second thing you\nlearned this week is a framework for brainstorming and selecting potentially\npromising projects to work on, either by yourself or\nwith a few friends, or as part of\na bigger company effort. Finally, you also learn how to organize the data as\nwell as the team, which again could be just you are a few friends or\nmuch bigger corporate team. We learn how to\norganize the data and team for executing\non an AI project. By the end of the week, you\nknow what it feels like and how to build\nyour own AI project, and maybe you'll be able\nto start exploring with some of your friends\npromising ideas to try. Let's go on to the next video.""",10,0,1
coursera,deeplearning.ai,ai-for-everyone,workflow-of-a-machine-learning-project,"b'Machine learning\nalgorithms can learn input to output or A to B mappings. So, how do you build\na machine learning project? In this video,\nyou\'ll learn what is the workflow of\nmachine learning projects. Let\'s take a look. As\na running example, I\'m going to use\nspeech recognition. So, some of you may have an\nAmazon Echo or Google Home or Apple Siri device or a Baidu\nDuerOS device in your homes. Some years back, I\'ve\ndone some work on Google\'s speech\nrecognition system that also led Baidu\'s DuerOS project. Today, I actually have a\nAmazon Echo in my kitchen. So, every time I\'m boiling\nan egg I will say, ""Alexa, set timer for three minutes,""\nand then it lets me know when the three minutes\nare up and my eggs are ready. So, how do you build a speech recognition system that can recognize when you say, ""Alexa,"" or ""Hey,\nGoogle,"" or ""Hey, Siri,"" or ""Hello, Baidu""? Let\'s go through the key steps of a machine learning project. Just for simplicity, I\'m\ngoing to use Amazon Echo or detecting the Alexa keywords\nas this running example. If you want to build\nan AI system or build a machine learning system\nto figure out when a user has said the word Alexa, the first step is\nto collect data. So, that means, you\nwould go around and get some people to say the word ""Alexa"" for you and you record\nthe audio of that. You\'ll also get a bunch of\npeople to say other words like ""Hello,"" or say lots\nof other words and record the audio of that as well. Having collected\na lot of audio data, a lot of these audio clips\nof people saying either ""Alexa"" or\nsaying other things, step two is to then\ntrain the model. This means you will use\na machine learning algorithm to learn an input to output\nor A to B mapping, where the input A would\nbe an audio clip. In the case of the first\naudio clip above, hopefully, it will tell you\nthat the user said ""Alexa,"" and in the case\nof audio clip two, shown on the right, hopefully, the system will learn to recognize that the user\nhas said ""Hello."" Whenever an AI team starts\nto train the model, meaning to learn the A to\nB or input-output mapping, what happens, pretty\nmuch every time, is the first attempt\ndoesn\'t work well. So invariably, the team will need to try many times or in AI, we call this iterate many times. You have to iterate\nmany times until, hopefully, the model looks\nlike is good enough. The third step is to then\nactually deploy the model. What that means is you\nput this AI software into an actual smart speaker\nand ship it to either a small group\nof test users or to a large group of users. What happens in a lot of AI products is that\nwhen you ship it, you see that it starts\ngetting new data and it may not work as well as\nyou had initially hoped. So, for example,\nI am from the UK. So, I\'m going to\npick on the British. But let\'s say you had trained your speech recognition system on American-accented speakers and you then ship this smart speaker\nto the UK and you start having British-accented\npeople say ""Alexa."" They may find that it doesn\'t recognize the speech as\nwell as you had hoped. When that happens, hopefully, you can get data back\nof cases such as maybe British-accented\nspeakers was not working as well\nas you\'re hoping, and then use this data to maintain and to update the model. So, to summarize,\nthe key steps of a machine learning project\nare to collect data, to train the model, the A to B mapping, and then to deploy the model. Throughout these steps, there is often a lot of iteration, meaning fine-tuning or\nadapting the model to work better or getting data back even after\nyou\'ve shipped it to, hopefully, make\nthe product better, which may or may not be\npossible depending on whether you\'re able\nto get data back. Let\'s look at these three steps\nand see how they apply on a different project on building a key component of\na self-driving car. So, remember the key steps, you collect data,\nyou train model, deploy model, since we\'ll revisit these steps\non the next slide. Let\'s say you\'re building\na self-driving car. One of the key components\nof a self-driving car is a machine learning algorithm\nthat takes as input, say a picture, of\nwhat\'s in front of your car and tells you\nwhere are the other cars. So, what\'s the first step of building this machine\nlearning system? Hopefully, you remember\nfrom the last slide that the first step was\nto collect data. So, if your goal is to have a machine learning algorithm\nthat can take as input an image and output\nthe position of other cars, the data you would\nneed to collect would be both images as well as position of other cars that you want\nthe AI system to output. So, let\'s say you start off with a few pictures like this. These are the inputs A to\nthe machine learning algorithm. You need to also tell it what is the output B you would want. So, for each of these pictures, you would draw a rectangle around the cars in the picture\nthat you wanted to detect. On this slide, I\'m hand drawing these rectangles,\nbut in practice, you will use some software\nthat lets you draw perfect rectangles rather than these hand-drawn ones. Then, having created\nthis dataset, what was the second step? Hope you remember that\nthe second step was to train the model. Now, invariably, when your AI engineers\nstart training a model, they\'ll find, initially, that\nit doesn\'t work that well. For example, given this picture, maybe the software,\nthe first few tries, thinks that that is a car. It\'s only by iterating\nmany times that you, hopefully, get a better result\nwhen figuring out that that is where\nthe car actually is. Finally, what was the third step? It was to deploy the model. Of course, in\nthe self-driving world, it\'s important to treat\nsafety as number one, and deploy model or to test the model only in ways\nthat can preserve safety. But when you put the software\nin cars on the road, you may find that there\nare new types of vehicles, say golf carts, that the software isn\'t\ndetecting very well. So, you get data back, say, pictures of\nthese golf carts, using new data to maintain and update the model so\nthat, hopefully, you can have your AI software continually get better\nand better to the point where you end up with\na software that can do a pretty good job detecting other cars from\npictures like these. In this video, you\nlearned what are the key steps of\na machine learning project, which are to collect data, to train the model, and then to deploy the model. Next, let\'s take\na look at what are the key steps or what is a workflow of\na data science project. Let\'s go on to the next video.'",11,0,1
coursera,deeplearning.ai,ai-for-everyone,workflow-of-a-data-science-project,"b""Unlike a machine\nlearning project, the output of a data\nscience project is often a set of\nactionable insights, a set of insights that may cause you to do things differently. So, data science projects have a different workflow than\nmachine learning projects. Let's take a look\nat one of the steps of a data science project. As our running example, let's say you want to\noptimize a sales funnel. Say you run a e-commerce or a online shopping\nwebsite that sells coffee mugs and so for a user to buy\na coffee mug from you, there's a sequence of steps\nthey'll usually follow. First, they'll visit\nyour website and take a look at the different coffee mugs\non offer, then eventually, they have to get\nto a product page, and then they'll have to put\nit into their shopping cart, and go to the shopping cart page, and then they'll finally\nhave to check out. So, if you want to optimize\nthe sales funnel to make sure that as many people as possible get through\nall of these steps, how can you use data science\nto help with this problem? Let's look at the key steps\nof a data science project. The first step is\nto collect data. So, on a website\nlike the one we saw, you may have a data\nset that stores when different users go to\ndifferent web pages. In this simple example, I'm assuming that\nyou can figure out the country that the users\nare coming from, for example, by looking at\ntheir computers' address, called an IP address, and figuring out\nwhat is the country from which they're originating. But in practice,\nyou can usually get quite a bit more data about users than just what\ncountry they're from. The second step is to\nthen analyze the data. Your data science team may\nhave a lot of ideas about what is affecting the performance\nof your sales funnel. For example, they may think that overseas customers\nare scared off by the international shipping costs which is why a lot\nof people go to the checkout page but\ndon't actually check out. If that's true then\nyou might think about whether to put\npart of shipping costs into the actual product costs\nor your data science team may think there are blips in the data whenever\nthere's a holiday. Maybe more people\nwill shop around the holidays because\nthey're buying gifts or maybe fewer people will shop around the holidays\nbecause they're staying home rather than sometimes shopping from\ntheir work computers. In some countries, there\nmay be time-of-day blips where in countries\nthat observe a siesta, so a time of rest like\nan afternoon rest, there may be fewer shoppers online and so\nyour sales may go down. They may then suggest\nthat you should spend fewer advertising dollars during the period of siesta because fewer people will go online\nto buy at that time. So, a good data science team may have many ideas and so they try many ideas or will say iterate many times to get good insights. Finally, the data science team will distill these insights down to a smaller number\nof hypotheses about ideas of what could be going well and what\ncould be going poorly as well as a smaller number\nof suggested actions such as incorporating shipping costs\ninto the product costs rather than having it\nas a separate line item. When you take some of\nthese suggested actions and deploy these changes\nto your website, you then start to get new data back as users behave\ndifferently now that you advertise\ndifferently at the time of siesta or have a different\ncheck out policy. Then your data science team\ncan continue to collect data and we analyze the new data periodically to see if\nthey can come up with even better hypotheses or\neven better actions over time. So the key steps of\na data science project are to collect the data, to analyze the data, and then to suggest\nhypotheses and actions, and then to continue\nto get the data back and reanalyze the data\nperiodically. Let's take this framework and\napply it to a new problem, to optimizing a\nmanufacturing line. So we'll take these three steps and use them on\nthe next slide as well. Let's say you run\na factory that's manufacturing thousands\nof coffee mugs a month for sale and you want to optimize the manufacturing line. So, these are the key steps\nin manufacturing coffee mugs. Step one is to mix the clay, so make sure the appropriate\namount of water is added. Step two is take this clay\nand to shape the mugs. Then you have to add the glaze, so add the coloring,\na protective cover. Then you have to heat this mug and we call\nthat firing the kiln. Finally, you would inspect the mug to make sure there aren't dents in the mug and it isn't cracked before you\nship it to customers. So, a common problem\nin manufacturing is to optimize the yield of\nthis manufacturing line to make sure that as few damaged\ncoffee mugs get produced as possible because those are coffee mugs you\nhave to throw away, resulting in time\nand material waste. What's the first step of\na data science project? I hope you remember from\nthe last slide that the first step is\nto collect data. So for example, you\nmay save data about the different batches of\nclay that you've mixed, such as who supplied the clay and how\nlong did you mix it, or maybe how much moisture\nwas in the clay, how much water did you add. You might also collect data about the different batches\nof mugs you made. So how much humidity\nwas in that batch? What was the temperature\nin the kiln and how long did you\nfire it in the kiln? Given all this data\nyou would then ask the data science team to analyze\nthe data and they would, as before, iterate many times\nto get good insights. So, they may find\nthat, for example, that whenever the humidity is too low and the kiln\ntemperature is too hot that there are cracks in\nthe mug or they may find out that because\nit's warmer in the afternoon that\nyou need to adjust the humidity and temperature depending on the time of day. Based on the insights from your data science team you get\nsuggestions for hypotheses and actions on how to\nchange the operations and manufacturing line in order to improve the productivity\nof the line. When you deploy the changes, you then get new data back that you can reanalyze\nperiodically so they can keep on optimizing the performance of\nyour manufacturing line. To summarize, the key steps of a data science project\nare to collect the data, to analyze the data, and then to suggest\nhypotheses and actions. In this video and the last video\nyou saw some examples of machine learning projects\nand data science projects. It turns out that machine\nlearning and data science are affecting almost every\nsingle job function. What I want to do in\nthe next video is show you how these ideas are affecting\nmany job functions, including perhaps yours and certainly that of many\nof your colleagues. Let's go on to the next video.""",12,0,1
coursera,deeplearning.ai,ai-for-everyone,every-job-function-needs-to-learn-how-to-use-data,"b'Data is transforming\nmany different job functions, whether you work in\nrecruiting or sales or marketing or manufacturing\nor agriculture, data is probably transforming\nyour job function. What\'s happened in\nthe last few decades is the digitization\nof our society. So, rather than handing out\npapers surveys like these, surveys are more\nlikely to be done in digital format or doctors still write\nsome handwritten notes but doctors handwritten note\nis increasingly likely to be a digital record and so to this in just about every\nsingle job function. This availability\nof data means that there\'s a good chance\nthat your job function could be helped with tools like data science or machine\nlearning. Let\'s take a look. In this video, I\nwant to run through many different job functions\nand discuss how data science and\nmachine learning can or will impact these\ndifferent types of jobs. Let\'s start with sales. You\'ve already seen\nin the last video how data science can be used to\noptimize a sales funnel. How about machine learning? If you\'re a salesperson\nyou may have a set of leads about\ndifferent people that you could reach out to to convince them to buy\nsomething from your company. Machine learning can help\nyou prioritize these leads. So, you might want to prioritize calling up the CEO of\na large company rather than the intern at a much smaller\ncompany and this type of automated leads sorting is making salespeople\nmore efficient. Let\'s look at more examples. Let say you\'re\nmanufacturing line manager. You\'ve already seen\nhow data science can help you optimize\na manufacturing line. How about machine learning? One of the steps of this manufacturing process\nis the final inspection. In fact today, in many factories there can\nbe hundreds or thousands of people using the human eye\nto check over objects, maybe coffee mugs, maybe other\nthings to see if they\'re scratches or dents and\nthat\'s called inspection. So, machine learning\ncan take this input, a dataset like this, and learn to automatically figure out if a coffee mug\nis defective or not. By automatically finding\nscratches or dents, it can reduce labor costs and also improve quality\nin your factory. This type of automated\nvisual inspection is one of the technologies that\nI think will have a big impact on manufacturing. This is something I\'ve been\nworking on myself as well. Let\'s see more examples. How about recruiting? When recruiting someone\nto join your company, there may be a pretty\npredictable sequence of steps where your recruiter or someone else would\nsend an email to a candidate and then you\'d\nhave a phone call with them, bring them on-site\nfor an interview and then extend an offer and\nmaybe close the offer. Similar to how\ndata science can be used to optimize a sales funnel, recruiting can also use\ndata science to optimize a recruiting funnel and in fact many recruiting organizations\nare doing so today. For example, if you\nfind that hardly anyone is making it from\nphone screen step to the on-site interviews\nstep then you may conclude that maybe\ntoo many people are getting into the phone screen\nstage or maybe the people doing\nthe phone screen are just being too tough\nand you should let more people get to\nthe onsite interview stage. This type of\ndata science is already having an impact on recruiting. What about machine\nlearning projects? Well, one of the steps\nof recruiting is to screen a lot of resumes to decide who to reach out to you. So, you may have to look at\none resume and say, ""Yes, let\'s email them"", look at\na different one to say, ""No, let us not move ahead\nwith this candidate."" Machine learning is\nstarting to make its way into automated resume screening. Does raise important\nethical questions such as making sure that your AI software does not exhibit undesirable forms of bias\nand treats people fairly, but machine learning\nis starting to make inroads into this and I hope can do so while making sure that the systems are ethical and fair. In the final week of this\nAI For Everyone course, you\'ll also learn\nmore about the issues of fairness and ethics in AI. What if you work in marketing? One of the common ways\nto optimize the performance in website\nis called AB testing, in which you launch\ntwo versions of website. Here version A has a red button, version B has a green button\nand you measure which websites causes people\nto click through more. So with this type of data, a data science team can\nhelp you gain insights and suggests hypotheses or actions for optimizing your website. How about machine learning\nand marketing? Today a lot of websites will give customized product\nrecommendations to show you the things you\'re most likely\nto want to buy and this actually significantly increases\nsales on these websites. For example, a clothing website after seeing the way\nI shop after while, will hopefully just recommend blue shirts to me because that\'s frankly pretty much\nthe only type shirt I ever buy, but maybe other\ncustomers will have more diverse and more interesting\nrecommendations than mine. But today these customized product recommendations\nactually drive a large percentage of sales on many large online\ne-commerce websites. One last example from\na totally different sector. Let\'s say you work\nin agriculture. Maybe you\'re a farmer working on the light industrial farm, how can data science help you? Today farmers are already using data science for crop analytics, where you can take data on the soil conditions,\nthe weather conditions, the presence of different crops\nin the market and have data science teams make\nrecommendations to what to plant, when to plant so\nas to improve use while maintaining the condition\nof the soil on your farm. This type of\ndata science is and will play a bigger and bigger role\nin agriculture. Let\'s also look at\nthe machine learning example. I think one of\nthe most exciting changes to agriculture is\nprecision agriculture. Here\'s a picture that I took on a farm with my cell phone. On the upper right is a cotton plants and shown\nin the middle is a weed. With machine learning,\nwe\'re starting to see products that\ncan go onto the farms, take a picture like\nthis and spray a weed killer in\na very precise way just onto the weeds so that it gets rid of the weed but without having to spray an excessive\namounts of weed killers. This type of machine learning technology is both\nhelping farmers increase crop yields while also helping to preserve\nthe environment. In this video, you saw how all of these job functions,\neverything from sales, recruiting to marketing to manufacturing to\nfarming agriculture, how all of these job functions are being affected by data, by data science and\nmachine learning. It seems like there\'s a lot of different things you\ncould do with AI. But how do you actually select a promising\nproject to work on? Let\'s talk about that\nin the next video.'",13,0,1
coursera,deeplearning.ai,ai-for-everyone,how-to-choose-an-ai-project-part-1,"b""If you want to try your\nhand at an AI project, how do you select a worthwhile\nproject to work on? Don't expect an idea to\nnaturally come overnight. Sometimes it happens, but\nsometimes it also takes a few days or maybe a few weeks to come up\nwith a worthy idea to pursue. In this video, you see a framework for brainstorming potentially\nexciting AI projects to pursue. Let's say you want to build\nan AI project for your business. You've already seen that AI\ncan't do everything, and so there's going to be a certain set\nof things that is what AI can do. So let's let the circle represent\nthe set of things that AI can do. Now, there's also going to be a certain\nset of things that is valuable for your business. So let's let this second circle represent\na set of things that are valuable for your business. What you would like to do is try to select\nprojects that are at the intersection of these two sets, so you select projects\nhopefully that are both feasible, that can be done with AI, and\nthat are also valuable for your business. So AI experts will tend to have\na good sense of what is and what isn't in the set on the left. And domain experts, experts in your\nbusiness, be it sales and marketing, or agriculture or something else, will have the best sense of what is\nactually valuable for your business. So when brainstorming projects that AI can\ndo and are valuable for your business, I will often bring together a team\ncomprising both people knowledgeable of AI, as well as experts in your\nbusiness area to brainstorm together. So that together they can try to\nidentify projects at the intersection of both of these two sets. So sometimes we also call these\ncross-functional teams, and that just means a team that\nincludes both AI experts, as well as domain experts,\nmeaning experts in your area of business. When brainstorming projects,\nthere's a framework that I've used with a lot of companies\nthat I found to be useful. So let me share with you three\nprinciples or three ideas for how you can have a team\nbrainstorm projects. First, even though there's been a lot\nof press coverage about AI automating jobs away, and this is an important\nsocietal issue that needs to addressed, when thinking about concrete AI projects,\nI find it much more useful to think about automating\ntasks rather than automating jobs. Take call center operations, there are a\nlot of tasks that happen in a call center. Ranging from people picking up the phone\nto answering phone calls to replying to emails, to taking specific actions, such as issuing a refund on\nbehalf of a customer request. But of all of these tasks that\nemployees in a call center do, there may be one,\ncall routing or email routing, that maybe particularly amenable\nto machine learning automation. And it's by looking at all these tasks\nthat the group of employees do and selecting one that may allow you to\nselect the most fruitful project for automation in the near term. Let's look at another example,\nthe job of a radiologist. There's been a lot of press about how\nAI my automate radiologists' jobs, but radiologists actually\ndo a lot of things. They read x-rays,\nthat's really important, but they also engage in their\nown continuing education. They consult with other doctors, they may mentor younger doctors, some of\nthem also consult directly with patients. And so it's by looking at all of these\ntasks that a radiologist does that you may Identify one of them, let's say AI\nassistance or AI automation for reading x-rays, that allows you to select\nthe most fruitful projects to work on. So what I would recommend is,\nif you look at your business, think about the tasks that people do, to\nsee if you can identify just one of them, or just a couple of them, that may be\nautomatable using machine learning. When I'm meeting CEOs of large companies\nto brainstorm AI projects for the company, a common question I'll also ask is, what\nare the main drivers of business value? And sometimes finding AI solutions or data science solutions to augment\nthis can be very valuable. Finally, a third question that\nI've asked that sometimes let to valuable project ideas is, what\nare the main pain points in your business? Some of them could be solved with AI,\nsome of them can't be solved with AI. But by understanding the main\npain points in the business, that can create a useful starting point\nfor brainstorming AI projects as well. I have one last piece of advice for\nbrainstorming AI projects, which is that you can make progress even without\nbig data, even without tons of data. Now don't get me wrong, having more\ndata almost never hurts, other than maybe needing to pay a bit more for disk\nspace or network bandwidth to transmit and store the data, having more data\nalmost always is only helpful. And I love having lots of data. It is also true that data makes some on\nbusinesses, like web search, defensible. Web search is a long tail business,\nmeaning that there are a lot of very, very rare web queries. And so seeing what people click on when\nthey search on all of these rare web queries does help the leading web search\nengines have a much better search experience. So big data is great when you can get it,\nbut I think big data also sometime over-hyped, and even with a small dataset,\nyou can still often make progress. Here's an example,\nlet's say you're building a automated visual inspection system for\nthe coffee mug. So you want to automatically detect that\nthe coffee mug on the right is defective. Well, if you had a million\npictures of good coffee mugs and defective coffee mugs, it'd be\ngreat to have that many examples of pictures of good and\nbad coffee mugs to feed your AI system. But I hope you have not manufactured\n1 million defective coffee mugs, because that feels like a very\nexpensive thing to have to throw away. So sometimes with as few as 100, or maybe\n1,000, or sometimes maybe as few as 10, you may be able to get started\non the machine learning project. The amount of data you need is\nvery problem dependent, and speaking with an AI engineer or\nAI expert would help you get better sense. There are some problems where\n1,000 images may not be enough, where you do need big data\nto get good performance. But my advice is, don't give up just because you don't\nhave a lot of data to start off with. And you can often still make progress,\neven with a small dataset. In this video, you saw a brainstorming\nframework, and a set of criteria for trying to come up with projects that\nhopefully can be doable with AI, and are also valuable for your business. Now, having brainstormed a list of\nprojects, how do you select one or select a small handful to\nactually commit to and work on? Let's talk about that in the next video.""",14,0,1
coursera,deeplearning.ai,ai-for-everyone,how-to-choose-an-ai-project-part-2,"b'Maybe you have a lot of ideas for possible AI projects to work on. But before committing to one, how do you make sure\nthat this really is a worthwhile project? If it\'s a quick project\nthat might take you just a few days maybe just jump in right away and see\nif it works or not, but some AI projects may\ntake many months to execute. In this video, I want to step you through the process that I use to double-check if a project is worth that many months of\neffort. Let\'s take a look. Before committing to\na big AI project, I will usually conduct\ndue diligence on it. Due diligence has a specific\nmeaning in the legal world. But informally, it just\nmeans that you want to spend some time to make sure what you hope is true really is true. You\'ve already seen how the best AI projects are\nones that are feasible. So, it\'s something\nthat AI can do, as well as valuable. We really want to\nchoose projects to that the intersection\nof these two sets. So, to make sure\na project is feasible, I will usually go through\ntechnical diligence, and make sure that\nthe project is valuable, I will usually go through a\nbusiness diligence process. Let me tell you more\nabout these two steps. Technical diligence is\nthe process of making sure that the AI system you\nhope to build really is doable, really is feasible. So, you might talk to\nAI experts about whether or not the AI system can actually meet the desired level\nof performance. For example, if you\nare hoping to build a speech system that is\n95 percent accurate, consulting of AI experts\nor perhaps reading some of the trade literature can give you a sense of whether\nthis is doable or not. Or if you want a system\nto inspect coffee mugs in a factory and you need your system to be\n99 percent accurate. Again, is this actually doable\nwith today\'s technology? A second important question\nfor technical diligence is how much data is needed to get to this desired level\nof performance, and do you have a way\nto get that much data. Third, would be engineering timeline to try to\nfigure out how long it will take and how many people\nit will take to build a system that you\nwould like to have built. In addition to\ntechnical diligence, I will often also conduct\nbusiness diligence to make sure that\nthe project you envision really is valuable\nfor the business. So, a lot of AI projects will drive value\nthrough lowering costs. For example, by\nautomating a few tasks or by squeezing more efficiency\nonto the system. A lot of AI systems can\nalso increase revenue. For example, driving more people to check out in\nyour shopping cart or you may be building\nan AI system to help you launch a new product or\na new line of business. So, business diligence\nis the process of thinking through carefully\nfor the AI system that you\'re building such as a speech recognition system\nthat\'s 95 percent accurate or a visual inspection system that\'s 99.9 percent accurate, would allow you to achieve\nyour business goals. Whether your business goal is to improve your current business or to even create brand new\nbusinesses in your company. When conducting\nbusiness diligence, I\'ll often end up building spreadsheet financial models to estimate the value\nquantitatively such as estimate how many dollars\nare actually saved or what do we think is\na reasonable assumption in terms of entries revenue, and to model out\nthe economics associated with a project before committing to many months of\neffort on a project. Although not explicitly\nlisted on this slide, one thing I hope you\nalso consider doing as a third type of diligence\nwhich is ethical diligence. I think there are a lot\nof things that AI can do that will even\nmake a lot of money, but that may not make\nsociety better off. So, in addition to technical diligence and\nbusiness diligence, I hope you also conduct ethical diligence and make\nsure that whatever you\'re doing is actually making humanity and making\nsociety better off. We\'ll also talk\nmore about this in the last week of\nthis course as well. As you\'re planning\nout your AI project, you also have to decide do\nyou want to build or buy? This is an age old question\nin the IT world and we\'re facing\nthis question in AI as well. For example, hardly any companies build their own\ncomputers these days. They buy someone\nelse\'s computers and hardly any companies build\ntheir own Wi-Fi routers, just buy a commercial\nWi-Fi router. How about machine learning\nand data science? Machine learning projects can\nbe in-house or outsource. I\'ve seen both of these models\nused successfully. Sometimes if you outsource\na machine learning project, you can have access\nmuch more quickly to talent and get going\nfaster on a project. It is nice if eventually\nyou build your own in-house AI team and can also\ndo these projects in house. You\'ll hear more about\nthis when we talk about AI translation playbook in\ngreater detail next week. Unlike machine\nlearning projects though, data science projects are\nmore commonly done in-house. They\'re not impossible\nto outsource, you can sometimes outsource them, but what I\'ve seen is that data science projects are\noften so closely tied to your business that it takes very deep day-to-day\nknowledge about your business to do\nthe best data science projects. So, just as a percentage,\nas a fraction, I see data science projects in-house more than machine\nlearning projects. Finally, in every\nindustry some things will be industry standard and you should avoid\nbuilding those. A common answer to the build\nversus buy question was, build the things\nthat are going to be quite specialized to you or completely specialized\nto you or they\'ll allow you to build a unique\ndefensible advantage, but the things that\nwill be industry standard probably\nsome other company will build and it\'ll be more\nefficient for you to just buy it rather than\nbuild it in-house. One of my teams have\na really poetic phrase which is, ""Don\'t sprint in front of a train,"" and what that means is, if this is a train running on a railway tracks and that\'s the small chimney\nwith the puff of smoke. What you don\'t want to do\nis to be the person or the engineer trying to sprint faster and faster\nahead of the train. The train is the industry\nstandard solution, and so, if there\'s\na company maybe a startup, maybe a big company or\nmaybe an open-source effort that is building\nan industry standard solution, then you may want\nto avoid trying to run faster and faster to\nkeep ahead of the train. Because even though you could sprint faster in the short term, eventually the train\nwill catch up and crash someone trying to\nsprint in front of a train. So, when there\'s\na massive force of an industry standard\nsolution that is been built, you might be better off just embracing an industry standard or embracing someone else\'s platform rather than trying to\ndo everything in-house. We all live in a world\nof limited resources, limited time, limited data, limited engineering resources, and so, I hope you can\nfocus those resources on the projects with\nour most unique and will make the biggest difference\nto your company. Through the process of technical diligence as well\nas business diligence, I hope you can start to\nidentify projects that are potentially valuable or that seem promising for your business. If the project is a big company, maybe it\'ll take\nmany months to do. It\'s not unusual for me\nto spend even a few weeks conducting this type of diligence before\ncommitting to a project. Now, say you found\na few promising projects, how do you engage\nwith an AI team? How do you work with an AI team to try to get\nthese projects done? Let\'s talk about that\nin the next video.'",15,0,1
coursera,deeplearning.ai,ai-for-everyone,working-with-an-ai-team,"b""Say you found an exciting project that you want to\ntry to execute on, how do you work with an\nAI team on this project? In this video, you learn how AI teams think about\ndata and therefore how you can interact\nwith AI teams to help them succeed on a project. Now, there is\none caveat which is, whether you have a cool idea but you don't have access\nto an AI team, you don't have any access\nto any AI engineers. Fortunately in today's world, if either yourself\nor you can encourage some of your engineering friends to take an online course or two on machine learning or\ndeep learning that often will give them enough knowledge to get going and make a start\nof an attempt, make a reasonable attempt\non these types of projects. So, let's talk about how you\ncan work with an AI team. First, it really helps\nyour AI team if you can specify an acceptance criteria\nfor the project. I've done a lot of work in\nautomated vision inspection. So, I'm going to use that as a running example in\nthese few slides. Let's say your goal is\nto detect defects in coffee mugs with\nat least 95% accuracy. So, that can be your acceptance criteria\nfor this project. But 95% accuracy, how do\nyou measure accuracy? One of the things that\nAI team would need is a dataset on which to\nmeasure the accuracy. So, dataset is just a set of pictures like these\ntogether with the labels, with the design output B that the first two coffee\nmugs are okay and the third one is defective. So, as part of\nyour specification for the acceptance criteria\nyou should make sure that the AI team has a dataset\non which to measure the performance so\nthat they can know if they've achieved 95% accuracy. The formal term for this dataset\nis called a test set. The test set may not\nneed to be too big, maybe a 1,000 pictures will be just fine\nfor this example. But, if you consult with an AI expert they can give\nyou a better sense of how big the test set needs to be\nfor them to be able to evaluate whether or not they're\ngetting to 95% accuracy. One novel part of\nAI systems is that their performance is usually specified in a statistical way. So, rather than asking for an AI system that just does\nsomething perfectly you see very often that we\nwant an AI system that performs at a certain\npercentage accuracy like this example here. So, when specifying\nyour acceptance criteria think of whether\nyour acceptance criteria needs to be specified in a statistical way\nwhere you specify on average hour does or what percent of time it has\nto get the right answer. Let's dive more deeply into\nthe concept of a test set. This is how AI teams\nthink about data. AI teams group data\ninto two main datasets. The first called\nthe training set and the second called the test set which we've already\ntalked a bit about. The training set is just a set of pictures together\nwith labels showing whether each of\nthese pictures is of a coffee mug that is\nokay or defective. So, the training\nset gives examples of both the input\nA the pictures of the coffee mugs as\nwell as the desired output B whether it's\nokay or defective. So, given this training set what a machine learning algorithm will do is learn in other words computes or figure out some mapping from A to B so\nthat you now have a piece of software that can take\nas input the input A and try to figure out what\nis the appropriate output B. So, the training set is the\ninput to the machine learning software that lets it figure out what is this A to B mapping. The second dataset that an AI team will use\nis the test set. As you've seen this is just\nanother set of images that's different from the training sets also with the labels provided. The way an AI team will evaluate their learning algorithms\nperformance is to give the images into the test set to the AI software and see what\nthe AI software outputs. For example, if on\nthis three test set images the AI software\noutputs okay for this, okay for this, and\nalso okay for this, then we will say\nthat they got two out of three examples right and so that's a 66.7% accuracy. In this figure\nthe training set and test set above have\nonly three pictures, in practice both of these datasets would be\nmuch bigger of course. You find that for\nmost problems the training set is much bigger than the test set. But you can talk to\nAI engineers to find out how much data they need\nfor a given problem. Finally, for technical reasons some AI teams will need not just one but\ntwo different test sets. If you hear AI teams talk about development or deaf\nor validation tests that's the second test\nset that they're using. The reasons why they\nneed two test set is quite technical and beyond\nthe scope of this course, but if an AI team asks you for two different test\nsets it's quite reasonable to try to\nprovide that to them. Before wrapping up\nthis video one pitfall I want to urge you to avoid is expecting a 100% accuracy\nfrom your AI software. Here's what I mean,\nlet's say this is your test set which you've already seen on the last slide. But, let me add a few more\nexamples to this test set. Here are some of the reasons\nit may not be possible for a piece of AI software\nto be a 100% accurate. First, machine\nlearning technology today despite being very powerful still has limitations and they just can't\ndo everything. So, you may be working on\na problem that it's just very difficult even for today's\nmachine learning technology. Second, insufficient data. If you don't have enough data specifically if you don't\nhave enough training data for the AI software to learn\nfrom it may be very difficult to get\na very high level of accuracy. Third, data is messy and sometimes data can be mislabeled. For example, this\ngreen coffee mug here looks perfectly okay to me, so, the label of it being\na defect looks like an incorrect label and that would hurt the performance\nof your AI software. Data can also be ambiguous. For example, it looks\nlike this coffee mug has a small scratch over there and it's\na pretty small scratch, so, maybe we will think\nof this though okay. But maybe this should actually have been a defect or maybe even different experts\nwon't agree if this particular coffee mug is okay and should pass\nthe inspection step. Some of these problems\ncan be ameliorated. For example, if you don't\nhave enough data maybe you can try to collect more data and more data will often help. Or you can also try to clean\nup mislabeled data or try to get your factories\nexperts to come to better agreement about\nthese ambiguous labels. So, there are ways to try to make these things better, but, a lot of AI systems\nare incredibly valuable even without\nachieving a 100% accuracy. So, I will urge you to discuss\nwith your AI engineers what is a reasonable level of accuracy to try to accomplish? Then try to find\nsomething that passes both technical diligence as well as business diligence without necessarily needing\na 100% accuracy. Congratulations on finishing\nall the videos for this week. You now know what it\nfeels like and what it takes to build an AI project. I hope you e start brainstorming and exploring some ideas. There is one more optional video\ndescribing some of the technical tools that AI teams use that you can\nwatch if you wish. But either way I look forward\nto seeing you next week, where you'll learn\nhow AI projects fit in the context\nof a bigger company. Look forward to\nseeing you next week.""",16,0,1
coursera,deeplearning.ai,ai-for-everyone,technical-tools-for-ai-teams-optional,"b""When you work with AI teams, you may hear them\nrefer to the tools that they're using to\nbuild these AI systems. In this video, I want\nto share with you some details and names of the most commonly used AI tools, so that you'd be able to better understand what these\nAI engineers are doing. We're fortunate that the AI\nworld today is very open, and many teams will openly\nshare ideas with each other. There are great machine learning open source frameworks that many teams will use to\nbuild their systems. So, if you hear of any\nof these: TensorFlow, PyTorch, Keras,\nMXNet, CNTK, Caffe, PaddlePaddle,\nScikit-learn, R or Weka, all of these are\nopen source machine learning frameworks that help AI teams be much more efficient in terms\nof writing software. Along with AI\ntechnology breakthroughs are also publish freely on the Internet on this website called Arxiv. Spelled like this. I hope that other academic\ncommunities also freely share their research since\nI've seen firsthand how much this accelerates progress\nin the whole field of AI. Finally, many teams will also share their code\nfreely on the Internet, most commonly on\na website called GitHub. This has become\nthe de facto repository for open source software in AI\nand in other sectors in AI. And by using appropriately\nlicensed open-source software, many teams can get going much faster than if they had to\nbuild everything from scratch. So, for example, if\nI search online for face recognition\nsoftware on GitHub, you might find\na web page like this. And if you scroll down, this actually has a pretty good, very readable description\nof software that is made available on this website for recognizing people's faces, and even finding parts\nof people's faces. There's just a ton\nof software that is freely downloadable for doing all sorts of things\non the Internet, and just double-check\nthe license, or AI team will double-check the license before using\nit in a product of course, but a lot of these software\nis open source, or is otherwise very permissively license\nfor anyone to use. Although GitHub is a technical website\nbuilt for engineers, if you want you should\nfeel free to play around GitHub and see what are the types of\nAI software people have released online as well. In addition to these open source\ntechnical tools, you often also hear AI engineers talk\nabout CPUs and GPUs. Here's what these terms mean. A CPU is the computer processor\nin your computer, whether it's your desktop, your laptop, or a computer\nserver off in the Cloud. CPU stands for\nthe central processing unit, and CPUs are made by Intel, and AMD, and a few\nother companies. This does a lot of the\ncomputation in your computer. GPU stands for\ngraphics processing unit. Historically, the GPU was\nmade to process pictures. So, if you play a video game, it's probably a GPU that is\ndrawing the fancy graphics. But what we found several years ago was that\nthe hardware that was originally built for\nprocessing graphics turns out to be very, very powerful for building\nvery large neural networks. So very large\ndeep learning algorithms. Given the need to build very large deep learning or very large neural\nnetwork systems, the AI community has had\nthis insatiable hunger for more and more\ncomputational power to train bigger and\nbigger neural networks. And GPUs have proved to be a fantastic fit to this type of computation that we need to have done to train very\nlarge neural networks. So, that's why GPUs are playing a big role in the rise\nof deep learning. And video is company that's\nbeen selling many GPUs. But other companies\nincluding Qualcomm, as well as Google making his own CPUs are\nincreasingly making specialized hardware for powering these very large neural networks. Finally, you might hear about\nCloud versus On-premises, or for short,\nOn-prem deployments. Cloud deployments\nrefer to if you rent compute servers such\nas from Amazon's AWS, or Microsoft's Azure,\nor Google's GCP in order to use\nsomeone else's service to do your computation. Whereas, an On-prem\ndeployment means buying your own compute\nservers and running the service locally\nin your own company. A detailed exploration\nof the pros and cons of these two options is beyond\nthe scope of this video. A lot of the world is moving\nto Cloud deployments. Whether you search online you\nfind many articles talking about the pros and cons of Cloud versus On-prem deployments. There is one last term\nyou might hear about, which is Edge deployments. If you are building\na self-driving car, there's not enough time\nto send data from a self-driving car to a Cloud server to decide if you should\nstop the car or not, and then send that message\nback to the self-driving car. So, the computation has to happen usually in a computer right\nthere inside the car. That's called an edge deployment, where you put\na processor right where the data is collected\nso that you can process the data and make a decision very quickly\nwithout needing to transmit the data over the Internet to be\nprocessed somewhere else. If you look at some of the smart speakers in\nyour home as well, this too is an edge deployment\nwhere some, not all, but some of the speech\nrecognition task is done by a processor that is built in right there into this smart speaker\nthat is inside your home. The main advantage of\nEdge deployment is it can increase\nresponse time of the system, and also reduce the amount of data you need to send\nover the network. But there are\nmany pros and cons as well about Edge\nversus Cloud versus On-prem deployments that you can also search online\nto read more about. Thanks to finishing\nthis optional video on the technical tools\nthat AI engineers use. Hopefully, when you hear them refer to some\nof these tools, you'll start to have a better\nsense of what they mean. I look forward to\nseeing you next week.""",17,0,1
coursera,deeplearning.ai,ai-for-everyone,week-3-introduction,"b""Welcome back. In the last two weeks you learned what\nis AI, and how to build an AI project. This week we'll look at the projects\nwe talked about already, and talk about how projects fit\nin the context of a company. Whether it's a for-profit, non-profit,\nor even within a government entity. For the sake of concreteness,\nI'll talk about building AI for companies will equally apply, no matter\nwhat type of organization you're in. Now, in case some of what you hear\nthis week sounds like CEO level talk, don't be intimidated. This is actually useful for\neveryone to know, the ones that hope, your company will\nhelp your organization Improve using AI. And it does take maybe two or three years\nfor a company to become good at AI. To embrace not just one AI project but\na sequence of valuable AI projects and become much more effective. But what I hope to do this week\nis help you paint a vision for where an organization can go over\na longer period of time like that. But also end the week with very concrete\nshort steps you can take right away. So let's get started. In detail,\nthese are the topics you'll see this week. First is case studies\nof complex AI products. So rather then a single\nmachine learning or data science module like\nwhat you saw last week. This week you'll see you how multiple\nmodules can come together to build a much more complex AI product such as\na smart speaker or a self driving car. You also learn what are the major\nroles in an AI team. So if ever you think your company can\nbuild a large AI team with maybe many dozens or maybe even hundreds of people,\nwhat are the things these people will do? We'll start to paint a roadmap for\nwhat building an AI team might look like. Third you also learn about\nthe AI transformation playbook of how to help your\ncompany become good at AI. Where it goes beyond doing one or\ntwo valuable projects, but about making a whole company good at AI\nand hopefully much more effective and valuable as a result. Finally, whereas some of these steps may\ntake a small number of years to complete. We'll wrap this weeks videos with concrete\nsuggestions on how you can take your first step, right away,\ntoward building AI in your company. Beyond these major topics, we'll also have\na couple of optional videos at the end, where you see a survey of major AI\napplication areas and techniques. So, after this week's videos,\nI hope you have a good vision of how you can help your company use AI and also\nhave first steps you can take right away. Lets get started by going\non to the next video.""",18,0,1
coursera,deeplearning.ai,ai-for-everyone,case-study-smart-speaker,"b'What does it feel like to\nwork on a complex AI product, where isn\'t just using a single machine learning\nalgorithm to map from A to B, but that learning\nalgorithm is part of a bigger more complex\nproject or product. I want to start it this week with two case studies of building\ncomplex AI products. First, building a smart speaker so that you can start to\nunderstand what it might feel like to maybe someday work on a complex AI product\nwithin your own company. Let\'s get started. Smart speakers and voice activated devices\nlike these are taking the world by storm and if you don\'t already\nhave one in your home, maybe you\'ll buy one someday. I\'d like to go through\na case study of how you would write AI software to get a smart speaker to respond to a verbal command such as\n""Hey device, tell me a joke."" For this example, rather\nthan using Alexa, or Okay Google, or Hey Siri, or Hello Baidu as the\nwake word or trigger word, to be more device agnostic, I\'m just going to\nuse ""Hey device"" as the trigger word or wake word to wake up, say, a smart speaker and let\'s say you wanted to tell you a joke. So how do you build\na piece of AI software to understand a command like\nthis and executes on it? These are the steps needed\nto process the command. There can be four steps. Step one is the trigger word\nor the wake word detection. The speaker uses a\nmachine learning algorithm to input the audio\nclip and output. Did they just hear\nthe wake word or the trigger word\n""Hey Device,"" so outputs 0 or one and once it hears the trigger\nword or wake word, once it hears ""Hey device,"" it then has to perform step two, which is speech recognition. So what the software has to do is take the audio of what came after ""Hey device"" and map that\nto ""Tell me a joke."" This is also done with\nmachine learning. Whereas the first step here used an A to B mapping to just tell it that it heard the trigger word, this uses a different\nA to B mapping to map the audio to a text\ntranscript of what you just said, which in this case is\nthe four words: tell me a joke. Now, the algorithm has to figure out what you actually want by saying these four words. And so the third step is\nintent recognition. That means to take\nwhat have you said and to figure out what you\nactually wanted to do. So today\'s smart speakers\nhave a limited set of commands such as they can tell a joke or they can tell the time. So you can say ""Hey device\nwhat time is it."" They can play music. They can sometimes help\nyou make a phone call. They can tell you\nwhat\'s the weather, ""Hey device, what\'s\nthe weather tomorrow?"" So, what intent recognition does is take the four words, the speech recognition\'s output, and use another piece\nof AI software, another A to B mapping, that\ninputs those four words and outputs which of these five\nor other intents do you have. So in this implementation of a machine learning algorithm, the input A is the text transcript ""tell\nme a joke"" and the output B is which of these five types of commands\ndid the user just utter. Of course, your smart\nspeaker may be able to understand even more commands\nthan just these five, in which case B would\nbe whichever of the five or 20 or 100 commands your smart speaker\nknows how to execute. No matter how you ask the smart speaker to tell\nyou a joke, hopefully, the intent recognition components will recognize\nyour intent correctly. So that you can also say\nnot just ""Hey device, tell me a joke,"" but\nalso ""Hey device, do you know any good\njokes"" or ""Hey device, tell me something funny."" Turns out, there are\nlot of ways for you to ask a smart speaker\nfor a joke and a well-designed intent\nrecognition system should recognize most of them. Finally, now that\nyour smart speaker has figured out that you really, really want to hear a joke, the last step is\nthat there will be a software engineer that\nhas written a piece of code to randomly select a joke and to play the joke\nback through the speaker. In other words, they\'ll\nexecute a joke. Just for the record,\nmy favorite joke is why are there\nso many shocking results in AI, because AI is\nthe new electricity. Shocking electricity, get it? Hope you enjoyed\nthat. So, all right, and in seriousness,\nyou can think of the four steps of the algorithm\nas these four steps, where the first step is\ntrigger word detection, second step, speech recognition,\nthen intent recognition, and then finally,\nexecution of what are the command the user asked\nthe smart speaker to execute. So the process of having four steps in an AI system\nlike this or multiple steps, this is sometimes\ncalled an AI pipeline, where you have\nmultiple AI components. Yes, it\'s possible to have machine\nlearning components which process data one step after another and it would not be unusual to have, say, four different teams in\nthe company where each team focuses on one of the components\nof this AI pipeline. That\'s how we often organize projects within a large company. Let\'s now look at\na more complex example. One of you issue a more complex commands\nlike ""Hey device, set timer for 10 minutes."" These are the steps needed\nto process the command. First step, same as before,\nis trigger word detection. So input an audio\nand just let me know when someone said\nthe trigger word hey device. Then speech recognition,\nwhere you input the rest of the audio and transcribe\nthe rest of the sound, the rest of the audio, ""set timer for 10 minutes."" And now, intent recognition has\nto input that text and output that your intent is that you want to set a timer. One difference between ""set timer for 10 minutes"" compared to the earlier example\nof ""tell me a joke"" is that you need to know how long to actually\nset the timer for. So in the execution step, you actually need\nto do two things. One is extract the duration. That means, look at the text, set timer for 10 minutes and pull out the phrase that\ntells you how long to actually set the timer for. And so if the user were\nto say ""Hey device, let me know when\n10 minutes is up,"" then this extract duration step\nwould have to pull out, again, the 10 minute phrase right\nthere. And of course, there are lots of ways to\nask for a 10-minute timer. You can also say, ""let me know\nwhen 10 minutes are up"" or ""set an alarm for 10 minutes\nfrom now"" and hopefully, the intent recognition\nand extract duration components will both be robust enough to recognize\nthat all of these are different ways of asking\nfor a 10-minute timer. Finally, to execute the command, there should be a specialized\nsoftware component in the smart speaker that can start a timer with a set duration. And after it has extracted\nyour intent and the duration, it would just start the timer\nwith that duration, so that the alarm goes off\nat the end of 10 minutes. Today\'s smart speakers\nhave many functions. Other than the two\nwe\'ve talked about of telling jokes and\nsetting a timer, here are some other\nfunctions that many smart speakers today can execute. And the key\nsteps of executing these commands are trigger word or the wake word detection, speech recognition to transcribe\nthe text in the command, intent recognition to\nfigure out which of these functions or which of these commands you\nwant to execute, and then a specialized program to execute whichever\ncommand you uttered. One of the challenges of\nthe smart speaker world is that, if you want your smart speaker to have this many different\nfunctions, say, 20 different functions,\nthen you do need software engineering\nteams to write 20 specialized pieces\nof software. One to play music, one\nto set the volume, one to make calls, one to\nask for the current time, one to convert units like from\nteaspoons to tablespoons, or to answer very simple\nquestions, and on and on. So it\'s actually quite a lot\nof work to write all of these specialized programs to execute all the\ndifferent commands you might want to execute in step four. And smart speakers\ntoday actually do so many things that is\ndifficult for many users to keep straight in their heads exactly what they\ncan and cannot do. So many smart speaker\ncompanies have been investing a lot in user training to try to let users know what are the things that smart speakers can do. Because on one hand, they can\'t do everything. There are lot of\nthings you can\'t ask smarts speakers to do\nsuch as please call all of my three friends\nand see when all of them are able to meet for dinner. So it\'s been\nan ongoing processes of smart speaker companies\nto explain to users what they\ncan and cannot do. Nonetheless, with what they\ncan do using voice to command, these speakers is making life much more convenient\nfor many people. I hope this video gave you\na sense of what it takes to build a complex AI product\nsuch as a smart speaker. In order to help you better understand how\nthese complex products work, let\'s go on to see a second\ncase study of how to piece together multiple AI components to build a self-driving car. Let\'s go on to the next video.'",19,0,1
coursera,deeplearning.ai,ai-for-everyone,case-study-self-driving-car,"b""One of the most\nexciting products of the AI era is\nthe self-driving car. Self-driving cars are also one of the most mysterious things you hear about in AI these days. In this video what I want\ndo is share with you a somewhat simplified description of a self-driving car so that you understand how you\ncan piece together multiple AI components in order to build\nthese amazing things. Let's get started. These\nare the key steps for deciding how to drive\nyour self-driving car. The car will take as input\nvarious sensors such as pictures of what's in front of the car or to\nthe sides or behind, as well as maybe radar or Lidar meaning\nlaser sensor readings. Given these inputs of a picture\nand maybe other things, it then has to detect other cars. So given that, hopefully, you'll figure out\nthere's a car there, as well as where are\nthe pedestrians, because we want to avoid other cars as well as\navoid pedestrians. Both car detection and pedestrian detection can be\ndone with machine learning using input/ output\nor A to B mappings that takes as input the picture and maybe radar and Lidar, sends the inputs and tells us where are the other\ncars and pedestrians. Finally, now that you know where are the other cars and\nwhere are the pedestrians, you can then feed\nthis information into another specialized\npiece of software, it's called a motion planning piece of software that plans the motion or plans the path that you want\nyour car to take, so that you can make progress to your destination while\navoiding any collisions. Once you've planned out\nthe motion for your car, you can then translate that into specific steering angle for your steering wheel and\nacceleration and brake commands, so how much to step\non the gas pedal, and how much to brake in\norder to get your car to move at the desired angle\nas well as speed. Let's look at the three key\nsteps of car detection, pedestrian detection, and\nmotion planning in more detail. Car detection uses\nsupervised learning. So, you've already seen how\na learning algorithm can take as input pictures like these and output the detected cars. For most self-driving\ncars rather than using only a front-facing camera, so a camera that looks forward, also often use cameras\nthat look to the left, to the right as well as to\nthe back so it can detect cars not just to the front\nbut all around it. This is usually done\nusing not just cameras but other sensors as well\nsuch as radar and Lidar. Next is pedestrian detection, and using a pretty similar type of sensors as well as techniques, self-driving cars can\ndetect pedestrians. Finally, I briefly mentioned\na motion planning step. So, what is that?\nHere's an example. Let's say you're driving\nyour car and there's this light blue car\nin front of you. The motion planning\nsoftware's job is to tell you what is the path, shown here in red, you should drive\nin order to follow the road and not\nhave an accident. So the motion planning\nsoftware's job is to output the path as well as speed at which\nyou should drive your car in order\nto follow the road, and the speed should be set so that you don't run\ninto the other car, but you also drive at\na reasonable speed on this road. Here's another example. If there's this gray car parked on the right side\nof the road, so you want to overtake\nthis stopped car, then the motion planning\nsoftware's job is to plot a path like that to let you veer a little\nbit to the left and safely overtake a stopped car. So far I've given a rather\nsimplified description of self-driving as comprising\nmainly these three components. Let's look at\na bit more detail of how an actual self-driving\ncar might work. This is a picture\nyou've seen so far. Input image, radar, or Lidar, sensor readings into car detection and\npedestrian detection, and that is then fed to motion planning to help you\nselect your path and speed. Now in a real self-driving car, you would usually use more than just cameras, radar, and Lidar. Most self-driving cars\ntoday will also use GPS to sense its position as\nwell as accelerometers, sometimes called an IMU,\nthis means accelerometers, and gyroscopes as well as a map because we know that cars are more likely\nto be found on a road, pedestrians are more likely\nto be found on sidewalks, although they are sometimes\nfound on the road as well. All this is usually additional information\nthat's fed in to detect cars and\npedestrians as well as other objects we'll\ntalk about in a second. Rather than just detecting\ncars and pedestrians, in order to drive safely\nyou also need to know where these cars and pedestrians\nare going in the future. So, another common component of self-driving cars is\ntrajectory prediction, where there's another\nAI component that tells you not just the cars and\npedestrians you found, but also where they're likely to go in\nthe next few seconds, so you can avoid them\neven as they're moving. To drive safely\nrequires more than just navigating other\ncars and pedestrians. You also need to know where are the lanes so you might\ndetect lane markings. If there's a traffic light\nyou also need to figure out where's\nthe traffic light, and is it showing a red, yellow, or green signal. Sometimes there are\nother obstacles, such as unexpected\ntraffic cones or maybe there's a flock of geese\nwalking in front of your car. That needs to be detected as well so that your car can avoid even other obstacles than\ncars and pedestrians. On a large self-driving car team, it would not be that unexpected\nto have a team or maybe a few people working on each of the boxes shown here in red, and it's by building all\nof these components and putting them together\nthat you can build a self-driving car. As you can tell both from this rather complex example\nof an AI pipeline, as well as the early example of the four-step AI pipeline\nfor the smart speaker, sometimes it takes\na team to build all of these different components\nof a complex AI product. What I'd like to do in\nthe next video is share with you what are the key roles\nin large AI teams. If you're either a one-person or small AI team now, that's okay, but I want you to\nhave a vision of what building a large AI team, maybe in the distant future,\nmight look like. Let's go on to the next video.""",20,0,1
coursera,deeplearning.ai,ai-for-everyone,example-roles-of-an-ai-team,"b""You saw from\nthe last two videos that some AI products may\nrequire a large AI team, maybe you have a 100 engineers or sometimes many more\nthan a 100 to build. What I would like to do in this video is share with\nyou the typical roles and responsibilities\nof a large AI team like this so you can better understand the types of work needed to build\nthese complex AI products. Now, even if you will be\nworking in a much smaller team, maybe a one or two or five person team for\nthe foreseeable future, I hope this video will still be useful to you because\nI hope it'll give you a sense of the different types of work that an AI\nteam might need to do even if you end\nup executing on this type of work with\na much smaller team. One caveat, because AI\nis evolving so fast, the job titles and various responsibilities are not yet a 100 percent defined, and they are a little bit different across\ndifferent companies. So, your company may use their job titles differently than what I'm presenting here, but I want to share a view how job tittles are often\nused in many companies, so that if you're someday\nbuilding up your own AI team, we hear about these roles\nyou would have at least some deeper\nunderstanding of what these job titles mean. So, let's get started. Many AI teams will have\nSoftware Engineers in them. So, for example, for\nthe smart speaker we needed to write\nspecialized software to execute on the joke or to set a timer or to answer questions\nabout today's weather. So, those are traditional\nsoftware engineering tasks. Or you're building a self-driving\ncar to make sure that your self-driving car software is reliable and doesn't crash. These are software\nengineering tasks. So, it's not uncommon\nfor AI teams to have enlarged fractions\nsometimes 50 percent, sometimes much much much\nmore than 50 percent of Software Engineers in them. The second common role is the\nMachine Learning Engineer. So, Machine Learning\nEngineer might write the software responsible\nfor generating the A to B mapping\nor for building other machine learning algorithms needed for your product. So, they might gather the data of pictures of cars\nand positions of cars, train a neural network or train a deep learning\nalgorithm and work iteratively to make sure that the learning algorithm is\ngiving accurate outputs. Another role that you\nsometimes hear about is the Machine\nLearning Researcher. The typical row of the\nMachine Learning Researcher is to extend the state of\nthe art in machine learning. Machine learning and AI more broadly are still\nadvancing rapidly. So, many companies for\nprofit and non-profit, more have Machine\nLearning Researchers responsible for extending\nthe state-of-the-art. Some Machine Learning\nResearchers will publish papers, but many companies also have Machine Learning Researchers\nthat do research, but are less focused\non publishing. There's one other job title\nthat's sort of in-between these two which is the Applied Machine\nLearning Scientists, which live somewhere between Machine Learning Engineer and Machine Learning Researcher. The Machine Learning Scientists kind of does a bit of both. They are often\nresponsible for going to the academic literature or\nthe research literature and finding the steady VR techniques and finding ways to adapt them to the problem they are facing such as how to take\nthe most cutting edge, trigger where the wicker\ndetection algorithm and adapt that to\nyour smart speaker. Let's look at some more of those. Today, there are a lot of Data Scientists\nworking in industries. The role of Data Scientist\nis not very well defined and the meaning\nis still evolving today. I think one of the primary responsibilities\nof Data Scientists is to examine data\nand provide insights, as well as to make\npresentations to teams or the executives in order to help drive business\ndecision-making. There also Data Scientists\ntoday doing other tasks. So, there are also\nData Scientists today whose work looks more like the Machine\nLearning Engineers, they are described on\nthe previous slide. The meaning of this job title\nis still evolving today. With the rise of big data, there are also more and more\nData Engineers whose main role is to help\nyou organize your data, meaning, to make sure that your data is saved and\nis easily accessible, secure and cost-effective way. So, why is saving data\nsuch as a big deal? Can't you just save them in a hard disk and be done with it. In some companies, data volumes\nhave gotten so big. There's actually quite a\nlot of work to manage them. To give you a sense of the scale, in computer science one MB\nstands for one megabytes and so a typical song on\nyour music player like a typical MP3 file may\nbe a few megabytes, say five megabytes would be\na non unusual MP3 file size. A 1,000 megabytes is\ncalled a gigabyte. A million megabytes is\ncalled a terabyte and a billion megabytes is\ncalled a petabyte. With today's hard disk sizes, saving a few megabytes\nis not a big deal. It's just like a mere MP3 file, but storing a 1,000 megabytes, also called a gigabyte, starts to be a bit slower. A typical hour-long\nmovie that you stream over the internet\nmaybe above gigabytes. So, that's quite a lot of data. To give you a sense of the scale, a self-driving car may collect multiple gigabytes of information every single minute\nof operations. So, it's as if every minute the self-driving car generates enough data to store\nmultiple hour-long movies. So, self-driving cars\nactually generate lot of data and saving the data for many days or weeks or\nmonths or years of operation starts to require\nserious data engineering. A terabyte is 1,000\ntimes bigger than that and a petabyte\nis yet another 1,000 times bigger than that\nof that teams that were responsible for saving\nseveral petabytes of information per day, but other than pretty large internet companies is not that common for a company to generate multiple petabytes of\ninformation per day. As you move down this scale to bigger\nand bigger datasets, it becomes harder and\nharder to make sure that data is stored in\na easy accessible, secure and cost-effective way, which is why Data Engineers become more and more important. Finally, you'll also hear\nsome people referred to AI Product Managers whose job is to help decide what to build. In other words, they\nhelp to figure out what's feasible and valuable. Traditional Product\nManager's job was already to decide what to build as well as sometimes\nsome other roles, but the AI Product\nManager now has to do this in the AI era\nand they're needing new skill sets to figure\nout what's feasible and valuable in light of what\nAI can and cannot do today. Because the field of\nAI is still evolving, none of these job titles are\ncompletely nailed down in the stone and different\ncompanies will use these job titles\nsomewhat differently. But I hope this gives\nyou a sense of some of the different types of\nwork needed to build very complex AI products as well as where some of\nthe job titles are evolving. To finish though, I\nwant to re-emphasize that you can get started\nwith a small team. You don't need a 100 people\nto do most AI projects. So, whether you just have one Software Engineer\nworking with you, or just a single Machine\nLearning Engineer, or just a single Data\nScientists, or maybe nobody, but yourself, if either you or an engineer\nworking with you has taken some online courses on machine learning or\ndeep learning or data science, that's often enough for you\nby yourself or for you and an engineer to start looking at some smaller volumes of data, start drawing\nsome conclusions or start trading some machine learning\nmodels to get going. So, even though I've tried\nto paint here a vision for what a large AI\nteam might look like, even if you have\nonly a small AI team, could be nobody by yourself, I would still encourage\nyou to get started and start exploring what\nthe projects you could do. In this video, you saw what\nan AI team might look like, but when you look at\na bigger company, an AI team doesn't\nlive in isolation. So, how does an AI team fit into a bigger company to help the whole company\nbecome good at AI? You might remember that\nin week one I briefly alluded to an AI\ntransformation playbook, which is a roadmap for\nyou to help a company, to help maybe a great company\nbecome great at AI. Now that you've\nlearned what is AI, how to do AI projects\nand even what AI teams in companies and\nthe competency AI projects and coms may look like, let's return to their AI\ntransmission playbook and go much deeper into the individual steps\nof the playbook so that you can understand\nwhat it takes to help a company over\nmaybe a small number of years become good\nat AI and hopefully become much more valuable and much more effective\nalong the way. Let's go into the AI transmission playbook\nin the next video.""",21,0,1
coursera,deeplearning.ai,ai-for-everyone,ai-transformation-playbook-part-1,"b""How can you help your company\nbecome good at AI? Based on my experience, starting leading the\nGoogle Brain Team as well as\nBaidu's AI group which were respectively the leading forces\nfor helping Google and Baidu become good\nand deeper in AI. I've spent a lot of time\nthinking about what it takes to help\na great company become a great AI company and\nI wound up writing an AI transformation playbook to help other countries\non this journey. In this video, I'd\nlike to share with you the details of the AI\ntransformation playbook so that you can better understand\nwhat it might take for your company to\nbecome good at AI. In case some this\nseems like things that only CEO's need to know, I think that that's not the case, it's useful for everyone in the company to\nunderstand whether it might take for your work to\nimpact not just a few projects but maybe have a bigger impact on\nthe company as a whole. Let's get started. Here are the five steps of the AI\ntransformation playbook. We'll dive into greater detail in a little bit but briefly, step one is for your company to execute pilot projects\nto gain momentum. Start to know what it feels\nlike to work on AI projects. Step two, is to build\nan in-house AI team. Step three, is to provide\nbroad AI training, not just the engineers but to many levels within a company\nincluding executives. Step four, is to develop your\nAI strategy and step five, is to develop internal and\nexternal communications about your company and AI. The way your company will execute the steps may not be totally sequential and so the different\nsteps may overlap. But this numbering gives\na maybe rough sense of the order in which I think\nyou could do these steps. In this video, we will go in\ngreater depth on the first three of these three steps and the next video we'll\ncover steps for and five. Let's start with step one, executing pilot projects\nto gain momentum. If you want your company\nto gain momentum with AI, the most important consideration for the initial project\nor projects, is for them to be\nsuccessful rather than necessarily be\nthe most valuable. For example, when I was\nleading the Google Brain Team, there was still a lot\nof skepticism at that time about deep learning. So, my first internal\ncustomer was Google's speech recognition team and speech recognition is nice to have this is useful\nbut it's actually not the most important\nor valuable project for the company's bottom lines. It's not as valuable as, for example web search\nor online appetizing. But by having my team make the Google Speech team\nmore successful, it started the fly wheel\nturning helps it get momentum because then\nthe peers and the other teams that are brothers and\nsisters the speech team started to see my team\nmake the speech team more successful and so they also started to gain favor in AI\nand wanted to work with us. So, my second\ninternal customer was the Google Maps Team to\nimproving the quality of the maps data\nusing deep learning and with the first\nsecond successes. I then started\nother conversations with the online advertising\nteam for example. So, when selecting\nyour initial project, try to pick something that you think has a good\nchance of success. They can start to fly wheel\nturning even if it may not be the most valuable project that you could eventually\ndo for the company. Because the goal of\nthe first few projects is just to gain momentum\nis also now you see if you can pick\nsomething that can show traction within\nsix to 12 months. So you can start to fly\nwheel turning quickly. Finally, your first one\nor two pilot projects can be either in-house\nor outsource. If you do not yet have a large in-house AI team,\nmight be possible, might even be advisable to outsource some or all of\nyour first couple of AI projects in order to\nget more expertise in house and to let you start\nbuilding that momentum faster. Now, beyond a certain point, you will need your own\nin-house AI team to execute a long-term sequence of maybe many dozens\nof AI projects. So, step two is to build\nan in-house AI team. A lot of companies are\norganized like this, where, there's a CEO and multiple business units\nwhich I'm going to abbreviate with BU that\nreports up to the CEO. So, what I recommend for\nmost companies is to build a centralized AI team and\nthen to take the talent in the matrix organization\nand two matrix them into these different business units\nto support their work. Why essentialize the AI? Let's take an example. Maybe this unit is your\ngift card business unit, and the BU leader may be great at whatever\nhe or she does. They may be greater than\ngift card business. But unless he or she is\nknowledgeable AI and knows how to build retain\nand managing AI team, it may be very difficult for\nthat business unit leader to hire and retain an appropriately manage\ntheir own AI talent. So, in that case, I think you also successfully much higher if you find that AI team leader that can be responsible\nfor consistent company wise standards for\nrecruiting, retention. Have essentialized AI team to give the team the community to talk to each other about how AI applies to\nyour business radical. It may be more efficient\nto take the AI talent in your centralized AI unit\nand matrix them into the gift card\nbusiness unit so that your AI talent can\nwork together with the gift card domain\nexperts in order to develop interesting\nAI projects together. One other responsibility\nfor the AI unit, is to build a company\nwide platforms. If there are software\nplatforms or other tools or data infrastructure that could be useful for the whole company, then a single business\nunit may not have the resources or\nthe incentive to build these company-wide platforms and resources that can support the whole company\nbut essentialized AI team maybe help built these company wide tools or platforms that can help\nmultiple business units. Finally, this new\nAI business unit can be under the CTO, the CIO, the chief data officer\nor the Chief Digital Officer or it could also be\nunder a new chief AI officer. The CAIO chief AI officer is a role that I'm\nseeing more and more often in different\ncompanies but if some other senior executive\nhas the right skill set, they could also\nmanage the AI unit. Finally, one last\nrecommendation, which is that, I think it is hopeful if to get the AI units started\nthe company or the CEO provides funding to\nbuild at the AI unit rather than required AI unit to get funding from\nthe business units. Eventually, after\nthe initial investment and after the initial ramp up, the AI unit will have to show his value that is creating for the business units\nbut having CEO inject funding at the outset\nso they can get going, will often help you get that initial momentum\nmuch faster. In addition to building\nan in-house AI team, I also recommend that you\nprovide broad AI training. Now, for accompany that\nbecome good at AI, is not just that you need\nengineers to know AI, you need multiple people\nat multiple levels of the company to understand how AI interacts\nwith their roles. For example, for executives\nand senior business leaders, I recommend that they learn what AI can do for your enterprise, that they learned\nthe basics of these of AI strategy and they learned enough about AI to make resource\nallocation decisions. So, how much training should executives senior\nbusiness leaders receive? I think that numbers of\nhours of training is not a very good way to measure training but with that caveat, I think you can deliver\na lot of this training with maybe four hours\nor so of training. Leaders of divisions\nwork on AI projects also need to know how to interact\nin their role with AI. I think these leaders\nwould need to understand how to set\nproject directions. So how the conduct technical and business diligence\nhow to make resource allocation decisions at the division level as well as how to track and monitor\nprogress of AI projects. So, this have a training I think would take at least 12 hours. Although, again number\nof hours is not a great metric for tracking\nhow much are they earning. Finally, many\ncompanies are hiring AI talent from outside\nbut I will also not underestimate the importance\nand the impact of training of your existing\nengineering workforce with AI skills for a software engineer to become proficient at AI does take a while so plan for maybe at least a 100\nhours of training. But I'm seeing\nmany companies provide training to help\nengineers learn to build and ship AI software to gather and managed data as was helped them become effective at executing on\nspecific AI projects. The world today does not have nearly enough AI engineers\nand so in-house training is a key part of many companies building up of their\nin-house AI capabilities. Finally, how do you get\nall this training done? Thanks to the rise of\nonline digital content, ranging from of course, online courses to also books and YouTube videos\nand blog posts. There is a lot of great content online about all of\nthese subjects and I think a good CLO\nshould work of experts to curate this type of\ncontent and motivated teams to complete these learning\nactivities rather than necessarily create contents which is much more expensive\nthing to do. So steps one to three of\nthe AI transmission favor. Okay, I hope that\nyour company will be able to start to execute an initial\nprojects, build the team, provide the training\nand really start to get a lot of momentum going in terms of helping you\naccompany you become more valuable or more\neffective using AI. Looking at the broader picture, AI also affects\ncompany strategy and how you align different stakeholders\nincluding investors, employees, customers with this transformation as a company. Let's go on to the next video\nto talk about AI strategy.""",22,0,1
coursera,deeplearning.ai,ai-for-everyone,ai-transformation-playbook-part-2,"b'In the last video,\nyou learned how to execute pilot projects to gain momentum for the in-house AI team and provide broad AI training. But you want your business, not just gain momentum in\nthe short-term using AI, but in the long term be a very valuable and maybe\neven defensible business. What can you do? Let\'s talk about AI strategy as well as perhaps important\nfor some companies, internal and external\ncommunications relative to AI. To recap, this is the five-step AI\ntransformation playbook, and in this video, we\'ll dive more deeply into\nthese final two steps. Step four of the AI\ntransmission playbook is to develop\nan AI strategy, which, I hope for you may mean\nto leverage AI to create an advantage specific to\nyour industry sector. One unusual part of\nthis playbook is that developing the AI strategy is step\nfour not step one. When I shared this with many CEOs consistent request\nof please feedback ago was, can you please put\nthe strategy as step one? Because I want to figure out\nwhat is my company strategy, then I want to find\nthe resources, and then execute on the strategy. But I\'ve found that\ncompanies that tried to define\nthe strategy as step one, before getting your feet wet, before trying out AI knowing what a\nfeasibility AI project. Companies like that\ntend to end up with sometimes very academic\nstrategies that are sometimes not true to life. So, for example,\nI\'ve seen some CEOs copy and paste newspaper\nheadlines into this strategy. We read that data is\nimportant, he say, ""My strategy is to focus on\ncollecting a lot of data, but for your company, that data may or may\nnot be valuable, and may or may not be a good\nstrategy for your company. So, I tend to\nrecommend to companies to start the other steps first, execute the pilot projects. Start building\na little bit of a team. Start providing some training, so that only after you understand AI and understand how it\nmay apply to your business, that you then formulate\nyour strategy. I think this will work much\nbetter for your company than if you tried to\nformulate an AI strategy, before your company including specifically\nthe executive team has some slightly deeper\nunderstanding of what AI can and cannot do for\nyour industry sector. In addition, you might consider\ndesigning a strategy that is aligned with\nthe virtuous cycle of AI. Let me illustrate that with\nan example from web search. One of the reasons that web search is a very\ndefensible business, meaning is very difficult\nfor new entrants to compete with the incumbents with the existing large\nweb search engines, is this: If a company\nhas a better product, maybe a slightly better product, then that web search engine\ncan acquire more users. Having more users means\nthat you collect more data because you get to observe what different users click on when they search for different terms, and that data can be fed into an AI engine to produce\nan even better product. So, this means that the company with somewhat better product, ends up with even more users, ends up with even more data, and does an even\nbetter product with this link being created\nby modern AI technology. It makes it very difficult\nfor a new entrant to break into this self-reinforcing\npositive feedback loop, called the virtuous cycle of AI. Fortunately though, this virtuous cycle\nof AI can be used by smaller teams entering\nnew verticals as well. So, I think today is\nvery difficult to build a new web search engine\nto compete with Google, or Baidu, or Bing, or Yandex. But if you are entering\na new vertical, a new application area\nwhere there isn\'t a entrenched incumbent, then you might really develop\na strategy that lets you be the one to take advantage\nof this virtuous cycle. Let me illustrate\nwith an example. There is a company called\nBlue River that was acquired by John Deere\nfor over US$300 million, and Blue River makes agricultural\ntechnology using AI. So, what they did was build these machines that would\nbe towed behind a tractor, in a big agricultural fields. This machine would\ntake pictures of crops and figure out which is\na crop and which is a weed, and use precision AI to\nkill off just the weeds, but not the crop. So, I knew some of\nthe founders of Blue River while they were Stanford students\nare taking my class. So, to get the project started, they actually just use\nstrap in his and sweat, they use their personal cameras and went out to a bunch of farms, and took a lot of pictures of crops in these\nagricultural fields. So, they started to collect\npictures of heads of cabbage and weeds\naround the cabbage. Once they had enough data, starts off with a small data set, they could train a basic product. The first product, frankly\nwasn\'t that great. It was trained on\na small data set, but it worked well enough to start to convince some farmers, some users to start\nto use their product, to tow this machine\nbehind the tractor, in order to start killing\nweeds for the farmers. Once this thing was running\naround the farms through the process of taking pictures of heads of cabbage\nand killing off weeds, they naturally acquired\nmore and more data. Over the next few years, what they did was\nthey were able to enter this positive\nfeedback loop, where having more data allows you to have\na better product. Having a better\nproduct allows you to convince more\nfarmers to use it. Having farmers use it allows\nyou to collect more data. Over several years, entering\na virtuous cycle [inaudible] , can allow you to collect a huge data asset that then makes your business\nquite defensible. In fact, at the time\nof acquisition, I\'m pretty sure that they had a much bigger data asset\nof pictures of heads of cabbage lying on a field than even\nthe large tech companies had, and does actually\nmakes the business relatively defensible from even the large\ntech companies that have another\nweb search data, but do not have\nnearly as many pictures as this company does of heads of cabbage lying\nin the agricultural fields. One more piece of advice. A lot of people\nthink that some of the large tech companies are great at AI, and I\nthink that\'s true. Some of the largest tech\ncompanies are very good at AI, but this doesn\'t\nmean you need to or should try to compete with\nthese large tech companies on AI in general because\nlot of AI needs to be specialized or verticalized\nfor your industry sector. So, for most companies to\nbe in your best interest to build AI specialized\nfor your industry, and to do great work in AI\nfor your application areas, rather than try to\ncompete or feel like you need to\ncompete left and right with the large tech companies\non AI over the place which just isn\'t true for\nmost companies. Other elements of an AI strategy. We are going to live in an AI power world and\nthe right strategy can help your company navigate these changes much\nmore effectively. You should also consider\ncreating a data strategy. Leading AI companies\nare very good at strategic data acquisition. For example, some of the large consumer facing AI companies will\nlaunch services, like a free email service, or a free photo-sharing service, or many other free services\nthat do not monetize, but allows them to\ncollect data in all sorts of ways that lets\nthem learn more about you, so they can serve you\nmore rather than adds, and thereby monetize\ntheir data in a way that is quite different than direct monetization\nabout that product. The way you acquire data is very different depending on\nyour industry vertical, but I have been involved in what feels like these\nmulti-year chess games, where other corporate\ncompetitors and I are playing multi-year games to see who can acquire\nthe most strategic data assets. You might also consider building a unified data warehouse. If you have 50 different\ndata warehouses under the control of\n50 different vice presidents, then is almost impossible for an AI engineer\nor for a piece of AI software to pull\ntogether all of this data in order\nto connect the dots. For example, if\nthe data warehouse for manufacturing is in a totally different place than the data warehouse for\ncustomer complaints, then how can an AI engineer pull together this data\nto figure out, whether the things that might\nhappen in manufacturing, that causes you to ship\na faulty cell phone, that causes a customer to\ncomplain two months later. So, a lot of leading AI\ncompanies have put a lot of upfront effort into\npulling the data into a single data warehouse\nbecause this increases the odds that a engineer\nor a piece of software, can connect the dots and spot the patterns between how\na elevated temperature in manufacturing today may\nresult in a faulty device that leads to\na customer complaint two months in the future, thus letting you go back to improve your\nmanufacturing processes. There are many examples of\nthis in multiple industries. You can also use AI to create network effects and\nplatform advantages. In industries with winner\ntake all dynamics, AI can be a huge accelerator. For example, take\nthe ride-sharing or the ride-hailing business. Today, companies like Uber, and Lyfts, and Ola, and DiDi, and Grab seemed like they have relatively\ndefensible businesses because they are platforms that connect drivers with passengers, and is quite difficult for\na new entrant to accumulate both a large rider audience and a large passenger audience\nat the same time. Social media platforms like Twitter and Facebook are\nalso very defensible because they are\nvery strong network effects where having a lot of people on one platform makes that platform more attractive to other people. So, it\'s very difficult for\na new entrant to break in. If you are working in\na business with these types of winner take all dynamics\nor winner take most dynamics, then if AI can be used to help\nyou we\'re growing faster. For example, with\na celebrating user acquisition, then that can pass translates into a much bigger chance\nthat your company will be the one to succeed\nin this business vertical. Strategy is very comfy and industry and\nsituation specific. So, it\'s hard to give\nstrategy advisers completely general to\nevery single company. But I hope that these principles give you\na framework for thinking about what might be some key elements of an AI strategy\nfor your company. Now, AI can also fit into more traditional\nstrategy frameworks. For example, Michael Porter, many years ago have written about low cost and\nhigh value strategies. If your company has\na low-cost strategy, then perhaps AI can be used to reduce costs for\nyour business or, if your company has\na high value strategy to deliver really, really valuable products\nwith a higher cost, then you might use AI to focus on increasing the value\nof your products. So, AI capabilities can also help argument existing elements of a broader corporate strategy. Lastly, as you\'re building these valuable and\ndefensible businesses, I hope that you also build only businesses that\nmake people better off. AI is a superpower. This is a very powerful\nthing that you can do to build\na great AI company, and so I hope that\nwhatever you do, you do this only in ways that\nmake humanity better off. The final step of\nthe AI transmission playbook is to develop internal and\nexternal communications. AI can change a company\nand its products, and its important to communicate appropriately with the relevant\nstakeholders about this. For example, this may include investor relations\nto make sure that your investors can\nvalue your company appropriately as an AI company. Investor relations may also includes government relations. For example, AI is\nentering health care, which is a highly regulated\nindustry because government has a legitimate need\nto protect patients, and so for AI to affect these highly\nregulated industries, I think is important for companies to communicate\nwith government, and to work collaboratively with them in public-private\npartnerships to make sure that AI solutions bring people the benefits it can, while also making\nsure that governments can protect consumers\nand protect patients. So, this would be true for health care or be true\nfor self-driving cars, it would be true for finance and many other AI industry verticals. If your products change, then consumer or user education\nwill be important. AI talent is very scarce\ninto this world and so, if you are able to\nshowcase some of your initial successes that could really help with\ntalent and recruiting. Finally, internal\ncommunications is also important if you\'re making\na shift in your company, then many people internally\nmay have worries, some legitimate and some less rational about AI and\ninternal communications, so reassure people where\nappropriate can only be helpful. With these five steps, I hope it gives you\na vision for how you might be the hope a company\nbecome good at AI. If you\'re interested in reading the detailed AI\ntransmission playbook, you can also download it from\nthis landing AI website. I hope you enjoyed\nthese two videos on the AI transmission playbook. I\'ve seen companies become\nmuch more value and much more effective by embracing\nand become good at AI, and I hope these ideas\nthey hope you take a first step toward helping\nyour company good at AI. Having said that, I have also\nseen many common pitfalls, the companies run\ninto when trying to implement AI across\nthe enterprise. Let\'s take a look at some of these common pitfalls in the next video so that hopefully, you can avoid them. Lets go on to the next video.'",23,0,1
coursera,deeplearning.ai,ai-for-everyone,ai-pitfalls-to-avoid,"b""I hope you'll be able\nto use AI to build exciting and valuable\nprojects either for yourself or for\nyour company and make life better both for\nyourself and for others. Along the way, I hope you\nalso manage to avoid some of the pitfalls I've seen\nsome AI teams fall into. Let's go over a five don'ts and dos for if you're trying to\nbuild AI for your company. First one, don't expect\nAI to solve everything. You already know that AI can do a lot but there's also\nlots AI cannot do. Instead, you should be realistic about what AI\ncan or cannot do, given the limitations\nof technology, data, and engineering resources. That's why I think technical\ndiligence in addition to business diligence\nis important for selecting feasible and\nvaluable AI projects. Second, don't just hire two or three machine\nlearning engineers and count solely on them to come up with use cases\nfor your company. Machine learning engineers are a scarce resource but\nyou should instead air the engineer talents with\nbusiness talent and work cross-functionally to find feasible and valuable projects. Is often the combination of the machine-learning\ntalents worked to business talent that can select the most valuable and\nfeasible projects. Third, don't expect AI project\nto work the first time. As you've already seen, AI development is often an iterative process\nso should plan for it through an\niterative process with multiple attempts\nneeded to succeed. Fourth, don't expect\ntraditional planning processes to apply without changes. Instead, you should\nwork with the AI team to establish timeline estimates, milestones, KPIs or metrics\nthat do make sense. The types of timeline\nestimates, milestones, and KPIs or metrics associated\nwith AI projects are a bit different than\nthe same things associated with non AI projects. So, hopefully working with some individuals knowledge about AI can help you come up with better ways of\nplanning AI projects. Finally, don't think you need superstar AI engineers\nbefore you can do anything. Instead, keep building\nthe team and get going with a team you\nhave realizing that there are many AI engineers\nin the world today including many that have learned\nprimarily from online courses. They can do a great job building valuable and feasible projects. If you can avoid\nthese AI pitfalls, you already be ahead of the game compared to many other companies. The important thing\nis to get started. You're second AI project would\nbe better than your first. Your third AI project would\nbetter than your second. So, the important thing\nis to get started and to attempt\nyour first AI project. In the final video for this week, I want to share with you\nsome concrete first steps you can take in AI. Let's go on to the next video.""",24,0,1
coursera,deeplearning.ai,ai-for-everyone,taking-your-first-step-in-ai,"b""This week, you saw\nsome examples of what it's like to build\na complex AI product, like a smart speaker,\nor self-driving car. You also learned\nabout the roles and responsibilities\nof large AI teams, and maybe what it's like\nto build a large AI team, and saw the AI\ntransformation playbook for helping a great company\nbecome a great AI company. In case some of this\nseems daunting, because some of these will take maybe two or three\nyears to execute. In case any of these\nseems daunting, it's okay because\nthe more important thing is that you're able to\ntake the first step. In fact, by taking this course, you've already taken\na great first step. So, I hope that after\nthis course you will take also an equally\ngood second step. So, in this video, I\nwant to share with you some concrete suggestions for the next step you can take toward AI for yourself\nor for your company. Here are some initial steps\nI would urge you to take. Rather than going it alone, consider having friends\nin your company or personal friends outside work\nto learn about AI with you. This could mean,\nasking them to take this course with\nyou or after you, or starting a reading\ngroup to read some books or\nother materials about AI. With what you've learned in\nthis course, you will also, especially if you have\nengineering friends, be able to start\nbrainstorming projects. No project is too small and it's better to start\nsmall and succeed, than to start too\nbig and not succeed. Many projects can be done just by you or perhaps by\nyou and a friend. If you and or a friend take an online course on\nmachine learning, that's enough know-how\nfor you to get started on many potentially very\nvaluable AI projects. In a company, you might also hire a few machine learning or data science people to help out, in addition to providing in-house training to develop\nthe in-house talent. When you're ready to go bigger, you might also try to have your company hire or\nappoint an AI leader, such as a VP of AI or\na Chief AI Officer. But maybe you don't need\na very senior AI leader before hiring just a few\nmachine learning or data scientists people to\nget going more quickly. Finally, I've spoken\nwith a lot of CEOs and Boards about\nAI transformations. If you want your company\nto become greater AI, you would also consider trying\nto discuss with your CEO, the possibility of trying to execute an AI transformation. I think the key question to ask your CEO or your Board is, will your company be\nmuch more valuable and/or much more effective\nif it was great at AI. And if you and they\nthink the answer is yes, that might be a good reason\nfor the company to try to execute\nan AI transformation. The different items\non this list have varying levels of\ndifficulty to execute, but I hope you will\nstart with what you can and then grow\nyour AI efforts from there. I see many people, some technical many\nnon-technical, help their companies learn about AI and start to use\nit effectively. After these videos, you now have the concrete tools\nto do the same. So, I hope you'll take advantage of them and help your company, help yourself, and\nhelp others as well. Finally, we have two more\noptional videos for this week on a survey of\nmajor AI application areas, as well as major AI techniques. So, if you've ever wondered, what do the terms Computer Vision and Natural Language\nProcessing mean? Or, what is\nReinforcement Learning? Or, what is\nUnsupervised Learning? Please take a look\nat these videos, since we'll teach you\nthese application areas, as well as technologies\nin the next two videos. We made these videos optional because they are a little\nbit more technical, but after watching them, you will be able to better\ncommunicate with AI engineers. So, I hope you take a look. Either way, thank you for watching all of\nthese videos this week, and look forward to seeing\nyou in next week's videos.""",25,0,1
coursera,deeplearning.ai,ai-for-everyone,survey-of-major-ai-application-areas-optional,"b'AI today is being successfully applied to image and video data, to language data, to speech data, to many other areas. In this video, you\nsee a survey of AI applied to these\ndifferent application areas and I hope that this may spark off some ideas\nof how you might be able to use these techniques someday for your own\nprojects as well. Let\'s take a look. One\nof the major successes of deep learning has\nbeen Computer Vision. Let\'s take a look at some examples of\ncomputer vision applications. Image classification and\nobject recognition refer to taking as input a picture like that and telling us what\nis in this picture. In this case, it\'d be a cat. Rather than just\nrecognizing cats, I\'ve seen AI algorithms able to recognize specific\ntypes of flowers, AI able to recognize\nspecific types of food and the ability to take as\ninput a picture and classify it into\nwhat type of object, and this is being used\nin all applications. One specific type of image classification\nthat has had a lot of traction is face recognition. This is how face recognition\nsystems today work. A user might register one or more pictures of their face to show the AI\nwhat they look like. Given a new image, the AI system can then say\nis this the same person? Is this you? Or is this a different person so that it can decide a decision, unlock the door or\nunlock the cell phone, unlocked the laptop or something else based on the identity\nof the person. Of course I hope\nface recognition will only be used in ways that respect\nindividuals privacy, we\'ll talk more about AI in\nsociety next week as well. A different type of computer vision algorithm\nis called object detection. So, rather than just tried to classify or recognize an object, you\'re trying to detect\nif the object appears. For example, in building\na self-driving car, we\'ve seen how an AI system\ncan take as input a picture like this and not just tell\nus yes or no, is there a car. Yes or no, is there pedestrian but actually tells\nus the position of the cars as well as the positions of the\npedestrians in this image, and object detection\nalgorithm can also take as input a picture\nlike that and just say, no I\'m not finding any cars or any pedestrians in that image. So rather than taking\na picture and labeling the whole image which is\nimage classification, instead an object\ndetection algorithm will take us input\nan image and tell us where in the picture\ndifferent objects are as was what are\nthe types of those objects. Image segmentation takes\nthis one step further. Given an image like this, an image segmentation\nalgorithm we output, where it tells us not\njust where the cars and pedestrians but tells us\nfor every single pixel, is this pixel part of this car or is this pixel\npart of a pedestrian. So it doesn\'t just draw rectangles around\nthe objects and detects, instead it draws\nvery precise boundaries around the objects that it finds. So, in reading\nx-rays for example, it would be an image\nsegmentation algorithm that could look at\nan x-ray scan or some other image of a human body and\ncarefully segment out, where\'s the liver or\nwhere\'s the heart or where is the bone\nin this image. Computer vision\ncan also deal with video and one application\nof that is tracking. In this example, rather than just detecting the runners\nin this video, it is also tracking in a video whether runners\nare moving over time. So, those little tails below the red boxes show\nhow the algorithm is tracking different people running across several\nseconds in the video. So, the ability to track\npeople and cars and maybe other moving objects\nin a video helps a computer figure out\nwhere things are going. If you\'re using a video camera to track wildlife for example, say birds flying around, a tracking algorithm will\nalso be the helper track individual birds flying across\nthe frames of your video. These are some of\nthe major areas of computer vision and\nperhaps some of them will be useful\nfor your projects. AI and deep learning\nspecifically is also making a lot of progress in\nNatural Language Processing. Natural Language\nProcessing or NLP refers to AI understanding\nnatural language, meaning the language\nthat you and I might use to communicate\nwith each other. One example is text\nclassification where the job of the AI\nis to input a piece of texts such as an email and tell us what is the cause or what is the category of this email, such as a spam or non-spam email. There are also\nwebsites that would inputs a product description. For example, you might write, I have a secondhand cellphone\nfor sale and automatically figure out what is the product category in\nwhich the list is product. So, that would go\nunder cellphones or electronics or if you write, I have a new t-shirt\nto sell then it would list it automatically\nunder clothing. One type of text\nclassification that has had a lot of attention is\nsentiment recognition. For example, a sentiment recognition algorithm can take as input a review like\nthis of a restaurant, the food was good and\nautomatically tries to tell us how many stars\nthis review might get. The food was good as\na pretty good review maybe that\'s four over\nfive-star review. Whereas if someone writes\nservice was horrible, then the sentiment recognition algorithm should\nbe able to tell us that this corresponds maybe\nto a one-star review. A second type of NLP or Natural Language Processing\nis information retrieval. Web search is perhaps the best known example of information retrieval\nwhere you type in the text query and you want the AI to help you find\nrelevant documents. Many corporations will also have internal information\nretrieval systems where you might have\nan interface to help you search just within\nyour company\'s set of documents for something relevant to a query that you might enter. Name entity recognition is another natural language\nprocessing technology. Let\'s illustrate it\nwith an example. Say you have this sentence\nand you want to find all the people names\nin the sentence. So, Queen Elizabeth the\nsecond is a person, Sir Paul McCartney as a person. So, the sentence Queen Elizabeth, the second night\nto Paul McCartney for a service of music at\nthe Buckingham Palace, it would be a name entity\nrecognition system to confine all the people\'s names in the sentence like this. If you want to find\nall the location names, all the place names in\na sentence like that, a named entity recognition\nsystem can also do so. Name entity recognition\nsystems can also automatically extract\nnames of companies, phone numbers, names\nof countries, and so, if you have a large document\ncollection and you want to find automatically\nall the company names, or all the company\nnames the occur together or all the\npeople\'s names, then a name entity\nrecognition system would be the tool you\ncould use to do that. Another major AI application\narea is machine translation. So, for example, if\nyou see this sentence in Japanese, AI [inaudible]. Then hopefully a machine\ntranslation system can input that and output the translation\nAI is in the electricity. The four items on this slide:\ntext classification, information retrieval,\nname entity recognition, and machine translation, are four major categories of\nuseful NLP applications. If you work with an NLP team\nyou may also hear them talk about parsing and part of\nspeech tagging technologies. Let me tell you what these are. Let\'s take the example sentence, ""The cat on the mat"". A part-of-speech tagging\nalgorithm will go through all the words and\ntell you which of these words are nouns, which of these words\nare verbs, and so on. For example, in the\nEnglish language cat and mat in the\nsentence are nouns. So, the part of\nspeech tagger we\'ll label these two words as nouns. According to the theory\nof English language, the word the is a determiner. Don\'t worry if you\'ve never\nheard of a determiner before, this is a word from the theory\nof English language, and the word on is a preposition. So, part of speech\ntagger will label these words like that.\nWell, why do you care? If you\'re building\na sentence classifier for restaurant reviews, then a part-of-speech tagging\nalgorithm would be able to tell you which are the nouns, which\nare the verbs, which are the adjectives, which are the adverbs,\nand so on, and therefore, help your AI system figure out which of the words\nto pay more attention to. For example, you\nshould probably pay more attention to the nouns since those seem like important\nwords. Maybe the verbs. Certainly the adjectives,\nwords like good, bad, delicious are adjectives, and your AI system may learn\nto ignore the determiners. Words like the which\nmay be matter less in terms of how a user is actually feeling\nabout the restaurant. A part of speech\ntagging system is usually not a final application. You hardly ever wake up\nin the morning and think, ""Boy, I wish I could get all the words in\nmy sentence tag."" There\'s often an important\npre-processing step. There\'s often an important\nintermediate step in a longer AI pipeline, where the first step is\npart-of-speech tagging, or parsing, which we\'ll\ntalk about in a second, and then the later steps are an application like\nsentence classification, or machine translation,\nor web search. Now, what is a parser? Given these five words, a parser helps group the\nwords together into phrases. For example, the cat is a phrase, and the mat is a phrase. So, a parser will draw these lines on top\nof the words to say, those words go together. On the mat is another phrase. Finally, the two\nphrases, the cat, as well as on the mat, these two phrases\nare then combined to form the overall sentence. So, this thing that\nI drew on top with the sentence tells you\nwhat words go with what words, and how the different words\nrelate to each other. While a parsing algorithm is also another final\nend-user product, it\'s often a commonly used step to help other AI algorithms. That\'s how classify tags\nare translated, and so on. Modern AI, specifically\ndeep learning has also completely transformed how software processes\naudio data such as speech. How is speech represented\nin a computer? This is an audio\nwaveform of one of my friends saying the phrase\nmachine learning. The x-axis here is time, and the vertical axis is what the microphone\nis recording. What the microphone is\nrecording is little variations, very rapid variations\nin air pressure which your year and your brain\nthen interpret as sound. This plot shows as a function of time, the horizontal axis, how the air pressure\nis changing very rapidly in response to someone say the word\nmachine learning. The problem of\nspeech recognition, also known as speech-to- text, is the problem of taking as\ninputs a plot like this, and figuring out what were\nthe words that someone said. A lot of speech recognition\'s\nrecent progress has been due to deep learning. One particular type\nof speech recognition is trigger word detection\nor wakeword detection. You saw this in\nthe earlier video with having an AI system detect\na trigger word or the wakeword such as Alexa, or Hey Google, or Hey devise. Speaker ID is a specialized\nspeech problem where the task is to listen to someone speak and figure out\nthe identity of the speaker. Just as face recognition helps verify your identity\nby taking a picture, speaker ID can also help verify your identity by\nlistening to you speak. Finally, speech\nsynthesis, also called text-to-speech or TTS is also\ngetting a lot of traction. Text-to-speech is\na problem of inputting a sentence written in text and turning that\ninto an audio file. Interestingly, whereas, text-to-speech is\noften abbreviated TTS, I don\'t often see\nspeech-to-text abbreviated STT. One quick example. Let\'s take the sentence, ""The quick brown fox\njumps over the lazy dog."" This is a fun sentence that\nyou often see NLP people use because this sentence contains every single letter from A to Z. So, that\'s ABC all the\nway up to X, Y, and Z. You can check all 26 letters\nappear in this sentence. Some letters appear\nmore than once. If you parse this sentence\ninto a TTS system, then you might get an\naudio upwards like this, The quick brown fox jumps\nover the lazy dog. Modern TTS systems\nare increasingly sounding more and more\nnatural and human-like. AI is also applied to\nmany applications in robotics and you\'ve already seen one example in\nthe self-driving car. In robotics, the term\nperception means figuring out what\'s in\nthe world around you based on the senses you have, be it cameras, or\nradar, or lidar. Shown on the right is\nthe 3D laser scan or the lidar scan of\na self-driving car as well as the vehicles that\nthis self-driving car the middle has detected in\nthe vicinity of your car. Motion planning refers to finding a path for\nyour robot to follow. So, if your car wants\nto make a left turn, the motion planner might\nplan a path as well as a speed for the car to\nmake a left turn that way. Finally, control refers\nto sending commands to the motors such as\nyour steering wheel motor, as well as your gas pedal, and brake motors in order to make the car smoothly follow\nthe path that you want. On this slide, I\'ll focus on the software and\nthe AI aspects of robotics. Of course, there\'s also a lot\nof important work being done to build hardware\nfor robotics as well. But a lot of the work AI on\nperception, motion planning, and control has focused\non the software rather than the hardware\nof robotics. In addition to these\nmajor application areas, machine learning is\nalso very broadly used. The examples you\'ve seen in\nthis video relate mainly to unstructured data such as\nimages, audio, and text. Machine learning is applied at least as much to structured data, and that means these tables of data some of which you saw\nin the earlier videos. But because unstructured\ndata such as images is so easy for\nhumans to understand, there\'s something very universal, very easy for any person to\nunderstand and empathize with when we talk about an AI system\nthat recognizes a cat. So, the popular press tends\nto cover AI progress on unstructured data much more than it does AI on structured data. Structured data also tends to be more specific to\na single company. So, it\'s harder for people to\nwrite about or understand, but AI on structured data, or machine learning on\nstructured data is creating tremendous economic value today as well as AI on\nunstructured data. I hope this survey of AI application areas\ngives you a sense that the wide range of data that AI is successfully\napplied to today, and maybe this even inspire\nyou to think of how some of these application areas may be useful for your own projects. Now, so far the one AI technique we\'ve spent the most time talking about is supervised learning. That means learning\ninputs, output, or A to B mappings from\nlabeled data where you give the AI system both A and B. But that\'s not the only\nAI technique out there. In fact, the term supervised\nlearning almost invites the question of\nwhat is unsupervised learning, or you might also have\nheard from media articles, from the news about\nreinforcement learning. So, what are all these\nother techniques? In the next video, the final optional video\nfor this week, we\'ll do a survey\nof AI techniques, and I hope that through that\nmaybe you\'ll see if some of these other AI techniques and supervised learning could be useful for your projects as well. Let\'s go on to the final\noptional video for the week.'",26,0,1
coursera,deeplearning.ai,ai-for-everyone,survey-of-major-ai-techniques-optional,"b""There are a lot of AI and\nmachine learning techniques today. And while supervised learning,\nthat is learning A to B mappings, is the most valuable one,\nat least economically today, there are many other techniques\nthat are worth knowing about. Let's take a look. The best known example of unsupervised\nlearning is clustering, here's an example. Let's say you run a grocery store that\nspecializes in selling potato chips. And you collect data on different\ncustomers, and keep track of how many different packets of potato chips\na single customer buys, as well as what's the average price per package that\nperson paid for their potato chips. So you sell some low end, cheaper\npotato chips, as well as some high end, more expensive packets of potato chips. And different people may buy different\nnumbers of potato chip packets, in a typical trip to your grocery store. Given data like this,\na clustering algorithm will say that it looks like you have two\nclusters in your data. Some of your customers tend to buy\nrelatively inexpensive potato chips, but buy a lot of packets. If your grocery store is near a college\ncampus, for example, you may find a lot of college students that\nare buying cheaper potato chips packets, but they sure buy a lot of them. And there's a second cluster in this\ndata of a different group of shoppers that buy fewer packets of potato chips,\nbut buy more expensive packets. A clustering algorithm looks at data like\nthis, and automatically groups the data into two clusters or more clusters, and\nis commonly used for market segmentation. And will help you discover things like\nthat if you have a college student audience that buys a certain type of\npotato chips, and a working professional audience that buys fewer potato chips but\nis willing to pay more. And this can help you market\ndifferently to these market segments. The reason this is called unsupervised\nlearning is the following. Whereas supervised learning\nalgorithms run an A to B mapping, and you have to tell the algorithm what\nis the output B that you want, an unsupervised learning algorithm doesn't\ntell the AI system exactly what it wants. Instead it gives the AI system a bunch\nof data such as this customer data, and it tells the AI to find something\ninteresting in the data, find something meaningful in the data. And in this case, a clustering algorithm\ndoesn't know in advance that there's a college student demographic and\na working professional demographic. Instead, it just tries to find what\nare the different market segments without being told in\nadvance what they are. So unsupervised learning algorithms, given data without any specific design\noutput labels, without the target label B, can automatically find something\ninteresting about the data. One example of unsupervised\nlearning that I have worked on was the slightly\ninfamous Google cat. In this project, I had my team run\na unsupervised learning algorithm on a very large set of YouTube videos, and we asked the algorithm,\ntell us what you find in YouTube videos. And one of the many things that\nare found in YouTube videos was cats, because somewhat stereotypically, YouTube\napparently has a lot of cat videos. But it was a remarkable result that\nwithout telling it in advance that it should find cats, the AI system,\nthe unsupervised learning algorithm, was able to discover the concept\nof a cat all by itself. Just by watching a lot of YouTube\nvideos and discovering that, boy, there are a lot of cats in YouTube videos. It's hard to visualize exactly what an AI\nalgorithm is thinking sometimes, but this picture on the right is\na visualization of the concept of the cat that the system had learned. Even though supervised learning\nis an incredibly valuable and powerful technique,\none of the criticisms of supervised learning is that it just needs so\nmuch labeled data. For example, if you're trying to use\nsupervise learning to get the AI system to recognized coffee mugs, then you may\ngive it 1000 pictures of coffee mug, or 10,000 pictures of mug coffee mug. And that's just a lot of picture of\ncoffee mugs coffee mug we will be giving our AI systems. For those of you that are parents, I can\nalmost guarantee to you that no parent on this planet, no matter how loving and\ncaring, has ever pointed out 10,000 unique coffee mugs to their children, to try to\nteach the children what is a coffee mug. So, AI systems today require\nmuch more labeled data to learn than when a human child or\nthan would most animals. Which is why AI researchers hold a lot of\nhope out for unsupervised learning as way, maybe in the future, for AI to learn much\nmore effectively in a more human like way, and more biological like way for\nmuch less labeled data. Now, we have pretty much no idea\nhow the biological brain works, and so to realize this vision, we'll take major breakthroughs in AI than\nnone of us know yet today how to realize. But many of us hold a lot of hope for\nthe future of unsupervised learning. Having said that,\nunsupervised learning is valuable today. There are some specific applications and\nnatural language processing, for example, where unsupervised learning helps\nthe quality of web search quite a bit, for example. But the value today of\nunsupervised learning is so a lot smaller than the value created\nthrough supervised learning. Another important AI technique\nis transfer learning. Let's look an example. Let's say you bought a self driving car,\nand you've trained your AI\nsystem to detect cars. But you didn't deploy your\nvehicle to a new city and somehow this new city has a lot of\ngolf carts travelling round, and so you need to also build a golf\ncart detection system. You may have your car detection\nsystem with a lot of images, say 100,000 images, but in this new\ncity where you just start operating, you may have a much smaller\nnumber of images of golf carts. Transfer learning is the technology\nthat lets you from a task A, such as car detection, and\nuse the knowledge to help you on a different task B,\nsuch as golf cart detection. Where transfer learning really shine\nis if having learn from a very large dataset of car detection, task A, you can\nnow do pretty well on golf cart detection, even though you have a much\nsmaller golf cart dataset. Because some of the knowledge it\nhas learned from the first task, of what the vehicles look like, what the\nwheels look like, how the vehicles move. Maybe that will be useful also for\ngolf cart detection. Transfer learning doesn't\nget a lot of press, but it is one of the very\nvaluable techniques in AI today. And for example, many computer vision\nsystems are built using transfer learning, and this makes a big difference\nto their performance. You may also have heard of a technique\ncalled reinforcement learning. So, what is reinforcement learning? Let me illustrate with another example. This is a picture of the Stanford\nautonomous helicopter. So it's instrumented with GPS,\naccelerometers, and a compass, so it always knows where it is. And let's say you want to write\na program to make it fly by itself. It's hard to used supervised learning\ninput/output, A to B mappings, because it's very difficult to\nspecify what is the optimal way. What is the best way to fly the helicopter\nwhen it is in a certain, given position. Reinforcement learning\noffers a different solution. I think of reinforcement learning\nas similar to how you might train a pet dog to behave. My family, when I was growing up,\nhad a pet dog. So, how do you train\na dog to behave itself? Well, we let the dog do\nwhatever it wanted to do, and then whenever it behaved\nwell we would praise it. You go, good dog, and whenever it does\nsomething bad you would go, bad dog. And overtime it learns to do more of the,\ngood dog, things, and fear of the bad dog things. Reinforcement learning takes\nthe same principle and applies it to a helicopter or\ntwo other things. So, we would have the helicopter\nflying around in a simulator so it could crash without hurting anyone. But we would let the AI fly\nthe helicopter however once, and whenever it flew the helicopter well,\nwe will go, good helicopter. And when if it crash,\nwe go bad helicopter. And then there was the AI's job to learn\nhow to fly the helicopter to get more of the good helicopter rewards, and fewer\nof the bad helicopter negative rewards. More formally, a reinforcement learning\nalgorithm uses a reward signal, to tell the AI when it's doing well or\npoorly. This means that whenever it's doing well, you give it a large positive number\nto give it a large positive reward. And whenever it's doing really bad job, you send it a negative number\nto give it a negative reward. And it's the AI's job to\nthe automatically learn to behave so as to maximize the rewards. So, good dog responds to giving\na positive number, and bad dog or bad helicopter, corresponds to\nyou giving a negative number. And that the AI will learn to\nget more of the behaviors that results in large positive numbers,\nor in large positive rewards. Let me show you a video of the Stanford\nautonomous helicopter after we did this. This is a video of the helicopter flying\nunder reinforcements learning control. I was the cameraman that day, and when you zoom out the camera,\nyou see the trees pointing the sky. So, we actually gave it a reward signal\nwhich rewarded the helicopter flying upside down. And using reinforcement learning, we built one of the most capable\nautonomous helicopters in the world. In addition to robotic control,\nreinforcement learning has also had a lot of traction in playing games, games such\nas Othello or checkers or chess or Go. You might have heard of AlphaGo, which did a very good job of playing\nGo using reinforcement learning. And reinforcement learning has also been\nvery effective at playing video games. One of the weaknesses of reinforcement\nlearning algorithms is that they can require a huge amount of data. So, if you are playing a video game, a\nreinforcement learning algorithm can play, essentially, an infinite\nnumber of video games. Because it's just a computer\nplaying a computer game, and get a huge amount of data to\nlearn how to behave better. Or for playing games like checkers,\nor other games. It can play a lot of games against itself,\nand for free, get a huge amount of data to feed in\nits reinforcement learning algorithm. In the case of the autonomous helicopter,\nwe had simulator for the helicopter, so it could fly in simulation for\na long time to figure out what works and what doesn't work for flying a helicopter. There so a lot of exciting research work\nbeing done to make reinforcement learning work, even for settings where you\nmay not have an accurate simulator. Whereas harder to get this\nhuge amounts of data. Despite the huge amount of media attention\non reinforcement learning, at least today it is creating significantly less\neconomic value than supervised learning. But there may be breakthroughs in\nthe future that could change that. And AI is advancing so rapidly that all\nof us certainly hope that there will be breakthroughs in all of these\nareas that we're talking about. GANs, or generative adversarial networks,\nare another exciting new AI technique. They were created by my former\nstudent Ian Goodfellow. GANs are very good at synthesizing\nnew images from scratch. Let me show you a video\ngenerated by a team from NVIDIA, that used GANs to synthesize\npictures of celebrities. And these are all pictures of people\nthat had never existed before. But by learning what's celebrities look\nlike from a databases celebrity images, it's able to synthesize all\nthese brand new pictures. There's exciting work by\ndifferent things right now on applying GANs to\nthe entertainment industry. Everything ranging from computer\ngraphics to computer games, to media, and to just making up new\ncontents like this from scratch. Finally, the knowledge\ngraph is another important AI technique that I think\nit's very underrated. If you do a search on Google of Leonardo\nda Vinci, you might find this set of results with this panel on the right\nof information about da Vinci. If you do a search on Ada Lovelace, you'll also similarly find a panel of\nadditional information on the right. This information is drawn from\na Knowledge Graph, which basically means a database that lists people and\nkey information about these people. Such as their birthday,\nwhen they pass away, their bio, and other properties of these individuals. Today different companies have built\nknowledge graphs of many different types of things not just people. But also they built these data\nbases of movies, of celebrities, of hotels, of airports, of scenic\nattractions, and on and on and on. For example, a Knowledge Graph with\nhotel information may have a big database of hotels as well as key\ninformation about these hotels. So that if you look them\nup on the map you can find the right information relatively quickly. The term Knowledge Graph was\ninitially popularized by Google, but this concept has spread\nto many other companies. Interestingly, even though Knowledge\nGraphs are creating a lot of economic value for\nmultiple large companies at this point, this is one subject that is relatively\nlittle-studied in academia. And so, the number of research papers\nyou see on Knowledge Graphs, seems to be disproportionately small, relative\nto the actual economic impact today. But, depending on what industry\nvertical you work in, perhaps some of the techniques for building\nKnowledge Graphs, will also be useful for building a big database of information\nabout something relevant to your company. In this video you learned about\nunsupervised learning, transfer learning, reinforcement learning,\nGANs, and knowledge graphs. It seems like a lot, doesn't it? I hope that some of these ideas will\nalso be useful to your projects, and that knowing what these algorithms are,\nwill make it easier for you to fruitful discussions\nwith AI engineers. In this week we've talk a lot\nabout how AI can affect companies. Maybe how you could use AI\nto affect your company. AI is also having a huge\nimpact on society. So, how can we understand the impact\nthat AI is having on society, as well as make sure that\nwe do ethical things? And that we use AI only to help people and\nmake people better off? And next week we'll talk about AI and\nsociety. Thanks for\nsticking with me up to this point, I look forward to seeing you in the final\nweek of videos for this course.""",27,0,1
coursera,deeplearning.ai,ai-for-everyone,week-4-introduction,"b""Welcome back. You're now in\nthe fourth and final week of AI for everyone and so close\nto finishing this course. AI is a superpower that enables a small team to affect\na huge number of people's lives. So, whether you're builder or user of AI or whether you're just somebody cares about\nAI's impact on society, is important that you\nlearn about these trends, so that you can make\nsure the work you do leave society better off. Let's take a look. AI\nis changing the world, but there's also been a lot\nof unnecessary hype about it. For citizens and\nbusiness leaders and government leaders to\nnavigate the rise of AI, it's importance is\nthat we all have a realistic view of AI. Now, in week one you\nalready started to learn about some of the technical\nlimitations of AI. AI has other limitations as well. For example, AI can be biased and discriminate unfairly\nagainst minorities or against other groups. So, how do we address this? AI technology is also susceptible\nto adversarial attacks, meaning for example, we\nlove our spam filters, it helps the email system\nkeep on functioning. But there are spammers\nthat are trying to attack spam filters. Even as we develop\nnew AI technologies, some of the new\ntechnologies may be susceptible to new types of attacks as well if people are deliberately trying\nto fool the AI. Developed economies such\nas the US and China, are already using AI extensively, but they'll also have\na big impact on developing economies and on\nthe global jobs landscape. Many of these issues are\nimplicated in AI and ethics. To make sure that the work\nwe do in AI is ethical. This is a topic that is complex and there\naren't simple answers. In fact, I think the topic\nof AI and ethics deserves it's own four week\nor much longer course. But I hope this week to\nat least touch on some of the major issues so that\neven as you build or use AI, that you understand some of the major issues associated\nwith the rise of AI. Finally, at the end of this week, we will wrap up AI for everyone. I look forward towards spending these last few videos of you. Let's get started by talking more about having\na realistic view of AI in a way that goes beyond just the technical and\nperformance limitations of AI. Let's go to the next video.""",28,0,1
coursera,deeplearning.ai,ai-for-everyone,a-realistic-view-of-ai,"b""AI is having a huge impact on society and on\nso many people's lives. So, for all of us to\nmake good decisions, it is important that we\nhave a realistic view of AI and be neither too optimistic\nnor too pessimistic. Here's what I mean. Did\nyou ever read the story of Goldilocks and the three bears maybe when you were a kid. Part of the story,\nwas that a bowl of porridge should\nneither be too hot nor too cold and a\nbed should neither be too firm nor too soft. I think we need a similar\nGoldilocks rule for AI, where I think it's important that we be neither too optimistic nor too pessimistic about what AI technology can or cannot do. For example, we should not be too optimistic about\nAI technologies and having an unrealistic view\nof AI technologies may make people think that sentience or super intelligence, artificial, general\nintelligence is coming soon, and we should invest\na lot of resources into defending against\nAI evil killer robots. I think there's nothing wrong with doing a few studies to think about what the distant\nfuture could look like if AI becomes\nsentience some day. Doing basic research\non that is really not a problem but\nwe shouldn't over allocate resources either\nto defending against a danger that realistically\nwill not come for long time. Maybe many decades, maybe\nmany hundreds of years. I think unnecessary fears about sentience,\nsuper intelligence, artificial general intelligence is distracting people from the real issues and\nit's also causing unnecessary fears about\nAI in parts of society. On the flip side, we don't want to be too\npessimistic about AI either. The extreme pessimist view of AI, is that AI cannot do everything. There are some things\nAI cannot do and so, another AI winter is coming. The term AI winter refers to a couple of episodes\nin history when AI had been over-hyped and\nwhen people figured out that AI couldn't do everything that they\nthought it would. It resulted in\na loss of faith and a decrease in investment in AI. One difference between AI now and the earlier winters\nof a few decades ago, is that AI today is creating\ntremendous economic value. We also see a surprisingly\nclear path for it to continue to create even more value in\nmultiple industries. So, the combination of\nthese two things ensures that AI will continue to grow\nfor the foreseeable future. Even though, it is also true that AI cannot do everything. Rather than being too\noptimistic or too pessimistic my story of Goldilocks learn that something in-between\nis just right. I think what we realize now, is that AI can't do everything. In fact, there's a lot\nit cannot do but it will transform industries and society. When you speak with\nfriends about AI, I hope you also tell them about this Goldilocks\nrule for AI, so, that they too can have\na more realistic view of AI. There are many limitations of AI. You have already\nseen earlier some of the performance limitations. For example, given\na small amount of data, a pure AI probably\ncannot fully automate a call center and give very flexible responses to whatever customers are\nemailing you with. But AI has other\nlimitations, as well. One of the limitations of AI\nis that explainability is hard and many high-performing\nAI systems are black boxes. Meaning that it works very well but the AI doesn't know how to explain why it does what\nit does. Here's an example. Let's say you have an AI system look at this X-ray image to diagnose if anything is\nwrong with the patient. In this example, which\nis a raw example, the AI system says that it thinks a patient has\nright-sided pneumothorax. Which means that\ntheir right lung is collapsed. But how do we know if\nthe AI is right and how do you know if\nyou should trust the AI system's diagnosis or not. There's been a lot of work on making AI systems\nexplain themselves. In this example, the heat map\nis the AI telling us what parts of the image it is looking at in order to\nmake this diagnosis. Because it is clearly basing its diagnosis on the right lung and in fact on some key features\nof the right lung. Seeing this image may give\nus more confidence that the AI is making\na reasonable diagnosis. Now, to be fair, humans are also not very good at explaining how we make\ndecisions ourselves. For example, you've already\nseen this coffee mug in the last weeks videos but how do you know\nit's a coffee mug? How does a human look at this and say, that's a coffee mug? You know there are some things\nyou can point to like, there's a room for liquid\nand it has a handle. But we humans are not\nvery good at explaining, how we can look at this\nand decide what it is. But because AI is\na relatively new thing, the lack of explainability is sometimes a barrier\nto its acceptance. Also, sometimes if an AI system isn't working then its ability to explain itself would also\nhelp us figure out how to go in and make\nthe AI system work better. So, explainability is one of the major open research areas. A lot of researchers\nare working on. What I see in practice, is that when an AI team\nwants to deploy something, that AI team must often able to come up\nwith an explanation that is good enough to enable the system to\nwork and be deployed. So, explainability is hotbed, its often not impossible\nbut we do need much better tools to help the AI systems\nexplain themselves. AI has some other\nserious limitations. As a society, we do not want to discriminate against individuals\nbased on their gender, based on their ethnicity and we want people to be treated fairly. But when AI systems are fed data that doesn't\nreflect these values, then an AI can become bias or can learn to discriminate\nagainst certain people. The AI community is working hard and is making\ngood progress on these issues but we're far from done and there's still\na lot of work to do. You'll learn more about biased AI in the next video\nand some ideas on how to make sure\nthat AI systems you work with are less biased. Finally, many AI\nsystems are making economically important\ndecisions and some AI systems are open\nto adversarial attacks, if someone else is deliberately out to fool your AI system. So, depending on\nyour application, it may be important to\nmake sure that you are not open to these types of\nattacks on your AI systems. The issues of AI and\ndiscrimination or AI and bias, as well as the issue of\nadversarial attacks on AI, are important both to you as a potential builder and user\nof AI as well as to society. In the next video, let's dive more deeply into\nthe issue of AI and bias.""",29,0,1
coursera,deeplearning.ai,ai-for-everyone,discrimination-bias,"b""How does an AI system become bias and therefore discriminated\nagainst some people? And how do we try to reduce or eliminate this effect\nin our AI systems? Let's start with example. A group at Microsoft found\nthis remarkable result that when AI learns from\ntext file on the internet, it can learn\nunhealthy stereotypes. To the credit, they also\nproposed technical solutions for reducing the amount of bias\nin this type of AI system. Here's what they found. By having an AI read text\non the Internet, it can learn about words, and you can ask it to\nreason about analogies. So, you can quiz\nthe AI system now that you've read all this text\non the Internet, in the analogy, man is to\nwoman as father is to what? So, the AI will output\nthe word mother, which reflects the way these words are typically\nused on the Internet. If you ask it men is to women, as king is to what? Then the same AI system will say, as King is to Queen, which again seems\nreasonable relative to the way these words are\nused on the Internet. The researchers also found\nthe following result, which is that if you ask it, man is to computer programmer\nas women is to what? That same AI system\nwould output the answer, woman is to homemaker. I think this answer is\nreally unfortunate. Less bias answer would\nbe of words to say, woman is to computer programmer. If we want our AI system\nto understand that men and women can equally\nbe computer programmers, just as men and women can\nequally be homemakers, then we would like it to output man is to\ncomputer programmer, as woman is to\ncomputer programmer, and also man is to homemaker\nas woman is to homemaker. How does an AI system learn to become bias like this from data? Let's dive a bit more into\nthe technical details. The way an AI system stores words is using a set of numbers. So, let's say the word\nman is stored, or we sometimes say represented\nas the two numbers 1,1. The way an AI system comes up with these numbers is through statistics of how the word\nman is used on the Internet. The specific process\nfor how these numbers are computed is quite complex and I won't\ngo into that here. But these numbers represent the typical usage of these words. In practice, an AI might have hundreds or thousands of\nnumbers to store a word, but I'm just going\nto use two numbers here to keep the example simpler. Let me take this number\nand plot it on a chart. So, the word man,\nI'm going to plot at the position 1,1 on\nthe figure on the right. By looking at the statistics\nof how the words or how the phrase\ncomputer programmer is used on the Internet, the AI will have\na different pair of numbers, say 3,2, to store or to represent the phrase\ncomputer programmer. Similarly, by looking at\nhow the word woman is used, it'll come up with\na different pair of numbers, say 2,3, to store or to\nrepresent the word woman. When you ask the AI system to\ncompute the analogy above, man is to computer programmer, as women is to what? Then what the AI system will do, is construct a parallelogram\nthat looks like this. It will ask, what is the word associated with the position 4,4? Because it will think that is\nthe answer to this analogy. One way to think about this mathematically is\nthat the AI thinks the relationship of man to computer programmer is that\nyou start from the word man, go two steps to the right,\nand one step up. So, to find the same answer\nfor women is to what,? You would also go two\nsteps to the right, and one step up. Unfortunately, when these numbers are derived from texts\non the Internet, and the AI system\nfinds that the way the word homemaker is used on the internet causes it to be\nplaced to the position 4,4, which is why the AI\nsystem comes up with this bias analogy. AI systems are already making\nimportant decisions today, and will continue to do\nso in the future as well. So, bias matters. For example, there's\na company that was using AI for hiring, and found that their hiring too discriminated against women. This is clearly unfair, and so this company\nshut down their tool. Second, there're also some facial recognition\nsystems that seem to work more accurately for light-skinned and\ndark-skinned individuals. If an AI system is\ntrained primarily on data of lighter\nskin individuals, then it will be more accurate\nfor that category of individuals to the extent that\nthese systems are used in, for example, criminal\ninvestigations, this can create a very\nbiased and unfair effect for dark-skinned individuals. So, many face recognition teams\ntoday are working hard to ensure that the systems do not exhibit this type of bias. There have also been AI or statistical\nloan approval systems that wound up discriminating against some minority\nethnic groups, and quoted them\na higher interest rate. Banks have also been working\nto make sure to diminish or eliminate this type of bias\nin their approval systems. Finally, I think it's important\nthat AI systems do not contribute to the toxic effect of reinforcing\nunhealthy stereotypes. For example, if\nan-eight-year old girl goes to an image search engine and searches for\nChief Executive Officer, if they see only pictures of\nmen or if they see no one that looks like themselves\neither by gender or ethnicity, we don't want them\nto be discouraged from pursuing a career that might lead her to someday be a Chief Executive Officer\nof a large company. Because of these issues, the AI community has put a lot of effort into combating bias. For example, we're\nstarting to have better and better\ntechnical solutions for reducing bias in AI systems. In the example you\nsaw at the start of this video of the AI\noutputting buyers analogies. Simplifying the\ndescription a little bit, researchers have found\nthat when an AI system learns a lot of different numbers with which to store words, there are few numbers that\ncorrespond to the bias. If you zero out those numbers, just set them to zero, then the bias diminishes\nsignificantly. A second solution\nis to try to use less bias and or\nmore inclusive data. For example, if you are building a\nface-recognition system, and make sure to include data\nfrom multiple ethnicities, and all genders, then your system will be less\nbiased and more inclusive. Second, many AI teams\nare subjecting their systems to\nbetter transparency and or auditing processes, so that we can constantly check what types of bias, if any, these AI systems are exhibiting, so that we can at least recognize the problem\nif it exists, and then take steps\nto address it. For example, many face\nrecognition teams are systematically checking how accurate their system is on different subsets of\nthe population to check whether, it is more or less accurate on dark-skinned versus\nlight-skinned individuals, for example. Having transparent\nsystems as well as systematic auditing\nprocesses increases the odds that will at least\nquickly spot a problem, in case there is one,\nso that we can fix it. Finally, I think having a diverse workforce will\nalso help reduce bias. If you have a diverse workforce, then the individuals in\nyour workforce are more likely to be able to\nspot different problems, and maybe they'll\nhelp make your data more diverse and more\ninclusive in the first place. By having more unique points of view as you're\nbuilding AI systems, I think there's a hope all of us create less bias applications. AI systems are making really\nimportant decisions today, and so the bias or potential\nfor bias is something we must pay attention to\nand work to diminish. One thing that makes\nme optimistic about this is that we actually\nhave better ideas today for reducing bias in AI than reducing bias in humans. So, while we should\nnever be satisfied until all AI bias is gone, and it will take us quite a\nbit of work to get there, I'm also optimistic\nif we could take AI systems that started off with a level similar to humans, because it learned from humans, and we can cut down the bias from there through technical\nsolutions or other means, so that as a society, we can hopefully make the decisions we're making\nthrough humans or through AI rapidly become more\nfair and less biased. In addition to\nthe problem of bias, one of the other limitations\nof AI is that it can be open to\nadversarial attacks. In the next video, you'll learn what are\nadversarial attacks, as well as some of\nthe things you could do to guard against them. Let's go on to the next video.""",30,0,1
coursera,deeplearning.ai,ai-for-everyone,adversarial-attacks-on-ai,"b'Even though modern AI\nis incredibly powerful, one of the limitations of\nmodern AI technologies especially deep learning is that sometimes it can be fooled. In particular, modern\nAI systems are sometimes susceptible\nto adversarial attacks, if someone else sets out deliberately to fool\nyour AI system. Let\'s take a look. Let\'s\nsay you give an AI system, this picture of a bird and\nask it to classify it. The AI system outputs that\nthis is a hummingbird. But lets we make a minor\nperturbation to this image. By minor perturbation, I mean\nto change the pixel values just a little bit and almost imperceptible change\nto most people. The same AI system then says\nthis is instead a hammer. Now, to person you might say, ""How is this even possible, the picture on\nthe right looks almost identical to the picture\non the left?"" In fact, the changes are almost imperceptible\nto the human eye. But an AI system sees the world very differently\nthan you and I do. So, it is susceptible to if an adversary makes\nchanges to a picture that could be imperceptible to you and me but their results in fooling the AI into thinking this picture is something\ntotally different. We call this an adversarial\nattack on an AI system. In computer security,\nan attack against a secure system means\nan attempt to make it do something other than\nwhat it was intended to do. In the same way, an adversarial\nattack on an AI system is an attempt to make\nit do something other than what it\nwas intended to do, such as trying to fool it into outputting\nincorrect classifications. Here\'s another example. Here\'s a picture of a hare with just a minor perturbation or a small change to\nthe pixel values, AI instead says this is a desk. The fact that computers see pictures differently\nthan humans do, gives it advantages\nand disadvantages. For example, computer systems are much better than you and me at reading barcodes\nand QR codes. But the way that\ndeep learning systems work also opens us up to these particular forms of attack which no human\nwould be fooled by. Today, AI is being used\nto filter out spam, to try to filter out hate speech, and attacks like\nthese will diminish the effectiveness\nof such filters. Now, the attacks on this slide require the ability to\nmodify an image directly. For example, a spammer might\ndirectly modify an image before they tried to upload it to a website or send it in an email. There are some attacks\nthat work by changing the physical\nworld as well. For example, a group at\nCarnegie Mellon University was able to design a funky pair\nof glasses like this. So, that when does man\nwears this pair of glasses, he can fool an AI system into thinking that he is\nactress Milla Jovovich. I think it\'s remarkable that just wearing a pair\nof glasses like this can fool an AI\nsystem into thinking that this man is\na well-known actress. A different group of\nresearchers from UC Berkeley, University of Michigan\nand other universities, showed that if you affects stickers like these\nonto a stop sign, you can fool an AI system into not seeing\nthe stop sign at all. It things there\'s something else there other than a stop sign. One interesting thing about\nthis example is that it looks like the stop sign just had some graffiti applied\non top of it. Most humans will still see this as a stop sign quite easily. But if you have\na computer vision system built into a self-driving\ncar for example, it would be really unfortunate if the car doesn\'t see\nthe stop sign anymore, because of these stickers\napplied on top of it. One last example, this one from a group of\nresearchers at Google, is if you show\na AI system this picture, it will say this is a banana. But what the researchers\ndid was design a sticker, so that if you place\nit into the scene, it misclassifies this banana. Let me show you the video\nthat the researchers made. Shown on the left is the classifier input and shown on the right is\nthe classifier output, where it thinks is\nvery likely a banana and maybe there\'s\na small chance is a slug. Looks okay. Let\'s see\nwhat happens when you put a sticker or put\na small patch into the scene. When the sticker is\nplaced in the scene, the AI system now is\nalmost certain that this picture is\na picture of a toaster. One interesting aspect\nof this work is that the authors of this paper cited at the bottom\nof this slide, actually published in their paper a picture of their sticker. So, that anyone in\nthe world could hypothetically\ndownload their paper, print out the sticker, and stick it somewhere\nif they want to fool an AI system into thinking there\'s a toaster\nwhere there isn\'t. Now, I do not support\nanyone attacking AI systems to fool them into thinking that toasters\nwere there or none, but this shows unfortunately\nhow the ease with which one could hypothetically\nattack these AI systems. What can we do to defend against these\nadversarial attacks? Fortunately, the AI world\nhas been working on new technologies to make\nthem harder to attack. The defenses tend to\nbe very technical, but there are ways of\nmodifying neural networks and other AI systems to make them\nsomewhat harder to attack. One downside is that these\ndefenses do incur some cost. For example, the AI system\nmay run a little bit slower. But this is an area of ongoing research\nand we\'re far from heavy adversarial\ndefenses that seem good enough for all of\nthe important applications that we want to apply AI to. For many AI systems, there may be no incentive for\nanyone to try to attack it. For example, if you\'re running a automatic visual\ninspection system to check if coffee mugs have\nscratches in your factory. Maybe not that many\npeople who have any incentive to try to fool your system into thinking a scratch coffee mug doesn\'t\nhave a scratch on it. But there are going to be AI applications where there\nwill be attacks as well. For those applications,\nI think that similar to spam versus anti-spam, where spammers are trying\nto get the spam email through and spam filters\nare trying to stop them. I think there will be\napplications where we will be in an arms race, as a AI community is building\nup defenses and there is a community of attackers\ntrying to fool our defenses. In my experience\nbuilding AI systems, one of the few times I felt I was at total war\nwith someone else was when I was leading anti-fraud teams\nfighting against forces. There is unfortunately\nsome amount of fraud on the Internet where people are trying to steal money and payments systems or\ncreate fraudulent accounts. The times I worked on\nanti-fraud systems was one of the few times that it really\nfelt like a zero-sum game, had an adversary, we would put up a defense and\nthey would react. They would launch an attack,\nmy teams had to react. Sometimes even hours\nto defend ourselves. So, I think over the next few years even as\nAI technologies evolves, there will be verticals\nlike that, like spam, like fraud where\nteams will be at war. In what my feel like a zero-sum\ngame against adversaries. Having said that, I also\ndon\'t want to over-hype the potential damage for\nadversarial AI systems. It is really important\nfor some applications. But there are also plenty of\nAI applications which are less likely to be subject\nto adversarial attacks. Now, in addition to\nadversarial attacks, AI unfortunately can\nalso be used for some adverse or\nsome negative use cases. Let\'s take a look at some\nof them in the next video as well as what we should\ndo to address them. Let\'s go on to the next video.'",31,0,1
coursera,deeplearning.ai,ai-for-everyone,adverse-uses-of-ai,"b""AI is incredibly powerful and the vast majority of users\nof AI are making people, companies, countries,\nsociety better off. But there are a few adverse\nuses of AI as well. Let's take a look\nat some of them and discuss what we\ncan do about them. AI technology has been used\nto create deepfakes and that means to synthesize video of people doing things that\nthey never actually did. The website BuzzFeed,\ncreated a video of former US President Barack Obama saying things that he never did. BuzzFeed was transparent about it and when they\npublish the video, it was really obvious\nbecause he told everyone that this\nis a fake video. But if this type of\ntechnology is used to target an individual and\nmake others think they said or did things\nthey never actually did, then these individuals could\nbe harmed and left to defend themselves against\nfake video evidence of something they\nnever actually did. Similar to the war of\nspam versus anti-spam, there is AI technology today for detecting if a video\nis a deepfake. But in today's world\nof social media, where a fake could spread around the world faster than\nthe truth can catch up, many people are concerned\nabout the potential of deepfakes to harm individuals. There's also a risk of\nAI technology being used to undermine democracy\nand privacy. For example, many governments\naround the world are trying to improve\ntheir citizens' lives, and have a lot of respect\nfor the government leaders that are uplifting\ntheir citizens. But there are also\nsome oppressive regimes that are not doing the right things\nby their citizens, that may seek to use\nthis type of technology to carry out oppressive\nsurveillance of their citizens. While governments have\nillegitimate need to improve public safety\nand reduce crime, there are also ways\nof using AI that feel more oppressive than uplifting\nof its own citizens. Closely related to this, is the rise of fake comments\nthat AI can generate. Using AI technology is now possible to generate\nfake comments. Either on the commercial side, fake comments of products, or in political discourse, fake comments about\npolitical matters in the public discourse, and to generate fake comments\nmuch more efficiently than if you only had\nhumans writing them. So, detecting such fake\ncomments and weeding them out, is an important technology\nfor maintaining trust in comments that we\nmight read online as well. Similar to the battles\nof spam versus anti-spam and fraud\nverses anti-fraud, I think that for all\nof these issues, there may be a competition on both sides for\nquite some time to come. Similar to the battles of\nspan versus anti-spam, fraud versus anti-fraud, I'm optimistic about how\nthese battles will play out. Because if you take\nspam filter as an example, there are a lot more\npeople that are motivated to make sure\nspam filters do work, that anti-spam does work. Then there are the smaller\nnumber of spammers trying to get this spam\nin to your inbox. Because of this, there's\na lot more resources on the side of anti-spam\nthan on the side of spam. Because society actually\nfunctions better, if anti-spam and\nanti-fraud works out well. Because of this, even though the AI community\nstill has a lot of work to do to defend against\nthese adverse use cases. Because society is\ngenuinely better off, if we could have\nonly good uses of AI, I am optimistic that\nthe balance of resources means that the side\nof good will prevail, but it will still take\na lot of work from the AI community over\nmany years to come. Next, AI is also having a big impact on\ndeveloping economies. Let's take a look at\nthat in the next video.""",32,0,1
coursera,deeplearning.ai,ai-for-everyone,ai-and-developing-economies,"b""Every time there is a major\ntechnological disruption such as of AI, it gives us a chance\nto remake the world. AI is a very advanced technology, yes affecting both\ndeveloped economies and developing economies. So, how could we make sure that even as AI creates\ntremendous wealth, that it uplifts all nations?\nLet's take a look. There's been a fairly\npredictable roadmap, almost a ladder, that many developing\neconomies have successfully executed\nin order to help the citizens gain\nskills and climb to higher levels of wealth. All the nations started off with low-end\nagricultural products, exporting crops, and then moved to low-end textile manufacturing, such as clothing manufacturing. Then as a population starts\nto gain a bit more wealth, become a bit more healthy, move on to low-end\ncomponents manufacturing, such as making less\nexpensive plastic parts. So, then moving on to low-end\nelectronics manufacturing, to high-end electronics\nmanufacturing, maybe to automotive\nmanufacturing, and so on. There's been this step-by-step progression by which\ndeveloping economies can hope their citizens gain skill and become\ndeveloped economies. One of the problems that\nAI could cause is that, a lot of the lower rungs\non the ladder are particularly susceptible\nto automation through AI. For example, as factories\nbecome more automated, or as agriculture\nbecomes more automated, there may be less needed\nand therefore lesser opportunities for\nlarge members of a population of some of these developing\neconomies to get onto the lower rungs of the economic ladder from which\nthey would then climb up. So, if we're knocking out some of the lower rungs of\nthe ladder through AI, through AI providing\nautomation on steroids, then it is incumbent on us to see if AI can also\ncreate a trampoline to hope some developing economies jump onto the trampoline and bounce maybe even more quickly to the highest rungs\nof this ladder. With the rise of\nearlier waves of technology, many economies have shown\nthat they can leapfrog developed economies and jump straight to a more\nadvanced technology. For example, here in the United States most\nof us had landlines. Phones that were connected\nvia wire to the wall. Because so many of us\nhad landline phones, that actually took a while to transition to wireless\nmobile phones. In contrast, many\ndeveloping economies including India and China\nbut many others as well, didn't bother to build\nnearly as many land lines, but skip straight\nto mobile phones. So, this was a leapfrog where developing economies\njumped straight over the earlier generation of technology and didn't bother to lay so many physical cables at every person's\nhouse and instead jumped straight to mobile phones. We're seeing a similar thing\nwith mobile payments where many developed economies have a mature credit card\nsystem and that actually is slowing down their adoption of mobile or cell phone payments compared to some developing economies\nwhich do not already have entrenched incumbents in\nthe credit card industry. I'm also seeing rapid adoption of online education in\ndeveloping economies. In countries that\nhave not yet built all the many many\nphysical schools and universities that they need, many educational leaders\nand governments are seeking ways to more\nquickly embrace online education compared to some of the developed\neconomies that have this built-up\nphysical infrastructure for in-person education. While developed\neconomies are also rapidly embracing all\nof these technologies, one of the advantages\nof developing economies is that without an entrenched\nincumbents system, perhaps there are areas that they could build even faster. The US and China are leading, and the UK and Canada and\na few other countries also have vibrant AI communities. But because AI is still\nso immature today, I think all AI communities\nare still immature. This means that even though AI is creating tremendous\neconomic value, most of the value to be created is still\noff in the future. This gives every nation\nan opportunity to be a large part of creating this value that hasn't\nbeen created yet, and even capturing\na large piece of it. So, I hope every nation\ncan figure out how to effectively use AI to continue\nto hope his citizens. My advice to developing\neconomies is to focus on the AI to strengthen\na country's vertical industries. For example, I think\nmost countries today should not try to build\ntheir own web search engine. There are already\ngreat web search engines and that was last\ndecades competition. Instead, if a country has a very strong vertical industry in say coffee bean manufacturing, then that country is\nactually uniquely qualified to do work in AI for coffee manufacturing\nand building AI for coffee manufacturing will even further strengthen what\nthat country is already good at. So, rather than needing for\nevery country to compete with the US and China\non AI in general, I would advice\nmost countries to use AI to strengthen what\nthat country is good at and what that country\nwants to do in the future. Finally, public-private\npartnerships, meaning governments and\ncorporations working together, can really help accelerate a vertical industry's\nAI developments. In highly regulated sectors, ranging from healthcare to transportation like\nself-driving cars to finance, there are certain\noutcomes that we want and certain outcomes\nthat we don't want. Governments that are\nthoughtful about crafting derived\nregulations to protect citizens while at\nthe same time enabling the adoption of AI solutions\nto these industries, will see faster local\neconomic growth as well as faster technology development\nwithin their country. Finally, developing\neconomies should invest in education because AI\nis still so immature. There's still plenty of room for every nation to\nlearn more about AI, maybe even build up its own\nAI workforce and participate in a significant way in this AI powered world\nthat we're building. In moments of\ntechnological disruption, leadership matters. Here\nin the United States, we once trusted our governments to put a man on\nthe moon and it worked. With the rise of AI,\nit creates a space, and in some countries\na need for leadership, whether in the government\nlevels, or in companies, or in education to\nhelp a country enter the AI era and embrace and adopt AI to keep on\nlifting up its citizens, and perhaps even keep on lifting up other\npeople worldwide. In this video, we've\ntouched briefly on the issue of AI and jobs. This is an important topic\nthat is widely discussed in many\ncountries right now. Let's go on to the next video, to take a deeper look\nat AI and jobs.""",33,0,1
coursera,deeplearning.ai,ai-for-everyone,ai-and-jobs,"b'AI is automation on steroids. Before the rise of modern AI, automation had already had a huge impact on a lot of jobs. With the rise of AI, the set of things we\ncan now automate is suddenly much bigger than before, and so this is also having an accelerating impact on jobs. How many jobs will be displaced? How many new jobs\nwill be created? I don\'t think anyone has a firm answer to\nthese questions yet, but let\'s take a look\nat some studies to try to understand\nwhat may be ahead. McKinsey Global Institute did a study in which\nthey estimated that 400 to 800 million jobs will be displaced by\nAI automation by 2030. These are very large numbers. On the flip side though, the same report also\nestimates that the number of jobs created by AI\nmay be even larger. There have been\nmany studies other than the McKinsey\nGlobal Institute\'s one. There is a range in\nthese estimates of numbers of jobs displaced\nand numbers of jobs created. For example, just focusing\non the United States, the numbers on this slide\nare worldwide, but just focusing on\nthe United States, PwC estimates about 16 million\njobs displaced by 2030. Bank of England estimates 80 million jobs\ndisplaced by 2035. So no one can predict with certainty exactly what\nwill happen in 2030, but there is a sense\nthat the impact of jobs worldwide will\nbe significant. I hope you find it as\nencouraging as I do though that AI is creating many jobs even as it\nis displacing some. I think many of\nthe jobs of the future, we may not even\nhave names for yet, be it drone traffic optimizer or 3D-printed clothing designer\nor as in healthcare, we\'ll have custom\nDNA-based drug designers. So even though there is concern\nabout AI displacing jobs, there is also hope\nof many new jobs, maybe even more new jobs\nbeing created in the future. Now you might wonder, how do we estimate how many jobs are\nlikely to be displaced? One typical way that these studies are\ncarried out would be to take a job and think of the\ntask that make up the job. For example, you might look at the task\nof the radiologist does or look at all the tasks\nthat a taxi driver does. Then for each of the task, estimate how amenable it is\nto automation through AI and if a job comprises mainly task that are\nhighly automatable, then the risks of the job being\ndisplaced will be higher. Most AI engineers find it\nmore useful to think of AI being applied to task\nrather than to people\'s jobs. But this framework allows us to use AI\'s ability\nto automate tasks to estimate how many jobs are likely to be displaced. So which are the jobs\nthat are most likely or least likely to be displaced\nthrough AI and automation? The OECD, a well-respected\nintergovernmental body, looked at the number of\njob types to estimate which of them are most and\nleast likely to be automated. The future is hard to\npredict with certainty, but perhaps not surprisingly, many other jobs that comprise more routine repetitive work are more amenable to automation, whereas many of the tasks\nthat are less repetitive, less routine or that involve more social interaction\nwith people maybe less susceptible\nto automation. How do we hope\ncitizens and nations navigate the coming impact\nof AI on jobs? Here are some solutions. First, conditional basic income. You may have heard of\nuniversal basic income in which a government would pay citizens\nwith no strings attached. I think people do\ndeserve a safety net. For individuals that are unemployed but are able to learn, I think a more effective\nversion may be conditional basic income\nin which we do provide the safety net but\nincentivize them to keep on learning and keep on investing\nin their own development. By providing a structure to help people that can learn do so, this will increase the odds that those individuals can\nre-enter the workforce, contribute to themselves,\ntheir families and to society, as well as to the tech space\nthat is paying for all this. Second, building\na lifelong learning society. By virtue of your taking\nthis course right now, you may already be part of this lifelong learning society. The old model of\neducation where you go to college for four years and then cost for\nthe remaining 40, that just does not work anymore into these rapidly\nchanging world. Through governments,\ncompanies and individuals realizing that all of us\nneed to keep on learning, this increases the odds that everyone will\nbe better position, even as jobs may go away. So, take advantage of the new\njobs being created as well. In the future, even after\ncompleting college, I think most individual\nshould keep on learning throughout\ntheir whole lives. Finally, there are\npolitical solutions being explored as well. Everything from incentivizing\nor helping with new job creation to legislation to make sure that\npeople are treated fairly. I hope that society\nwill figure out the right political solutions to navigate the coming impact\nof AI on jobs as well. One question now sometimes asked is what should you do if\nyou want to work in AI? Recently, a radiologist\nresident served radiologists near the start of his career. He actually asked me. He said, ""Hey, Andrew, I\'m hearing a lot about the coming impacts\nof AI on radiology."" He said, ""Should I quit my profession and just learn\nAI and do AI instead?"" My answer to him was\nno. You could do that. You can actually\nquit whatever you are doing and pick\nup AI from scratch. It is entirely\npossible to do that. Many people have done that. This one other alternative that you could consider\nthough, which is, I said to this radiology\nresident consider doing work in AI plus radiology because with your\nknowledge of radiology, if in addition you learned\nsomething about AI, you would be better\npositioned to do work at the intersection of radiology and AI than most other people. So, if you want to\ndo more work in AI, it is possible in\ntoday\'s world to learn AI from scratch through online courses\nand other resources. But if you take whatever you\nare already knowledgeable in and learn some AI and\ndo your area plus AI, then you might be more uniquely qualified to do\nvery valuable work by applying AI to whatever area you are\nalready an expert in. So, I hope this video helps you navigate the coming impacts\nof AI in jobs. Let\'s go on to the next and\nfinal video of this course.'",34,0,1
coursera,deeplearning.ai,ai-for-everyone,conclusion,"b""Congratulations on coming to\nthe last video of this course. AI is a super power, and understanding it allows you to do things\nthat very few people on the planet can. Let's summarize what you've\nseen in this course. In the first week, you learned\nabout AI technology, what is AI and what is machine learning? What's supervised learning,\nthat is learning inputs, outputs, or A to B mappings. As well as what is data signs, and how\ndata feeds into all of these technologies? Importantly, you also saw examples\nof what AI can and cannot do. In the second week, you learned what\nit feels like to build an AI project. You saw the workflow of machine learning\nprojects, of collecting data, building a system and deploying it, as well as\nthe workflow of data science projects. And you also learned about carrying\nout technical diligence to make sure a project is feasible, together with\nbusiness diligence to make sure that the project is valuable, before you commit\nto taking on a specific AI project. In the third week, you learned how such AI projects could\nfit in the context of your company. You saw examples of complex AI products,\nsuch as a smart speaker, a self-driving car, the roles and\nresponsibilities of large AI teams. And you also saw the AI transmission\nplaybook, the five step playbook for helping a company become\na great AI company. I hope these materials throughout\nthese first three weeks can help you brainstorm AI projects or think about how you might want to use AI\nin your company or in your organization. In this week, week four,\nyou learned about AI and society. You saw some of the limitations of AI\nbeyond just technical limitations, and also learned about how AI is affecting\ndeveloping economies and jobs worldwide. You've learned a lot in these four weeks,\nbut AI is a complex topic. So I hope you will keep on learning, whether through additional online courses,\nthrough Coursera or deeplearning.ai, or books, or blogs,\nor just by talking to friends. If you ever want to try your\nhand at building AI technology, it is now easier than\never to learn to code and learn how to implement AI\ntechnology through these resources. If you'd like to keep on\nreceiving information about AI, you can also sign up for\nthe deeplearning.ai mailing list, by going to the deeplearning.ai\nwebsite and signing up there. I'll occasionally send you\nuseful information about AI through that mailing list. Congratulations on finishing this course. You're now significantly ahead\nof many large companies' CEOs in your understanding of AI and\nyour ability to plan for the rise of AI. So I hope that you provide\nleadership to others as well that are trying to navigate these issues. Lastly, I want to say to you, thank\nyou very much for taking this course. I know that you're busy with your own\nwork or school, and friends and family, and I'm grateful that you spend so much\ntime with me and in this course learning these complex issues relating to both\nthe technology and the impact of AI. So thank you very much for both the time\nand the effort you put Into this course.""",35,0,1
coursera,stanford_university,machine-learning,welcome-to-machine-learning,"b""What is machine learning? You probably use it dozens of times\na day without even knowing it. Each time you do a web search on Google or\nBing, that works so well because their machine learning software\nhas figured out how to rank what pages. When Facebook or Apple's photo application\nrecognizes your friends in your pictures, that's also machine learning. Each time you read your email and a spam filter saves you from having\nto wade through tons of spam, again, that's because your computer has learned\nto distinguish spam from non-spam email. So, that's machine learning. There's a science of getting computers to\nlearn without being explicitly programmed. One of the research projects that I'm\nworking on is getting robots to tidy up the house. How do you go about doing that? Well what you can do is have the robot\nwatch you demonstrate the task and learn from that. The robot can then watch what objects\nyou pick up and where to put them and try to do the same thing\neven when you aren't there. For me, one of the reasons I'm\nexcited about this is the AI, or artificial intelligence problem. Building truly intelligent machines, we can do just about anything that you or\nI can do. Many scientists think the best way to\nmake progress on this is through learning algorithms called neural networks,\nwhich mimic how the human brain works, and I'll teach you about that, too. In this class,\nyou learn about machine learning and get to implement them yourself. I hope you sign up on our website and\njoin us.""",1,0,1
coursera,stanford_university,machine-learning,welcome,"b""Welcome to this free online class on\nmachine learning. Machine learning is one of the most exciting recent technologies.\nAnd in this class, you learn about the state of the art and also gain practice\nimplementing and deploying these algorithms yourself. You've probably use a learning\nalgorithm dozens of times a day without knowing it. Every time you use a web\nsearch engine like Google or Bing to search the internet, one of the reasons\nthat works so well is because a learning algorithm, one implemented by Google or\nMicrosoft, has learned how to rank web pages. Every time you use Facebook or\nApple's photo typing application and it recognizes your friends' photos, that's\nalso machine learning. Every time you read your email and your spam filter saves you\nfrom having to wade through tons of spam email, that's also a learning algorithm.\nFor me one of the reasons I'm excited is the AI dream of someday building machines\nas intelligent as you or me. We're a long way away from that goal, but many AI\nresearchers believe that the best way to towards that goal is through learning\nalgorithms that try to mimic how the human brain learns. I'll tell you a little bit\nabout that too in this class. In this class you learn about state-of-the-art\nmachine learning algorithms. But it turns out just knowing the algorithms and\nknowing the math isn't that much good if you don't also know how to actually get\nthis stuff to work on problems that you care about. So, we've also spent a lot\nof time developing exercises for you to implement each of these algorithms and\nsee how they work fot yourself. So why is machine learning so prevalent today?\nIt turns out that machine learning is a field that had grown out of the field of\nAI, or artificial intelligence. We wanted to build intelligent machines and it turns\nout that there are a few basic things that we could program a machine to do such as\nhow to find the shortest path from A to B. But for the most part we just did not know\nhow to write AI programs to do the more interesting things such as web search or\nphoto tagging or email anti-spam. There was a realization that the only way to do\nthese things was to have a machine learn to do it by itself. So, machine learning\nwas developed as a new capability for computers and today it touches many\nsegments of industry and basic science. For me, I work on machine learning and\nin a typical week I might end up talking to helicopter pilots, biologists, a bunch\nof computer systems people (so my colleagues here at Stanford) and averaging\ntwo or three times a week I get email from people in industry from Silicon Valley\ncontacting me who have an interest in applying learning algorithms to their own\nproblems. This is a sign of the range of problems that machine learning touches.\nThere is autonomous robotics, computational biology, tons of things in Silicon Valley\nthat machine learning is having an impact on. Here are some other examples of\nmachine learning. There's database mining. One of the reasons machine learning has so\npervaded is the growth of the web and the growth of automation All this means that\nwe have much larger data sets than ever before. So, for example tons of Silicon\nValley companies are today collecting web click data, also called clickstream data,\nand are trying to use machine learning algorithms to mine this data to understand\nthe users better and to serve the users better, that's a huge segment of\nSilicon Valley right now. Medical records. With the advent of automation, we\nnow have electronic medical records, so if we can turn medical records into medical\nknowledge, then we can start to understand disease better. Computational biology.\nWith automation again, biologists are collecting lots of data about gene\nsequences, DNA sequences, and so on, and machines running algorithms are giving us\na much better understanding of the human genome, and what it means to be human.\nAnd in engineering as well, in all fields of engineering, we have larger and larger,\nand larger and larger data sets, that we're trying to understand using learning\nalgorithms. A second range of machinery applications is ones that we cannot\nprogram by hand. So for example, I've worked on autonomous helicopters for many\nyears. We just did not know how to write a computer program to make this helicopter\nfly by itself. The only thing that worked was having a computer learn by itself how\nto fly this helicopter. [Helicopter whirling] Handwriting recognition. It turns out one\nof the reasons it's so inexpensive today to route a piece of mail across the\ncountries, in the US and internationally, is that when you write an envelope like\nthis, it turns out there's a learning algorithm that has learned how to read your\nhandwriting so that it can automatically route this envelope on its way, and so it\ncosts us a few cents to send this thing thousands of miles. And in fact if you've\nseen the fields of natural language processing or computer vision,\nthese are the fields of AI pertaining to understanding language or understanding\nimages. Most of natural language processing and most of computer vision today is\napplied machine learning. Learning algorithms are also widely used for self-\ncustomizing programs. Every time you go to Amazon or Netflix or iTunes Genius, and it\nrecommends the movies or products and music to you, that's a learning algorithm.\nIf you think about it they have million users; there is no way to write a million\ndifferent programs for your million users. The only way to have software give these\ncustomized recommendations is to become learn by itself to customize itself to\nyour preferences. Finally learning algorithms are being used today to\nunderstand human learning and to understand the brain. We'll talk about\nhow researches are using this to make progress towards the big AI dream. A few\nmonths ago, a student showed me an article on the top twelve IT skills. The skills\nthat information technology hiring managers cannot say no to. It was a\nslightly older article, but at the top of this list of the twelve most desirable IT\nskills was machine learning. Here at Stanford, the number of recruiters\nthat contact me asking if I know any graduating machine learning students\nis far larger than the machine learning students we graduate each year. So I\nthink there is a vast, unfulfilled demand for this skill set, and this is a great time to\nbe learning about machine learning, and I hope to teach you a lot about machine\nlearning in this class. In the next video, we'll start to give a more formal\ndefinition of what is machine learning. And we'll begin to talk about the main\ntypes of machine learning problems and algorithms. You'll pick up some of the\nmain machine learning terminology, and start to get a sense of what are the\ndifferent algorithms, and when each one might be appropriate.""",2,0,1
coursera,stanford_university,machine-learning,what-is-machine-learning,"b""What is machine learning? In this video,\nwe will try to define what it is and also try to give you a sense of when\nyou want to use machine learning. Even among machine learning practitioners, there isn't a well accepted definition of\nwhat is and what isn't machine learning. But let me show you a couple of examples\nof the ways that people have tried to define it. Here's a definition of what is machine\nlearning as due to Arthur Samuel. He defined machine learning\nas the field of study that gives computers the ability to\nlearn without being explicitly learned. Samuel's claim to fame was that back in\nthe 1950, he wrote a checkers playing program and the amazing thing about\nthis checkers playing program was that Arthur Samuel himself\nwasn't a very good checkers player. But what he did was he had to programmed\nmaybe tens of thousands of games against himself, and by watching what sorts of\nboard positions tended to lead to wins and what sort of board positions\ntended to lead to losses, the checkers playing program learned over\ntime what are good board positions and what are bad board positions. And eventually learn to play checkers\nbetter than the Arthur Samuel himself was able to. This was a remarkable result. Arthur Samuel himself turns out not\nto be a very good checkers player. But because a computer has the patience\nto play tens of thousands of games against itself, no human has\nthe patience to play that many games. By doing this, a computer was able to\nget so much checkers playing experience that it eventually became a better\ncheckers player than Arthur himself. This is a somewhat informal definition and\nan older one. Here's a slightly more recent\ndefinition by Tom Mitchell who's a friend of Carnegie Melon. So Tom defines machine\nlearning by saying that a well-posed learning problem\nis defined as follows. He says, a computer program is said to\nlearn from experience E with respect to some task T and some performance\nmeasure P, if its performance on T, as measured by P,\nimproves with experience E. I actually think he came out with this\ndefinition just to make it rhyme. For the checkers playing examples,\nthe experience E would be the experience of having the program\nplay tens of thousands of games itself. The task T would be the task\nof playing checkers, and the performance measure P\nwill be the probability that wins the next game of checkers\nagainst some new opponent. Throughout these videos,\nbesides me trying to teach you stuff, I'll occasionally ask you a question to\nmake sure you understand the content. Here's one. On top is a definition of machine\nlearning by Tom Mitchell. Let's say your email program watches which\nemails you do or do not mark as spam. So in an email client like this, you might click the Spam button to report\nsome email as spam but not other emails. And based on which\nemails you mark as spam, say your email program learns\nbetter how to filter spam email. What is the task T in this setting? In a few seconds,\nthe video will pause and when it does so, you can use your mouse to select one\nof these four radio buttons to let me know which of these four you think\nis the right answer to this question. So hopefully you got that\nthis is the right answer, classifying emails is the task T. In fact, this definition defines\na task T performance measure P and some experience E. And so, watching you label\nemails as spam or not spam, this would be the experience E and\nand the fraction of emails correctly classified,\nthat might be a performance measure P. And so on the task of systems performance, on the performance measure P will\nimprove after the experience E. In this class, I hope to teach you about various\ndifferent types of learning algorithms. There are several different\ntypes of learning algorithms. The main two types are what we\ncall supervised learning and unsupervised learning. I'll define what these terms mean\nmore in the next couple videos. It turns out that in supervised learning, the idea is we're going to teach\nthe computer how to do something. Whereas in unsupervised learning,\nwe're going to let it learn by itself. Don't worry if these two\nterms don't make sense yet. In the next two videos, I'm going to say exactly what\nthese two types of learning are. You might also hear other ghost terms\nsuch as reinforcement learning and recommender systems. These are other types of machine learning\nalgorithms that we'll talk about later. But the two most use types of learning\nalgorithms are probably supervised learning and unsupervised learning. And I'll define them in the next two\nvideos and we'll spend most of this class talking about these two\ntypes of learning algorithms. It turns out what are the other things\nto spend a lot of time on in this class is practical advice for\napplying learning algorithms. This is something that I\nfeel pretty strongly about. And exactly something that I don't\nknow if any other university teachers. Teaching about learning algorithms\nis like giving a set of tools. And equally important or more important than giving you the tools as they\nteach you how to apply these tools. I like to make an analogy to\nlearning to become a carpenter. Imagine that someone is teaching\nyou how to be a carpenter, and they say, here's a hammer, here's\na screwdriver, here's a saw, good luck. Well, that's no good. You have all these tools but the more important thing is to learn\nhow to use these tools properly. There's a huge difference between people\nthat know how to use these machine learning algorithms, versus people that\ndon't know how to use these tools well. Here, in Silicon Valley where I live, when I go visit different companies even\nat the top Silicon Valley companies, very often I see people trying to apply machine\nlearning algorithms to some problem and sometimes they have been going at for\nsix months. But sometimes when I look at what their\ndoing, I say, I could have told them like, gee, I could have told you six months\nago that you should be taking a learning algorithm and applying it in like\nthe slightly modified way and your chance of success will\nhave been much higher. So what we're going to do in this class is\nactually spend a lot of the time talking about how if you're actually trying\nto develop a machine learning system, how to make those best practices type\ndecisions about the way in which you build your system. So that when you're finally learning\nalgorithim, you're less likely to end up one of those people who end up persuing\nsomething after six months that someone else could have figured out just\na waste of time for six months. So I'm actually going to spend a lot of\ntime teaching you those sorts of best practices in machine learning and\nAI and how to get the stuff to work and how the best people do it in\nSilicon Valley and around the world. I hope to make you one of the best\npeople in knowing how to design and build serious machine learning and\nAI systems. So that's machine learning, and\nthese are the main topics I hope to teach. In the next video, I'm going to define\nwhat is supervised learning and after that what is unsupervised learning. And also time to talk about when\nyou would use each of them.""",3,0,1
coursera,stanford_university,machine-learning,supervised-learning,"b'In this video, I\'m going to define what is probably the most common type of Machine Learning problem, which is Supervised Learning. I\'ll define Supervised Learning more formally later, but it\'s probably best to explain or start with an example of what it is, and we\'ll do the formal definition later. Let\'s say you want to predict housing prices. A while back a student collected data sets from the City of Portland, Oregon, and let\'s say you plot the data set and it looks like this. Here on the horizontal axis, the size of different houses in square feet, and on the vertical axis, the price of different houses in thousands of dollars. So, given this data, let\'s say you have a friend who owns a house that is say 750 square feet, and they are hoping to sell the house, and they want to know how much they can get for the house. So, how can the learning algorithm help you? One thing a learning algorithm might be want to do is put a straight line through the data, also fit a straight line to the data. Based on that, it looks like maybe their house can be sold for maybe about $150,000. But maybe this isn\'t the only learning algorithm you can use, and there might be a better one. For example, instead of fitting a straight line to the data, we might decide that it\'s better to fit a quadratic function, or a second-order polynomial to this data. If you do that and make a prediction here, then it looks like, well, maybe they can sell the house for closer to $200,000. One of the things we\'ll talk about later is how to choose, and how to decide, do you want to fit a straight line to the data? Or do you want to fit a quadratic function to the data? There\'s no fair picking whichever one gives your friend the better house to sell. But each of these would be a fine example of a learning algorithm. So, this is an example of a Supervised Learning algorithm. The term Supervised Learning refers to the fact that we gave the algorithm a data set in which the, called, ""right answers"" were given. That is we gave it a data set of houses in which for every example in this data set, we told it what is the right price. So, what was the actual price that that house sold for, and the task of the algorithm was to just produce more of these right answers such as for this new house that your friend may be trying to sell. To define a bit more terminology, this is also called a regression problem. By regression problem, I mean we\'re trying to predict a continuous valued output. Namely the price. So technically, I guess prices can be rounded off to the nearest cent. So, maybe prices are actually discrete value. But usually, we think of the price of a house as a real number, as a scalar value, as a continuous value number, and the term regression refers to the fact that we\'re trying to predict the sort of continuous values attribute. Here\'s another Supervised Learning examples. Some friends and I were actually working on this earlier. Let\'s say you want to look at medical records and try to predict of a breast cancer as malignant or benign. If someone discovers a breast tumor, a lump in their breast, a malignant tumor is a tumor that is harmful and dangerous, and a benign tumor is a tumor that is harmless. So obviously, people care a lot about this. Let\'s see collected data set. Suppose you are in your dataset, you have on your horizontal axis the size of the tumor, and on the vertical axis, I\'m going to plot one or zero, yes or no, whether or not these are examples of tumors we\'ve seen before are malignant, which is one, or zero or not malignant or benign. So, let\'s say your dataset looks like this, where we saw a tumor of this size that turned out to be benign, one of this size, one of this size, and so on. Sadly, we also saw a few malignant tumors cell, one of that size, one of that size, one of that size, so on. So in this example, I have five examples of benign tumors shown down here, and five examples of malignant tumors shown with a vertical axis value of one. Let\'s say a friend who tragically has a breast tumor, and let\'s say her breast tumor size is maybe somewhere around this value, the Machine Learning question is, can you estimate what is the probability, what\'s the chance that a tumor as malignant versus benign? To introduce a bit more terminology, this is an example of a classification problem. The term classification refers to the fact, that here, we\'re trying to predict a discrete value output zero or one, malignant or benign. It turns out that in classification problems, sometimes you can have more than two possible values for the output. As a concrete example, maybe there are three types of breast cancers. So, you may try to predict a discrete value output zero, one, two, or three, where zero may mean benign, benign tumor, so no cancer, and one may mean type one cancer, maybe three types of cancer, whatever type one means, and two mean a second type of cancer, and three may mean a third type of cancer. But this will also be a classification problem because this are the discrete value set of output corresponding to you\'re no cancer, or cancer type one, or cancer type two, or cancer types three. In classification problems, there is another way to plot this data. Let me show you what I mean. I\'m going to use a slightly different set of symbols to plot this data. So, if tumor size is going to be the attribute that I\'m going to use to predict malignancy or benignness, I can also draw my data like this. I\'m going to use different symbols to denote my benign and malignant, or my negative and positive examples. So, instead of drawing crosses, I\'m now going to draw O\'s for the benign tumors, like so, and I\'m going to keep using X\'s to denote my malignant tumors. I hope this figure makes sense. All I did was I took my data set on top, and I just mapped it down to this real line like so, and started to use different symbols, circles and crosses to denote malignant versus benign examples. Now, in this example, we use only one feature or one attribute, namely the tumor size in order to predict whether a tumor is malignant or benign. In other machine learning problems, when we have more than one feature or more than one attribute. Here\'s an example, let\'s say that instead of just knowing the tumor size, we know both the age of the patients and the tumor size. In that case, maybe your data set would look like this, where I may have a set of patients with those ages, and that tumor size, and they look like this, and different set of patients that look a little different, whose tumors turn out to be malignant as denoted by the crosses. So, let\'s say you have a friend who tragically has a tumor, and maybe their tumor size and age falls around there. So, given a data set like this, what the learning algorithm might do is fit a straight line to the data to try to separate out the malignant tumors from the benign ones, and so the learning algorithm may decide to put a straight line like that to separate out the two causes of tumors. With this, hopefully we can decide that your friend\'s tumor is more likely, if it\'s over there that hopefully your learning algorithm will say that your friend\'s tumor falls on this benign side and is therefore more likely to be benign than malignant. In this example, we had two features namely, the age of the patient and the size of the tumor. In other Machine Learning problems, we will often have more features. My friends that worked on this problem actually used other features like these, which is clump thickness, clump thickness of the breast tumor, uniformity of cell size of the tumor, uniformity of cell shape the tumor, and so on, and other features as well. It turns out one of the most interesting learning algorithms that we\'ll see in this course, as the learning algorithm that can deal with not just two, or three, or five features, but an infinite number of features. On this slide, I\'ve listed a total of five different features. Two on the axis and three more up here. But it turns out that for some learning problems what you really want is not to use like three or five features, but instead you want to use an infinite number of features, an infinite number of attributes, so that your learning algorithm has lots of attributes, or features, or cues with which to make those predictions. So, how do you deal with an infinite number of features? How do you even store an infinite number of things in the computer when your computer is going to run out of memory? It turns out that when we talk about an algorithm called the Support Vector Machine, there will be a neat mathematical trick that will allow a computer to deal with an infinite number of features. Imagine that I didn\'t just write down two features here and three features on the right, but imagine that I wrote down an infinitely long list. I just kept writing more and more features, like an infinitely long list of features. It turns out we will come up with an algorithm that can deal with that. So, just to recap, in this course, we\'ll talk about Supervised Learning, and the idea is that in Supervised Learning, in every example in our data set, we are told what is the correct answer that we would have quite liked the algorithms have predicted on that example. Such as the price of the house, or whether a tumor is malignant or benign. We also talked about the regression problem, and by regression that means that our goal is to predict a continuous valued output. We talked about the classification problem where the goal is to predict a discrete value output. Just a quick wrap up question. Suppose you\'re running a company and you want to develop learning algorithms to address each of two problems. In the first problem, you have a large inventory of identical items. So, imagine that you have thousands of copies of some identical items to sell, and you want to predict how many of these items you sell over the next three months. In the second problem, problem two, you have lots of users, and you want to write software to examine each individual of your customer\'s accounts, so each one of your customer\'s accounts. For each account, decide whether or not the account has been hacked or compromised. So, for each of these problems, should they be treated as a classification problem or as a regression problem? When the video pauses, please use your mouse to select whichever of these four options on the left you think is the correct answer. So hopefully, you got that. This is the answer. For problem one, I would treat this as a regression problem because if I have thousands of items, well, I would probably just treat this as a real value, as a continuous value. Therefore, the number of items I sell as a continuous value. For the second problem, I would treat that as a classification problem, because I might say set the value I want to predict with zero to denote the account has not been hacked, and set the value one to denote an account that has been hacked into. So, just like your breast cancers where zero is benign, one is malignant. So, I might set this be zero or one depending on whether it\'s been hacked, and have an algorithm try to predict each one of these two discrete values. Because there\'s a small number of discrete values, I would therefore treat it as a classification problem. So, that\'s it for Supervised Learning. In the next video, I\'ll talk about Unsupervised Learning, which is the other major category of learning algorithm.'",4,0,1
coursera,stanford_university,machine-learning,unsupervised-learning,"b'In this video, we\'ll talk about the second major type of machine learning problem, called Unsupervised Learning. In the last video, we talked about Supervised Learning. Back then, recall data sets that look like this, where each example was labeled either as a positive or negative example, whether it was a benign or a malignant tumor. So for each example in Supervised Learning, we were told explicitly what is the so-called right answer, whether it\'s benign or malignant. In Unsupervised Learning, we\'re given data that looks different than data that looks like this that doesn\'t have any labels or that all has the same label or really no labels. So we\'re given the data set and we\'re not told what to do with it and we\'re not told what each data point is. Instead we\'re just told, here is a data set. Can you find some structure in the data? Given this data set, an Unsupervised Learning algorithm might decide that the data lives in two different clusters. And so there\'s one cluster and there\'s a different cluster. And yes, Supervised Learning algorithm may break these data into these two separate clusters. So this is called a clustering algorithm. And this turns out to be used in many places. One example where clustering is used is in Google News and if you have not seen this before, you can actually go to this URL news.google.com to take a look. What Google News does is everyday it goes and looks at tens of thousands or hundreds of thousands of new stories on the web and it groups them into cohesive news stories. For example, let\'s look here. The URLs here link to different news stories about the BP Oil Well story. So, let\'s click on one of these URL\'s and we\'ll click on one of these URL\'s. What I\'ll get to is a web page like this. Here\'s a Wall Street Journal article about, you know, the BP Oil Well Spill stories of ""BP Kills Macondo"", which is a name of the spill and if you click on a different URL from that group then you might get the different story. Here\'s the CNN story about a game, the BP Oil Spill, and if you click on yet a third link, then you might get a different story. Here\'s the UK Guardian story about the BP Oil Spill. So what Google News has done is look for tens of thousands of news stories and automatically cluster them together. So, the news stories that are all about the same topic get displayed together. It turns out that clustering algorithms and Unsupervised Learning algorithms are used in many other problems as well. Here\'s one on understanding genomics. Here\'s an example of DNA microarray data. The idea is put a group of different individuals and for each of them, you measure how much they do or do not have a certain gene. Technically you measure how much certain genes are expressed. So these colors, red, green, gray and so on, they show the degree to which different individuals do or do not have a specific gene. And what you can do is then run a clustering algorithm to group individuals into different categories or into different types of people. So this is Unsupervised Learning because we\'re not telling the algorithm in advance that these are type 1 people, those are type 2 persons, those are type 3 persons and so on and instead what were saying is yeah here\'s a bunch of data. I don\'t know what\'s in this data. I don\'t know who\'s and what type. I don\'t even know what the different types of people are, but can you automatically find structure in the data from the you automatically cluster the individuals into these types that I don\'t know in advance? Because we\'re not giving the algorithm the right answer for the examples in my data set, this is Unsupervised Learning. Unsupervised Learning or clustering is used for a bunch of other applications. It\'s used to organize large computer clusters. I had some friends looking at large data centers, that is large computer clusters and trying to figure out which machines tend to work together and if you can put those machines together, you can make your data center work more efficiently. This second application is on social network analysis. So given knowledge about which friends you email the most or given your Facebook friends or your Google+ circles, can we automatically identify which are cohesive groups of friends, also which are groups of people that all know each other? Market segmentation. Many companies have huge databases of customer information. So, can you look at this customer data set and automatically discover market segments and automatically group your customers into different market segments so that you can automatically and more efficiently sell or market your different market segments together? Again, this is Unsupervised Learning because we have all this customer data, but we don\'t know in advance what are the market segments and for the customers in our data set, you know, we don\'t know in advance who is in market segment one, who is in market segment two, and so on. But we have to let the algorithm discover all this just from the data. Finally, it turns out that Unsupervised Learning is also used for surprisingly astronomical data analysis and these clustering algorithms gives surprisingly interesting useful theories of how galaxies are formed. All of these are examples of clustering, which is just one type of Unsupervised Learning. Let me tell you about another one. I\'m gonna tell you about the cocktail party problem. So, you\'ve been to cocktail parties before, right? Well, you can imagine there\'s a party, room full of people, all sitting around, all talking at the same time and there are all these overlapping voices because everyone is talking at the same time, and it is almost hard to hear the person in front of you. So maybe at a cocktail party with two people, two people talking at the same time, and it\'s a somewhat small cocktail party. And we\'re going to put two microphones in the room so there are microphones, and because these microphones are at two different distances from the speakers, each microphone records a different combination of these two speaker voices. Maybe speaker one is a little louder in microphone one and maybe speaker two is a little bit louder on microphone 2 because the 2 microphones are at different positions relative to the 2 speakers, but each microphone would cause an overlapping combination of both speakers\' voices. So here\'s an actual recording of two speakers recorded by a researcher. Let me play for you the first, what the first microphone sounds like. One (uno), two (dos), three (tres), four (cuatro), five (cinco), six (seis), seven (siete), eight (ocho), nine (nueve), ten (y diez). All right, maybe not the most interesting cocktail party, there\'s two people counting from one to ten in two languages but you know. What you just heard was the first microphone recording, here\'s the second recording. Uno (one), dos (two), tres (three), cuatro (four), cinco (five), seis (six), siete (seven), ocho (eight), nueve (nine) y diez (ten). So we can do, is take these two microphone recorders and give them to an Unsupervised Learning algorithm called the cocktail party algorithm, and tell the algorithm - find structure in this data for you. And what the algorithm will do is listen to these audio recordings and say, you know it sounds like the two audio recordings are being added together or that have being summed together to produce these recordings that we had. Moreover, what the cocktail party algorithm will do is separate out these two audio sources that were being added or being summed together to form other recordings and, in fact, here\'s the first output of the cocktail party algorithm. One, two, three, four, five, six, seven, eight, nine, ten. So, I separated out the English voice in one of the recordings. And here\'s the second of it. Uno, dos, tres, quatro, cinco, seis, siete, ocho, nueve y diez. Not too bad, to give you one more example, here\'s another recording of another similar situation, here\'s the first microphone :  One, two, three, four, five, six, seven, eight, nine, ten. OK so the poor guy\'s gone home from the cocktail party and he \'s now sitting in a room by himself talking to his radio. Here\'s the second microphone recording. One, two, three, four, five, six, seven, eight, nine, ten. When you give these two microphone recordings to the same algorithm, what it does, is again say, you know, it sounds like there are two audio sources, and moreover, the album says, here is the first of the audio sources I found. One, two, three, four, five, six, seven, eight, nine, ten. So that wasn\'t perfect, it got the voice, but it also got a little bit of the music in there. Then here\'s the second output to the algorithm. Not too bad, in that second output it managed to get rid of the voice entirely. And just, you know, cleaned up the music, got rid of the counting from one to ten. So you might look at an Unsupervised Learning algorithm like this and ask how complicated this is to implement this, right? It seems like in order to, you know, build this application, it seems like to do this audio processing you need to write a ton of code or maybe link into like a bunch of synthesizer Java libraries that process audio, seems like a really complicated program, to do this audio, separating out audio and so on. It turns out the algorithm, to do what you just heard, that can be done with one line of code - shown right here. It take researchers a long time to come up with this line of code. I\'m not saying this is an easy problem, But it turns out that when you use the right programming environment, many learning algorithms can be really short programs. So this is also why in this class we\'re going to use the Octave programming environment. Octave, is free open source software, and using a tool like Octave or Matlab, many learning algorithms become just a few lines of code to implement. Later in this class, I\'ll just teach you a little bit about how to use Octave and you\'ll be implementing some of these algorithms in Octave. Or if you have Matlab you can use that too. It turns out the Silicon Valley, for a lot of machine learning algorithms, what we do is first prototype our software in Octave because software in Octave makes it incredibly fast to implement these learning algorithms. Here each of these functions like for example the SVD function that stands for singular value decomposition; but that turns out to be a linear algebra routine, that is just built into Octave. If you were trying to do this in C++ or Java, this would be many many lines of code linking complex C++ or Java libraries. So, you can implement this stuff as C++ or Java or Python, it\'s just much more complicated to do so in those languages. What I\'ve seen after having taught machine learning for almost a decade now, is that, you learn much faster if you use Octave as your programming environment, and if you use Octave as your learning tool and as your prototyping tool, it\'ll let you learn and prototype learning algorithms much more quickly. And in fact what many people will do to in the large Silicon Valley companies is in fact, use an algorithm like Octave to first prototype the learning algorithm, and only after you\'ve gotten it to work, then you migrate it to C++ or Java or whatever. It turns out that by doing things this way, you can often get your algorithm to work much faster than if you were starting out in C++. So, I know that as an instructor, I get to say ""trust me on this one"" only a finite number of times, but for those of you who\'ve never used these Octave type programming environments before, I am going to ask you to trust me on this one, and say that you, you will, I think your time, your development time is one of the most valuable resources. And having seen lots of people do this, I think you as a machine learning researcher, or machine learning developer will be much more productive if you learn to start in prototype, to start in Octave, in some other language. Finally, to wrap up this video, I have one quick review question for you. We talked about Unsupervised Learning, which is a learning setting where you give the algorithm a ton of data and just ask it to find structure in the data for us. Of the following four examples, which ones, which of these four do you think would will be an Unsupervised Learning algorithm as opposed to Supervised Learning problem. For each of the four check boxes on the left, check the ones for which you think Unsupervised Learning algorithm would be appropriate and then click the button on the lower right to check your answer. So when the video pauses, please answer the question on the slide. So, hopefully, you\'ve remembered the spam folder problem. If you have labeled data, you know, with spam and non-spam e-mail, we\'d treat this as a Supervised Learning problem. The news story example, that\'s exactly the Google News example that we saw in this video, we saw how you can use a clustering algorithm to cluster these articles together so that\'s Unsupervised Learning. The market segmentation example I talked a little bit earlier, you can do that as an Unsupervised Learning problem because I am just gonna get my algorithm data and ask it to discover market segments automatically. And the final example, diabetes, well, that\'s actually just like our breast cancer example from the last video. Only instead of, you know, good and bad cancer tumors or benign or malignant tumors we instead have diabetes or not and so we will use that as a supervised, we will solve that as a Supervised Learning problem just like we did for the breast tumor data. So, that\'s it for Unsupervised Learning and in the next video, we\'ll delve more into specific learning algorithms and start to talk about just how these algorithms work and how we can, how you can go about implementing them.'",5,0,1
coursera,stanford_university,machine-learning,model-representation,"b'Our first learning algorithm will be\nlinear regression. In this video, you\'ll see what the model looks like and more\nimportantly you\'ll see what the overall process of supervised learning looks like. Let\'s\nuse some motivating example of predicting housing prices. We\'re going to use a data\nset of housing prices from the city of Portland, Oregon. And here I\'m gonna\nplot my data set of a number of houses that were different sizes that were sold\nfor a range of different prices. Let\'s say that given this data set, you have a\nfriend that\'s trying to sell a house and let\'s see if friend\'s house is size of\n1250 square feet and you want to tell them how much they might be able to sell the\nhouse for. Well one thing you could do is fit a model. Maybe fit a straight line\nto this data. Looks something like that and based on that, maybe you could tell your friend\nthat let\'s say maybe he can sell the house for around $220,000.\nSo this is an example of a supervised learning algorithm. And it\'s\nsupervised learning because we\'re given the, quotes, ""right answer"" for each of\nour examples. Namely we\'re told what was the actual house, what was the actual\nprice of each of the houses in our data set were sold for and moreover, this is\nan example of a regression problem where the term regression refers to the fact\nthat we are predicting a real-valued output namely the price. And just to remind you\nthe other most common type of supervised learning problem is called the\nclassification problem where we predict discrete-valued outputs such as if we are\nlooking at cancer tumors and trying to decide if a tumor is malignant or benign.\nSo that\'s a zero-one valued discrete output. More formally, in supervised learning, we have\na data set and this data set is called a training set. So for housing prices\nexample, we have a training set of different housing prices and our job is to\nlearn from this data how to predict prices of the houses. Let\'s define some notation\nthat we\'re using throughout this course. We\'re going to define quite a lot of\nsymbols. It\'s okay if you don\'t remember all the symbols right now but as the\ncourse progresses it will be useful [inaudible] convenient notation. So I\'m gonna use\nlower case m throughout this course to denote the number of training examples. So\nin this data set, if I have, you know, let\'s say 47 rows in this table. Then I\nhave 47 training examples and m equals 47. Let me use lowercase x  to denote the\ninput variables often also called the features. That would be the x is here, it would the input features. And I\'m gonna use y to denote my output variables or the\ntarget variable which I\'m going to predict and so that\'s the second\ncolumn here. [inaudible] notation, I\'m going to use (x, y) to denote a single\ntraining example. So, a single row in this table corresponds to a single training\nexample and to refer to a specific training example, I\'m going to use this\nnotation x(i) comma gives me y(i) And, we\'re going to use this to refer to the ith\ntraining example. So this superscript i over here, this is not exponentiation\nright? This (x(i), y(i)), the superscript i in parentheses that\'s just an index into my\ntraining set and refers to the ith row in this table, okay? So this is not x to\nthe power of i, y to the power of i. Instead (x(i), y(i)) just refers to the ith row of this\ntable. So for example, x(1) refers to the input value for the first training example so\nthat\'s 2104. That\'s this x in the first row. x(2) will be equal to\n1416 right? That\'s the second x and y(1) will be equal to 460.\nThe first, the y value for my first training example, that\'s what that (1)\nrefers to. So as mentioned, occasionally I\'ll ask you a question to let you check your\nunderstanding and a few seconds in this video a multiple-choice question\nwill pop up in the video. When it does, please use your mouse to select what you\nthink is the right answer. What defined by the training set is. So here\'s how this\nsupervised learning algorithm works. We saw that with the training set like our\ntraining set of housing prices and we feed that to our learning algorithm. Is the job\nof a learning algorithm to then output a function which by convention is\nusually denoted lowercase h and h stands for hypothesis And what the job of\nthe hypothesis is, is, is a function that takes as input the size of a house like\nmaybe the size of the new house your friend\'s trying to sell so it takes in the value of\nx and it tries to output the estimated value of y for the corresponding house.\nSo h is a function that maps from x\'s to y\'s. People often ask me, you\nknow, why is this function called hypothesis. Some of you may know the\nmeaning of the term hypothesis, from the dictionary or from science or whatever. It\nturns out that in machine learning, this is a name that was used in the early days of\nmachine learning and it kinda stuck. \'Cause maybe not a great name for this sort of\nfunction, for mapping from sizes of houses to the predictions, that you know....\nI think the term hypothesis, maybe isn\'t the best possible name for this, but this is the\nstandard terminology that people use in machine learning. So don\'t worry too much\nabout why people call it that. When designing a learning algorithm, the next\nthing we need to decide is how do we represent this hypothesis h. For this and\nthe next few videos, I\'m going to choose our initial choice , for representing the\nhypothesis, will be the following. We\'re going to represent h as follows. And we will write this as\nh<u>theta(x) equals theta<u>0</u></u> plus theta<u>1 of x. And as a shorthand,\nsometimes instead of writing, you</u> know, h subscript theta of x, sometimes\nthere\'s a shorthand, I\'ll just write as a h of x. But more often I\'ll write it as a\nsubscript theta over there. And plotting this in the pictures, all this means is that,\nwe are going to predict that y is a linear function of x. Right, so that\'s the\ndata set and what this function is doing, is predicting that y is some straight\nline function of x. That\'s h of x equals theta 0 plus theta 1 x, okay? And why a linear\nfunction? Well, sometimes we\'ll want to fit more complicated, perhaps non-linear\nfunctions as well. But since this linear case is the simple building block, we will\nstart with this example first of fitting linear functions, and we will build on\nthis to eventually have more complex models, and more complex learning\nalgorithms. Let me also give this particular model a name. This model is\ncalled linear regression or this, for example, is actually linear regression\nwith one variable, with the variable being x. Predicting all the prices as functions\nof one variable X. And another name for this model is univariate linear\nregression. And univariate is just a fancy way of saying one variable. So,\nthat\'s linear regression. In the next video we\'ll start to talk about just how\nwe go about implementing this model.'",6,0,1
coursera,stanford_university,machine-learning,cost-function,"b'In this video we\'ll define\nsomething called the cost function, this will let us figure out how to fit the\nbest possible straight line to our data. In linear progression, we have a training\nset that I showed here remember on notation M was the number of training\nexamples, so maybe m equals 47. And the form of our hypothesis, which we use to make predictions\nis this linear function. To introduce a little bit more\nterminology, these theta zero and theta one, they stabilize what I\ncall the parameters of the model. And what we\'re going to do in\nthis video is talk about how to go about choosing these two parameter\nvalues, theta 0 and theta 1. With different choices of\nthe parameter\'s theta 0 and theta 1, we get different hypothesis,\ndifferent hypothesis functions. I know some of you will probably\nbe already familiar with what I am going to do on the slide, but\njust for review, here are a few examples. If theta 0 is 1.5 and theta 1 is 0, then the hypothesis function\nwill look like this. Because your hypothesis function will\nbe h of x equals 1.5 plus 0 times x which is this constant value\nfunction which is phat at 1.5. If theta0 = 0, theta1 = 0.5, then\nthe hypothesis will look like this, and it should pass through this point 2,1 so that you now have h(x). Or really h of theta(x), but sometimes\nI\'ll just omit theta for brevity. So h(x) will be equal to just 0.5 times x,\nwhich looks like that. And finally, if theta zero equals one,\nand theta one equals 0.5, then we end up with a hypothesis\nthat looks like this. Let\'s see,\nit should pass through the two-two point. Like so, and this is my new vector of x,\nor my new h subscript theta of x. Whatever way you remember, I said that\nthis is h subscript theta of x, but that\'s a shorthand,\nsometimes I\'ll just write this as h of x. In linear regression, we have a training\nset, like maybe the one I\'ve plotted here. What we want to do, is come up with\nvalues for the parameters theta zero and theta one so that the straight line\nwe get out of this, corresponds to a straight line that somehow fits the data\nwell, like maybe that line over there. So, how do we come up with values, theta zero, theta one, that\ncorresponds to a good fit to the data? The idea is we get to choose our\nparameters theta 0, theta 1 so that h of x,\nmeaning the value we predict on input x, that this is at least\nclose to the values y for the examples in our training set,\nfor our training examples. So in our training set, we\'ve given\na number of examples where we know X decides the wholes and\nwe know the actual price is was sold for. So, let\'s try to choose values for\nthe parameters so that, at least in the training set, given the X in the training set we make reason of\nthe active predictions for the Y values. Let\'s formalize this. So linear regression,\nwhat we\'re going to do is, I\'m going to want to solve\na minimization problem. So I\'ll write minimize over theta0 theta1. And I want this to be small, right? I want the difference between h(x) and\ny to be small. And one thing I might do is try\nto minimize the square difference between the output of the hypothesis and\nthe actual price of a house. Okay.\nSo lets find some details. You remember that I was using\nthe notation (x(i),y(i)) to represent the ith training example. So what I want really is to\nsum over my training set, something i = 1 to m, of the square difference between,\nthis is the prediction of my hypothesis when it is\ninput to size of house number i. Right?\nMinus the actual price that house number I was sold for, and I want to\nminimize the sum of my training set, sum from I equals one through M,\nof the difference of this squared error, the square difference between\nthe predicted price of a house, and the price that it was actually sold for. And just remind you of notation, m here\nwas the size of my training set right? So my m there is my number\nof training examples. Right that hash sign is the abbreviation\nfor number of training examples, okay? And to make some of our,\nmake the math a little bit easier, I\'m going to actually look at\nwe are 1 over m times that so let\'s try to minimize my\naverage minimize one over 2m. Putting the 2 at the constant one\nhalf in front, it may just sound the math probably easier so minimizing\none-half of something, right, should give you the same values of the process, theta\n0 theta 1, as minimizing that function. And just to be sure,\nthis equation is clear, right? This expression in here, h subscript theta(x), this is our usual, right? That is equal to this plus theta one xi. And this notation,\nminimize over theta 0 theta 1, this means you\'ll find me the values of theta 0 and\ntheta 1 that causes this expression to be minimized and this expression\ndepends on theta 0 and theta 1, okay? So just a recap. We\'re closing this problem as, find me\nthe values of theta zero and theta one so that the average, the 1 over the 2m, times the sum of square errors between\nmy predictions on the training set minus the actual values of the houses\non the training set is minimized. So this is going to be my overall\nobjective function for linear regression. And just to rewrite this out a little bit\nmore cleanly, what I\'m going to do is, by convention we usually\ndefine a cost function, which is going to be exactly this,\nthat formula I have up here. And what I want to do is\nminimize over theta0 and theta1. My function j(theta0, theta1). Just write this out. This is my cost function. So, this cost function is also\ncalled the squared error function. When sometimes called the squared\nerror cost function and it turns out that why do we\ntake the squares of the erros. It turns out that these squared error\ncost function is a reasonable choice and works well for problems for\nmost regression programs. There are other cost functions\nthat will work pretty well. But the square cost function is\nprobably the most commonly used one for regression problems. Later in this class we\'ll talk about\nalternative cost functions as well, but this choice that we just had should\nbe a pretty reasonable thing to try for most linear regression problems. Okay. So that\'s the cost function. So far we\'ve just seen a mathematical\ndefinition of this cost function. In case this function j of theta zero,\ntheta one. In case this function seems\na little bit abstract, and you still don\'t have a good\nsense of what it\'s doing, in the next video, in the next\ncouple videos, I\'m actually going to go a little bit deeper into what\nthe cause function ""J"" is doing and try to give you better intuition about what\nis computing and why we want to use it...'",7,0,1
coursera,stanford_university,machine-learning,cost-function-intuition-i,"b""In the previous video, we gave the\nmathematical definition of the cost function. In this video, let's look at\nsome examples, to get back to intuition about what the cost function is doing, and\nwhy we want to use it. To recap, here's what we had last time. We want to fit a\nstraight line to our data, so we had this formed as a hypothesis with these\nparameters theta zero and theta one, and with different choices of the parameters\nwe end up with different straight line fits. So the data which are fit\nlike so, and there's a cost function, and that was our optimization objective.\n[sound] So this video, in order to better visualize the cost function J, I'm going\nto work with a simplified hypothesis function, like that shown on the right. So\nI'm gonna use my simplified hypothesis, which is just theta one times X. We can,\nif you want, think of this as setting the parameter theta zero equal to 0. So I\nhave only one parameter theta one and my cost function is similar to before\nexcept that now H of X that is now equal to just theta one times X. And I have only\none parameter theta one and so my optimization objective is to minimize j of\ntheta one. In pictures what this means is that if theta zero equals zero that\ncorresponds to choosing only hypothesis functions that pass through the origin,\nthat pass through the point (0, 0). Using this simplified definition of a hypothesizing cost\nfunction let's try to understand the cost function concept better. It turns out that\ntwo key functions we want to understand. The first is the hypothesis function, and\nthe second is a cost function. So, notice that the hypothesis, right, H of X. For a\nface value of theta one, this is a function of X. So the hypothesis is a\nfunction of, what is the size of the house X. In contrast, the cost function, J,\nthat's a function of the parameter, theta one, which controls the slope of the\nstraight line. Let's plot these functions and try to understand them both better.\nLet's start with the hypothesis. On the left, let's say here's my training set with\nthree points at (1, 1), (2, 2), and (3, 3). Let's pick a value theta one, so when theta one\nequals one, and if that's my choice for theta one, then my hypothesis is going to\nlook like this straight line over here. And I'm gonna point out, when I'm plotting\nmy hypothesis function. X-axis, my horizontal axis is labeled X, is labeled\nyou know, size of the house over here. Now, of temporary, set\ntheta one equals one, what I want to do is figure out what is j of theta one, when\ntheta one equals one. So let's go ahead and compute what the cost function has\nfor. You'll devalue one. Well, as usual, my cost function is defined as follows,\nright? Some from, some of 'em are training sets of this usual squared error term.\nAnd, this is therefore equal to. And this. Of theta one x I minus y I and if you\nsimplify this turns out to be. That. Zero Squared to zero squared to zero squared which\nis of course, just equal to zero. Now, inside the cost function. It turns out each\nof these terms here is equal to zero. Because for the specific training set I have or my\n3 training examples are (1, 1), (2, 2), (3,3). If theta one is equal to one. Then h of x. H of x\ni. Is equal to y I exactly, let me write this better. Right? And so, h of x minus\ny, each of these terms is equal to zero, which is why I find that j of one is equal\nto zero. So, we now know that j of one Is equal to zero. Let's plot that. What I'm\ngonna do on the right is plot my cost function j. And notice, because my cost\nfunction is a function of my parameter theta one, when I plot my cost function,\nthe horizontal axis is now labeled with theta one. So I have j of one zero\nzero so let's go ahead and plot that. End up with. An X over there. Now lets look at\nsome other examples. Theta-1 can take on a range of different values. Right? So\ntheta-1 can take on the negative values, zero, positive values. So what if theta-1\nis equal to 0.5. What happens then? Let's go ahead and plot that. I'm now going to\nset theta-1 equals 0.5, and in that case my hypothesis now looks like this. As a line\nwith slope equals to 0.5, and, lets compute J, of 0.5. So that is going to be\none over 2M of, my usual cost function. It turns out that the cost function is\ngoing to be the sum of square values of the height of this line. Plus the sum of\nsquare of the height of that line, plus the sum of square of the height of that\nline, right? ?Cause just this vertical distance, that's the difference between,\nyou know, Y. I. and the predicted value, H of XI, right? So the first example\nis going to be 0.5 minus one squared. Because my hypothesis predicted 0.5.\nWhereas, the actual value was one. For my second example, I get, one minus two\nsquared, because my hypothesis predicted one, but the actual housing price was two.\nAnd then finally, plus. 1.5 minus three squared. And so that's equal to one over\ntwo times three. Because, M when trading set size, right, have three training\nexamples. In that, that's times simplifying for the parentheses it's 3.5.\nSo that's 3.5 over six which is about 0.68. So now we know that j of 0.5 is\nabout 0.68.[Should be 0.58] Lets go and plot that. Oh excuse me, math error, it's actually 0.58. So\nwe plot that which is maybe about over there. Okay? Now, let's do one more. How\nabout if theta one is equal to zero, what is J of zero equal to? It turns out that\nif theta one is equal to zero, then H of X is just equal to, you know, this flat\nline, right, that just goes horizontally like this. And so, measuring the errors.\nWe have that J of zero is equal to one over two M, times one squared plus two\nsquared plus three squared, which is, One six times fourteen which is about 2.3. So\nlet's go ahead and plot as well. So it ends up with a value around 2.3\nand of course we can keep on doing this for other values of theta one. It turns\nout that you can have you know negative values of theta one as well so if theta\none is negative then h of x would be equal to say minus 0.5 times x then theta\none is minus 0.5 and so that corresponds to a hypothesis with a\nslope of negative 0.5. And you can actually keep on computing these errors.\nThis turns out to be, you know, for 0.5, it turns out to have really high error. It\nworks out to be something, like, 5.25. And so on, and the different values of theta\none, you can compute these things, right? And it turns out that you, your computed\nrange of values, you get something like that. And by computing the range of\nvalues, you can actually slowly create out. What does function J of Theta say and\nthat's what J of Theta is. To recap, for each value of theta one, right? Each value\nof theta one corresponds to a different hypothesis, or to a different straight\nline fit on the left. And for each value of theta one, we could then derive a\ndifferent value of j of theta one. And for example, you know, theta one=1,\ncorresponded to this straight line straight through the data. Whereas theta\none=0.5. And this point shown in magenta corresponded to maybe that line, and theta\none=zero which is shown in blue that corresponds to this horizontal line. Right, so for each\nvalue of theta one we wound up with a different value of J of theta one and we\ncould then use this to trace out this plot on the right. Now you remember, the\noptimization objective for our learning algorithm is we want to choose the value\nof theta one. That minimizes J of theta one. Right? This was our objective function for\nthe linear regression. Well, looking at this curve, the value that minimizes j of\ntheta one is, you know, theta one equals to one. And low and behold, that is indeed\nthe best possible straight line fit through our data, by setting theta one\nequals one. And just, for this particular training set, we actually end up fitting\nit perfectly. And that's why minimizing j of theta one corresponds to finding a\nstraight line that fits the data well. So, to wrap up. In this video, we looked up\nsome plots. To understand the cost function. To do so, we simplify the\nalgorithm. So that it only had one parameter theta one. And we set the\nparameter theta zero to be only zero. In the next video. We'll go back to the original problem\nformulation and look at some visualizations involving both theta zero\nand theta one. That is without setting theta zero to zero. And hopefully that will give\nyou, an even better sense of what the cost function j is doing in the original\nlinear regression formulation.""",8,0,1
coursera,stanford_university,machine-learning,cost-function-intuition-ii,"b""In this video, lets delve deeper and get\neven better intuition about what the cost function is doing. This video assumes that\nyou're familiar with contour plots. If you are not familiar with contour plots or\ncontour figures some of the illustrations in this video may or may not make sense to\nyou but is okay and if you end up skipping this video or some of it does not quite\nmake sense because you haven't seen contour plots before. That's okay and you will\nstill understand the rest of this course without those parts of this. Here's our\nproblem formulation as usual, with the hypothesis parameters, cost function, and\nour optimization objective. Unlike before, unlike the last video, I'm\ngoing to keep both of my parameters, theta zero, and theta one, as we generate our\nvisualizations for the cost function. So, same as last time, we want to understand the\nhypothesis H and the cost function J. So, here's my training set of housing prices\nand let's make some hypothesis. You know, like that one, this is not a particularly\ngood hypothesis. But, if I set theta zero=50 and theta one=0.06, then I end up\nwith this hypothesis down here and that corresponds to that straight line. Now given\nthese value of theta zero and theta one, we want to plot the corresponding, you\nknow, cost function on the right. What we did last time was, right, when we only had\ntheta one. In other words, drawing plots that look like this as a function of theta\none. But now we have two parameters, theta zero, and theta one, and so the plot gets\na little more complicated. It turns out that when we have only one parameter, that\nthe parts we drew had this sort of bow shaped function. Now, when we have two\nparameters, it turns out the cost function also has a similar sort of bow shape. And,\nin fact, depending on your training set, you might get a cost function that maybe\nlooks something like this. So, this is a 3-D surface plot, where the axes\nare labeled theta zero and theta one. So as you vary theta zero and theta one, the two\nparameters, you get different values of the cost function J (theta zero, theta one)\nand the height of this surface above a particular point of theta zero, theta one.\nRight, that's, that's the vertical axis. The height of the surface of the points\nindicates the value of J of theta zero, J of theta one. And you can see it sort of\nhas this bow like shape. Let me show you the same plot in 3D. So here's the same\nfigure in 3D, horizontal axis theta one and vertical axis J(theta zero, theta one), and if I rotate\nthis plot around. You kinda of a get a sense, I hope, of this bowl\nshaped surface as that's what the cost function J looks like. Now for the purpose\nof illustration in the rest of this video I'm not actually going to use these sort\nof 3D surfaces to show you the cost function J, instead I'm going to use\ncontour plots. Or what I also call contour figures. I guess they mean the same thing.\nTo show you these surfaces. So here's an example of a contour figure, shown on the\nright, where the axis are theta zero and theta one. And what each of these ovals,\nwhat each of these ellipsis shows is a set of points that takes on the same value for\nJ(theta zero, theta one). So concretely, for example this, you'll take\nthat point and that point and that point. All three of these points that I just\ndrew in magenta, they have the same value for J (theta zero, theta one). Okay.\nWhere, right, these, this is the theta zero, theta one axis but those three have\nthe same Value for J (theta zero, theta one) and if you haven't seen contour\nplots much before think of, imagine if you will. A bow shaped function that's coming\nout of my screen. So that the minimum, so the bottom of the bow is this point right\nthere, right? This middle, the middle of these concentric ellipses. And imagine a\nbow shape that sort of grows out of my screen like this, so that each of these\nellipses, you know, has the same height above my screen. And the minimum with the\nbow, right, is right down there. And so the contour figures is a, is way to,\nis maybe a more convenient way to visualize my function J. [sound] So, let's\nlook at some examples. Over here, I have a particular point, right? And so this is,\nwith, you know, theta zero equals maybe about 800, and theta one equals maybe a\n-0.15 . And so this point, right, this point in red corresponds to one\nset of pair values of theta zero, theta one and the corresponding, in fact, to that\nhypothesis, right, theta zero is about 800, that is, where it intersects\nthe vertical axis is around 800, and this is slope of about -0.15. Now this line is\nreally not such a good fit to the data, right. This hypothesis, h(x), with these values of theta zero, theta one, it's really not such a good fit\nto the data. And so you find that, it's cost. Is a value that's out here that's\nyou know pretty far from the minimum right it's pretty far this is a pretty high cost\nbecause this is just not that good a fit to the data. Let's look at some more\nexamples. Now here's a different hypothesis that's you know still not a\ngreat fit for the data but may be slightly better so here right that's my point that\nthose are my parameters theta zero theta one and so my theta zero value. Right?\nThat's bout 360 and my value for theta one. Is equal to zero. So, you know, let's\nbreak it out. Let's take theta zero equals 360 theta one equals zero. And this pair\nof parameters corresponds to that hypothesis, corresponds to flat line, that is, h(x) equals 360 plus zero times x. So that's the hypothesis. And\nthis hypothesis again has some cost, and that cost is, you know, plotted as the\nheight of the J function at that point. Let's look at just a couple of examples.\nHere's one more, you know, at this value of theta zero, and at that value of theta\none, we end up with this hypothesis, h(x) and again, not a great fit to the data,\nand is actually further away from the minimum. Last example, this is actually not quite at the minimum, but\nit's pretty close to the minimum. So this is not such a bad fit to the, to the data,\nwhere, for a particular value, of, theta zero. Which, one of them has value, as in\nfor a particular value for theta one. We get a particular h(x). And this is, this\nis not quite at the minimum, but it's pretty close. And so the sum of squares\nerrors is sum of squares distances between my, training samples and my hypothesis.\nReally, that's a sum of square distances, right? Of all of these errors. This is\npretty close to the minimum even though it's not quite the minimum. So with these\nfigures I hope that gives you a better understanding of what values of the cost\nfunction J, how they are and how that corresponds to different hypothesis and so as\nhow better hypotheses may corresponds to points that are closer to the minimum of this cost\nfunction J. Now of course what we really want is an efficient algorithm, right, a\nefficient piece of software for automatically finding The value of theta\nzero and theta one, that minimizes the cost function J, right? And what we, what\nwe don't wanna do is to, you know, how to write software, to plot out this point,\nand then try to manually read off the numbers, that this is not a good way to do\nit. And, in fact, we'll see it later, that when we look at more complicated examples,\nwe'll have high dimensional figures with more parameters, that, it turns out,\nwe'll see in a few, we'll see later in this course, examples where this figure,\nyou know, cannot really be plotted, and this becomes much harder to visualize. And\nso, what we want is to have software to find the value of theta zero, theta one\nthat minimizes this function and in the next video we start to talk about\nan algorithm for automatically finding that value of theta zero and theta one\nthat minimizes the cost function J.""",9,0,1
coursera,stanford_university,machine-learning,gradient-descent,"b""We previously defined the cost function J. In this video, I want to tell you about\nan algorithm called gradient descent for minimizing the cost function J. It turns out gradient descent\nis a more general algorithm, and is used not only in linear regression. It's actually used all over\nthe place in machine learning. And later in the class,\nwe'll use gradient descent to minimize other functions as well, not just the cost\nfunction J for the linear regression. So in this video, we'll talk about\ngradient descent for minimizing some arbitrary function J and then in later\nvideos, we'll take this algorithm and apply it specifically to the cost\nfunction J that we have defined for linear regression. So here's the problem setup. Going to assume that we have\nsome function J(theta 0, theta 1) maybe it's the cost\nfunction from linear regression, maybe it's some other\nfunction we wanna minimize. And we want to come up\nwith an algorithm for minimizing that as a function\nof J(theta 0, theta 1). Just as an aside it turns out that\ngradient descent actually applies to more general functions. So imagine, if you have a function that's\na function of J, as theta 0, theta 1, theta 2, up to say some theta n,\nand you want to minimize theta 0. You minimize over theta 0 up to theta\nn of this J of theta 0 up to theta n. And it turns our gradient\ndescent is an algorithm for solving this more general problem. But for the sake of brevity, for the sake of succinctness of notation,\nI'm just going to pretend I have only two parameters throughout\nthe rest of this video. Here's the idea for gradient descent. What we're going to do is we're going to\nstart off with some initial guesses for theta 0 and theta 1. Doesn't really matter what they are, but\na common choice would be we set theta 0 to 0, and set theta 1 to 0,\njust initialize them to 0. What we're going to do in gradient descent\nis we'll keep changing theta 0 and theta 1 a little bit to try to reduce\nJ(theta 0, theta 1), until hopefully, we wind at a minimum, or\nmaybe at a local minimum. So let's see in pictures\nwhat gradient descent does. Let's say you're trying to\nminimize this function. So notice the axes, this is theta 0, theta 1 on the horizontal axes and\nJ is the vertical axis and so the height of the surface shows J and\nwe want to minimize this function. So we're going to start off with theta 0,\ntheta 1 at some point. So imagine picking some value for\ntheta 0, theta 1, and that corresponds to starting at some\npoint on the surface of this function. So whatever value of theta 0,\ntheta 1 gives you some point here. I did initialize them to 0, 0 but sometimes you initialize\nit to other values as well. Now, I want you to imagine\nthat this figure shows a hole. Imagine this is like the landscape\nof some grassy park, with two hills like so, and I want us\nto imagine that you are physically standing at that point on the hill,\non this little red hill in your park. In gradient descent, what we're going to\ndo is we're going to spin 360 degrees around, just look all around us, and ask,\nif I were to take a little baby step in some direction, and I want to go\ndownhill as quickly as possible, what direction do I take\nthat little baby step in? If I wanna go down, so I wanna physically walk down this\nhill as rapidly as possible. Turns out, that if you're standing at that\npoint on the hill, you look all around and you find that the best direction\nis to take a little step downhill is roughly that direction. Okay, and\nnow you're at this new point on your hill. You're gonna, again, look all around and say what direction should I step in order\nto take a little baby step downhill? And if you do that and take another step,\nyou take a step in that direction. And then you keep going. From this new point you look around, decide what direction would\ntake you downhill most quickly. Take another step, another step, and so on until you converge to this\nlocal minimum down here. Gradient descent has\nan interesting property. This first time we ran gradient descent\nwe were starting at this point over here, right? Started at that point over here. Now imagine we had initialized gradient\ndescent just a couple steps to the right. Imagine we'd initialized gradient descent\nwith that point on the upper right. If you were to repeat this process, so\nstart from that point, look all around, take a little step in the direction of\nsteepest descent, you would do that. Then look around,\ntake another step, and so on. And if you started just a couple of steps\nto the right, gradient descent would've taken you to this second local\noptimum over on the right. So if you had started this first point,\nyou would've wound up at this local optimum, but if you started just\nat a slightly different location, you would've wound up at a very\ndifferent local optimum. And this is a property of\ngradient descent that we'll say a little bit more about later. So that's the intuition in pictures. Let's look at the math. This is the definition of\nthe gradient descent algorithm. We're going to just repeatedly\ndo this until convergence, we're going to update my parameter\ntheta j by taking theta j and subtracting from it alpha times\nthis term over here, okay? So let's see, there's lot of details in\nthis equation so let me unpack some of it. First, this notation here,\n:=, gonna use := to denote assignment, so\nit's the assignment operator. So briefly, if I write a := b, what\nthis means is, it means in a computer, this means take the value in b and\nuse it overwrite whatever value is a. So this means set a to be equal to\nthe value of b, which is assignment. And I can also do a := a + 1. This means take a and\nincrease its value by one. Whereas in contrast,\nif I use the equal sign and I write a equals b,\nthen this is a truth assertion. Okay?\nSo if I write a equals b, then I'll asserting that the value of\na equals to the value of b, right? So the left hand side,\nthat's the computer operation, where we set the value\nof a to a new value. The right hand side, this is asserting,\nI'm just making a claim that the values of a and b are the same, and so whereas you\ncan write a := a + 1, that means increment a by 1, hopefully I won't ever write\na = a + 1 because that's just wrong. a and a + 1 can never be\nequal to the same values. Okay? So this is first part of the definition. This alpha here is a number that\nis called the learning rate. And what alpha does is\nit basically controls how big a step we take downhill\nwith creating descent. So if alpha is very large, then that\ncorresponds to a very aggressive gradient descent procedure where we're\ntrying take huge steps downhill and if alpha is very small, then we're taking\nlittle, little baby steps downhill. And I'll come back and say more about this\nlater, about how to set alpha and so on. And finally, this term here,\nthat's a derivative term. I don't wanna talk about it right now, but\nI will derive this derivative term and tell you exactly what this is later, okay? And some of you will be more familiar\nwith calculus than others, but even if you aren't familiar with calculus,\ndon't worry about it. I'll tell you what you need\nto know about this term here. Now, there's one more subtlety\nabout gradient descent which is in gradient descent we're going to update,\nyou know, theta 0 and theta 1, right? So this update takes place for\nj = 0 and j = 1, so you're gonna update theta 0 and\nupdate theta 1. And the subtlety of how you\nimplement gradient descent is for this expression, for this update equation, you want to simultaneously\nupdate theta 0 and theta 1. What I mean by that is that in this\nequation, we're gonna update theta 0 := theta 0 minus something, and\nupdate theta 1 := theta 1 minus something. And the way to implement is you should\ncompute the right hand side, right? Compute that thing for theta 0 and\ntheta 1 and then simultaneously, at the same time,\nupdate theta 0 and theta 1, okay? So let me say what I mean by that. This is a correct implementation of\ngradient descent meaning simultaneous update. So I'm gonna set temp0 equals that,\nset temp1 equals that so basic compute the right-hand sides, and\nthen having computed the right-hand sides and stored them into variables temp0\nand temp1, I'm gonna update theta 0 and theta 1 simultaneously because\nthat's the correct implementation. In contrast, here's an incorrect implementation that\ndoes not do a simultaneous update. So in this incorrect implementation,\nwe compute temp0, and then we update theta 0, and then we\ncompute temp1, and then we update temp1. And the difference between\nthe right hand side and the left hand side implementations\nis that If you look down here, you look at this step, if by this\ntime you've already updated theta 0, then you would be using the new value of\ntheta 0 to compute this derivative term. And so this gives you a different value\nof temp1, than the left-hand side, right? Because you've now plugged in the new\nvalue of theta 0 into this equation. And so, this on the right-hand side\nis not a correct implementation of gradient descent, okay? So I don't wanna say why you need\nto do the simultaneous updates. It turns out that the way gradient\ndescent is usually implemented, which I'll say more about later, it actually turns out to be more natural\nto implement the simultaneous updates. And when people talk\nabout gradient descent, they always mean simultaneous update. If you implement the non\nsimultaneous update, it turns out it will probably work anyway. But this algorithm wasn't right. It's not what people refer\nto as gradient descent, and this is some other algorithm\nwith different properties. And for various reasons this can behave\nin slightly stranger ways, and so what you should do is really implement the\nsimultaneous update of gradient descent. So, that's the outline of\nthe gradient descent algorithm. In the next video, we're going to go\ninto the details of the derivative term, which I wrote up but didn't really define. And if you've taken a calculus class\nbefore and if you're familiar with partial derivatives and derivatives, it turns out\nthat's exactly what that derivative term is, but in case you aren't familiar\nwith calculus, don't worry about it. The next video will give\nyou all the intuitions and will tell you everything you need to know\nto compute that derivative term, even if you haven't seen calculus, or even if you\nhaven't seen partial derivatives before. And with that, with the next video,\nhopefully we'll be able to give you all the intuitions\nyou need to apply gradient descent.""",10,0,1
coursera,stanford_university,machine-learning,gradient-descent-intuition,"b""In the previous video, we gave a mathematical\ndefinition of gradient descent. Let's delve deeper and in this video get\nbetter intuition about what the algorithm is doing and why the steps of the gradient\ndescent algorithm might make sense. Here's a gradient descent algorithm\nthat we saw last time and just to remind you this parameter, or this\nterm alpha is called the learning rate. And it controls how big a step we take\nwhen updating my parameter theory j. And this second term here\nis the derivative term And what I wanna do in this video is give\nyou that intuition about what each of these two terms is doing and why when put\ntogether, this entire update makes sense. In order to convey these intuitions, what\nI want to do is use a slightly simpler example, where we want to minimize\nthe function of just one parameter. So say we have a cost function,\nj of just one parameter, theta one, like we did a few videos back,\nwhere theta one is a real number. So we can have one d plots,\nwhich are a little bit simpler to look at. Let's try to understand what gradient\ndecent would do on this function. So let's say, here's my function,\nJ of theta 1. And so that's mine. And where theta 1 is a real number. All right? Now, let's have in this slide its grade in\ndescent with theta one at this location. So imagine that we start off\nat that point on my function. What grade in descent would\ndo is it will update. Theta one gets updated as\ntheta one minus alpha times d d theta one J of theta one, right? And as an aside, this derivative term,\nright, if you're wondering why I changed the notation\nfrom these partial derivative symbols. If you don't know what the difference is\nbetween these partial derivative symbols and the dd theta, don't worry about it. Technically in mathematics you\ncall this a partial derivative and call this a derivative, depending on the\nnumber of parameters in the function J. But that's a mathematical technicality. And so for the purpose of this lecture, think of these partial symbols and d,\nd theta 1, as exactly the same thing. And don't worry about what\nthe real difference is. I'm gonna try to use the mathematically\nprecise notation, but for our purposes these two notations\nare really the same thing. And so\nlet's see what this equation will do. So we're going to compute this derivative,\nnot sure if you've seen derivatives in calculus before, but what the derivative\nat this point does, is basically saying, now let's take the tangent to that point,\nlike that straight line, that red line, is just touching this function, and\nlet's look at the slope of this red line. That's what the derivative is, it's saying what's the slope of the line\nthat is just tangent to the function. Okay, the slope of a line is just this\nheight divided by this horizontal thing. Now, this line has a positive slope, so it has a positive derivative. And so\nmy update to theta is going to be theta 1, it gets updated as theta 1,\nminus alpha times some positive number. Okay. Alpha the the learning,\nis always a positive number. And, so we're going to take theta one is\nupdated as theta one minus something. So I'm gonna end up moving\ntheta one to the left. I'm gonna decrease theta one,\nand we can see this is the right thing to do cuz I actually\nwanna head in this direction. You know, to get me closer\nto the minimum over there. So, gradient descent so\nfar says we're going the right thing. Let's look at another example. So let's take my same function J,\nlet's try to draw from the same function, J of theta 1. And now, let's say I had to say initialize\nmy parameter over there on the left. So theta 1 is here. I glare at that point on the surface. Now my derivative term DV theta one\nJ of theta one when you value into that this point, we're gonna look\nat right the slope of that line, so this derivative term\nis a slope of this line. But this line is slanting down,\nso this line has negative slope. Right. Or alternatively, I say that this\nfunction has negative derivative, just means negative slope at that point. So this is less than equals to 0, so\nwhen I update theta, I'm gonna have theta. Just update this theta of minus\nalpha times a negative number. And so I have theta 1 minus a negative\nnumber which means I'm actually going to increase theta,\nbecause it's minus of a negative number, means I'm adding something to theta. And what that means is that I'm\ngoing to end up increasing theta until it's not here, and increase theta\nwish again seems like the thing I wanted to do to try to get me\ncloser to the minimum. So this whole theory of intuition\nbehind what a derivative is doing, let's take a look at the rate term\nalpha and see what that's doing. So here's my gradient descent\nupdate mural, that's this equation. And let's look at what could happen\nif alpha is either too small or if alpha is too large. So this first example,\nwhat happens if alpha is too small? So here's my function J, J of theta. Let's all start here. If alpha is too small, then what\nI'm gonna do is gonna multiply my update by some small number, so\nend up taking a baby step like that. Okay, so this one step. Then from this new point,\nI'm gonna have to take another step. But if alpha's too small,\nI take another little baby step. And so if my learning rate is\ntoo small I'm gonna end up taking these tiny tiny baby steps\nas you try to get to the minimum. And I'm gonna need a lot of steps\nto get to the minimum and so if alpha is too small gradient\ndescent can be slow because it's gonna take these tiny tiny baby steps and so it's gonna need a lot of steps before it\ngets anywhere close to the global minimum. Now how about if our alpha is too large? So, here's my function Jf filter,\nturns out that alpha's too large, then gradient descent can overshoot the\nminimum and may even fail to convert or even divert, so here's what I mean. Let's say it's all our data there,\nit's actually close to minimum. So the derivative points to the right,\nbut if alpha is too big, I want to take a huge step. Remember, take a huge step like that. So it ends up taking a huge step, and\nnow my cost functions have strong roots. Cuz it starts off with this value, and\nnow, my values are strong in verse. Now my derivative points to the left,\nit says I should decrease data. But if my learning is too big, I may take a huge step going from\nhere all the way to out there. So we end up being over there, right? And if my is too big, we can take another\nhuge step on the next elevation and kind of overshoot and overshoot and\nso on, until you already notice I'm actually getting further and\nfurther away from the minimum. So if alpha is to large,\nit can fail to converge or even diverge. Now, I have another question for you. So this a tricky one and when I was first\nlearning this stuff it actually took me a long time to figure this out. What if your parameter theta 1\nis already at a local minimum, what do you think one step\nof gradient descent will do? So let's suppose you initialize\ntheta 1 at a local minimum. So, suppose this is your initial\nvalue of theta 1 over here and is already at a local optimum or\nthe local minimum. It turns out the local optimum,\nyour derivative will be equal to zero. So for that slope, that tangent point,\nso the slope of this line will be equal to zero and thus this\nderivative term is equal to zero. And so your gradient descent update, you have theta one cuz I updated this\ntheta one minus alpha times zero. And so what this means is that if you're\nalready at the local optimum it leaves theta 1 unchanged cause its\nupdates as theta 1 equals theta 1. So if your parameters are already\nat a local minimum one step with gradient descent does absolutely\nnothing it doesn't your parameter which is what you want because it keeps\nyour solution at the local optimum. This also explains why gradient\ndescent can converse the local minimum even with the learning rate alpha fixed. Here's what I mean by that\nlet's look in the example. So here's a cost function J of theta that maybe I want to minimize and\nlet's say I initialize my algorithm, my gradient descent algorithm,\nout there at that magenta point. If I take one step in gradient descent,\nmaybe it will take me to that point, because my derivative's\npretty steep out there. Right? Now, I'm at this green point, and if I\ntake another step in gradient descent, you notice that my derivative,\nmeaning the slope, is less steep at the green point than compared to\nat the magenta point out there. Because as I approach the minimum, my\nderivative gets closer and closer to zero, as I approach the minimum. So after one step of descent,\nmy new derivative is a little bit smaller. So I wanna take another step\nin the gradient descent. I will naturally take a somewhat smaller\nstep from this green point right there from the magenta point. Now with a new point, a red point, and\nI'm even closer to global minimum so the derivative here will be even\nsmaller than it was at the green point. So I'm gonna another step\nin the gradient descent. Now, my derivative term is even\nsmaller and so the magnitude of the update to theta one is even smaller,\nso take a small step like so. And as gradient descent runs, you will automatically take smaller and\nsmaller steps. Until eventually you're taking\nvery small steps, you know, and you finally converge to\nthe to the local minimum. So just to recap, in gradient descent\nas we approach a local minimum, gradient descent will\nautomatically take smaller steps. And that's because as we\napproach the local minimum, by definition the local minimum is\nwhen the derivative is equal to zero. As we approach local minimum, this\nderivative term will automatically get smaller, and so gradient descent will\nautomatically take smaller steps. This is what so\nno need to decrease alpha or the time. So that's the gradient descent algorithm\nand you can use it to try to minimize any cost function J, not the cost function\nJ that we defined for linear regression. In the next video,\nwe're going to take the function J and set that back to be exactly linear\nregression's cost function, the square cost function that\nwe came up with earlier. And taking gradient descent and this great\ncause function and putting them together. That will give us our\nfirst learning algorithm, that'll give us a linear\nregression algorithm.""",11,0,1
coursera,stanford_university,machine-learning,gradient-descent-for-linear-regression,"b""In previous videos, we talked about\nthe gradient descent algorithm and we talked about the linear regression\nmodel and the squared error cost function. In this video we're gonna put together\ngradient descent with our cost function, and that will give us an algorithm for\nlinear regression or putting a straight line to our data. So this was what we worked\nout in the previous videos. This gradient descent algorithm\nwhich you should be familiar and here's the linear regression model\nwith our linear hypothesis and our squared error cost function. What we're going to do is\napply gradient descent to minimize our squared\nerror cost function. Now in order to apply gradient descent,\nin order to, you know, write this piece of code, the key term we\nneed is this derivative term over here. So you need to figure out what is\nthis partial derivative term and plugging in the definition\nof the cause function j. This turns out to be this. Sum from y equals 1 though m. Of this squared error cost function term. And all I did here was I just, you know plug in the definition\nof the cost function there. And simplifying a little bit more,\nthis turns out to be equal to this. Sigma i equals one through m of theta\nzero plus theta one x i minus Yi squared. And all I did there was I took\nthe definition for my hypothesis and plugged it in there. And turns out we need to figure out\nwhat is this partial derivative for two cases for J equals 0 and J equals 1. So we want to figure out what\nis this partial derivative for both the theta 0 case and\nthe theta 1 case. And I'm just going to\nwrite out the answers. It turns out this first term is,\nsimplifies to 1/M sum from over my training step\nof just that of X(i)- Y(i) and for this term partial derivative\nlet's write the theta 1, it turns out I get this term. Minus Y(i) times X(i). Okay and computing these partial derivatives,\nso we're going from this equation. Right going from this equation to\neither of the equations down there. Computing those partial derivative terms\nrequires some multivariate calculus. If you know calculus, feel free to work\nthrough the derivations yourself and check that if you take the derivatives,\nyou actually get the answers that I got. But if you're less familiar with calculus,\ndon't worry about it and it's fine to just take these equations\nthat were worked out and you won't need to know calculus or anything like\nthat, in order to do the homework so let's implement gradient descent and\nget back to work. So armed with these definitions or\narmed with what we worked out to be the derivatives which is really just\nthe slope of the cost function j we can now plug them back in to\nour gradient descent algorithm. So here's gradient descent for linear regression which is gonna\nrepeat until convergence, theta 0 and theta 1 get updated as you know this thing\nminus alpha times the derivative term. So this term here. So here's our linear regression algorithm. This first term here. That term is of course just the partial\nderivative with respect to theta zero, that we worked out on a previous slide. And this second term here,\nthat term is just a partial derivative in respect to theta 1,\nthat we worked out on the previous line. And just as a quick reminder, you must,\nwhen implementing gradient descent. There's actually this detail that\nyou should be implementing it so the update theta 0 and\ntheta 1 simultaneously. So. Let's see how gradient descent works. One of the issues we saw with gradient\ndescent is that it can be susceptible to local optima. So when I first explained gradient\ndescent I showed you this picture of it going downhill on the surface, and we saw\nhow depending on where you initialize it, you can end up at different local optima. You will either wind up here or here. But, it turns out that\nthat the cost function for linear regression is always going to\nbe a bow shaped function like this. The technical term for this is that\nthis is called a convex function. And I'm not gonna give the formal\ndefinition for what is a convex function, C, O, N, V, E, X. But informally a convex function\nmeans a bowl shaped function and so this function doesn't have any local\noptima except for the one global optimum. And does gradient descent on this type\nof cost function which you get whenever you're using linear regression it will\nalways converge to the global optimum. Because there are no other local optimum,\nglobal optimum. So now let's see this algorithm in action. As usual, here are plots of the hypothesis\nfunction and of my cost function j. And so let's say I've initialized\nmy parameters at this value. Let's say, usually you initialize\nyour parameters at zero, zero. Theta zero and theta equals zero. But for the demonstration,\nin this physical infrontation I've initialized you know, theta zero at\n900 and theta one at about -0.1 okay. And so this corresponds to h(x)=-900-0.1x,\n[the intercept should be +900] is this line,\nout here on the cost function. Now, if we take one step\nin gradient descent, we end up going from this point out here,\nover to the down and left, to that second point over there. And you notice that my line\nchanged a little bit, and as I take another step of gradient\ndescent, my line on the left will change. Right? And I've also moved to a new\npoint on my cost function. And as I take further steps of gradient\ndescent, I'm going down in cost. So my parameters and\nsuch are following this trajectory. And if you look on the left,\nthis corresponds with hypotheses. That seem to be getting to be better and\nbetter fits to the data until eventually I've now wound up at\nthe global minimum and this global minimum corresponds to this hypothesis,\nwhich gets me a good fit to the data. And so that's gradient descent,\nand we've just run it and gotten a good fit to my\ndata set of housing prices. And you can now use it to predict,\nyou know, if your friend has a house\nsize 1250 square feet, you can now read off the value and tell\nthem that I don't know maybe they could get $250,000 for their house. Finally just to give this another\nname it turns out that the algorithm that we just went over is sometimes\ncalled batch gradient descent. And it turns out in machine learning I\ndon't know I feel like us machine learning people were not always great\nat giving names to algorithms. But the term batch gradient\ndescent refers to the fact that in every step of gradient descent, we're\nlooking at all of the training examples. So in gradient descent,\nwhen computing the derivatives, we're computing the sums [INAUDIBLE]. So ever step of gradient descent we end up\ncomputing something like this that sums over our m training examples and so\nthe term batch gradient descent refers to the fact that we're looking at\nthe entire batch of training examples. And again,\nit's really not a great name, but this is what machine\nlearning people call it. And it turns out that there are sometimes\nother versions of gradient descent that are not batch versions,\nbut they are instead. Do not look at the entire training set but look at small subsets of\nthe training sets at a time. And we'll talk about those versions\nlater in this course as well. But for now using the algorithm we just\nlearned about or using batch gradient descent you now know how to implement\ngradient descent for linear regression. So that's linear regression\nwith gradient descent. If you've seen advanced\nlinear algebra before, so some of you may have taken a class\nin advanced linear algebra. You might know that there exists\na solution for numerically solving for the minimum of the cost\nfunction j without needing to use an iterative algorithm\nlike gradient descent. Later in this course we'll talk about\nthat method as well that just solves for the minimum of the cost function j\nwithout needing these multiple steps of gradient descent. That other method is called\nthe normal equations method. But in case you've heard of that\nmethod it turns out that gradient descent will scale better to larger data\nsets than that normal equation method. And now that we know about gradient\ndescent we'll be able to use it in lots of different contexts and we'll use it in lots of different\nmachine learning problems as well. So congrats on learning about your\nfirst machine learning algorithm. We'll later have exercises in which we'll\nask you to implement gradient descent and hopefully see these algorithms right for\nyourselves. But before that I first want to\ntell you in the next set of videos. The first one to tell you\nabout a generalization of the gradient descent algorithm that\nwill make it much more powerful. And I guess I'll tell you\nabout that in the next video.""",12,0,1
coursera,stanford_university,machine-learning,matrices-and-vectors,"b'Let\'s get started with our linear algebra review. In this video I want to tell you what are matrices and what are vectors. A matrix is a rectangular array of numbers written between square brackets. So, for example, here is a matrix on the right, a left square bracket. And then, write in a bunch of numbers. These could be features from a learning problem or it could be data from somewhere else, but the specific values don\'t matter, and then I\'m going to close it with another right bracket on the right. And so that\'s one matrix. And, here\'s another example of the matrix, let\'s write 3, 4, 5,6. So matrix is just another way for saying, is a 2D or a two dimensional array. And the other piece of knowledge that we need is that the dimension of the matrix is going to be written as the number of row times the number of columns in the matrix. So, concretely, this example on the left, this has 1, 2, 3, 4 rows and has 2 columns, and so this example on the left is a 4 by 2 matrix -  number of rows by number of columns. So, four rows, two columns. This one on the right, this matrix has two rows. That\'s the first row, that\'s the second row, and it has three columns. That\'s the first column, that\'s the second column, that\'s the third column So, this second matrix we say it is a 2 by 3 matrix. So we say that the dimension of this matrix is 2 by 3. Sometimes you also see this written out, in the case of left, you will see this written out as R4 by 2 or concretely what people will sometimes say this matrix is an element of the set R 4 by 2. So, this thing here, this just means the set of all matrices that of dimension 4 by 2 and this thing on the right, sometimes this is written out as a matrix that is an R 2 by 3. So if you ever see, 2 by 3. So if you ever see something like this are 4 by 2 or are 2 by 3, people are just referring to matrices of a specific dimension. Next, let\'s talk about how to refer to specific elements of the matrix. And by matrix elements, other than the matrix I just mean the entries, so the numbers inside the matrix. So, in the standard notation, if A is this matrix here, then A sub-strip IJ is going to refer to the i, j entry, meaning the entry in the matrix in the ith row and jth column. So for example a1-1 is going to refer to the entry in the 1st row and the 1st column, so that\'s the first row and the first column and so a1-1 is going to be equal to 1, 4, 0, 2. Another example, 8 1 2 is going to refer to the entry in the first row and the second column and so A 1 2 is going to be equal to one nine one. This come from a quick examples. Let\'s see, A, oh let\'s say A 3 2, is going to refer to the entry in the 3rd row, and second column, right, because that\'s 3 2 so that\'s equal to 1 4 3 7. And finally, 8 4 1 is going to refer to this one right, fourth row, first column is equal to 1 4 7 and if, hopefully you won\'t, but if you were to write and say well this A 4 3, well, that refers to the fourth row, and the third column that, you know, this matrix has no third column so this is undefined, you know, or you can think of this as an error. There\'s no such element as 8 4 3, so, you know, you shouldn\'t be referring to 8 4 3. So, the matrix gets you a way of letting you quickly organize, index and access lots of data. In case I seem to be tossing up a lot of concepts, a lot of new notations very rapidly, you don\'t need to memorize all of this, but on the course website where we have posted the lecture notes, we also have all of these definitions written down. So you can always refer back, you know, either to these slides, possible coursework, so audible lecture notes if you forget well, A41 was that? Which row, which column was that? Don\'t worry about memorizing everything now. You can always refer back to the written materials on the course website, and use that as a reference. So that\'s what a matrix is. Next, let\'s talk about what is a vector. A vector turns out to be a special case of a matrix. A vector is a matrix that has only 1 column so you have an N x 1 matrix, then that\'s a remember, right? N is the number of rows, and 1 here is the number of columns, so, so matrix with just one column is what we call a vector. So here\'s an example of a vector, with I guess I have N equals four elements here. so we also call this thing, another term for this is a four dmensional vector, just means that this is a vector with four elements, with four numbers in it. And, just as earlier for matrices you saw this notation R3 by 2 to refer to 2 by 3 matrices, for this vector we are going to refer to this as a vector in the set R4. So this R4 means a set of four-dimensional vectors. Next let\'s talk about how to refer to the elements of the vector. We are going to use the notation yi to refer to the ith element of the vector y. So if y is this vector, y subscript i is the ith element. So y1 is the first element,four sixty, y2 is equal to the second element, two thirty two -there\'s the first. There\'s the second. Y3 is equal to 315 and so on, and only y1 through y4 are defined consistency 4-dimensional vector. Also it turns out that there are actually 2 conventions for how to index into a vector and here they are. Sometimes, people will use one index and sometimes zero index factors. So this example on the left is a one in that specter where the element we write is y1, y2, y3, y4. And this example in the right is an example of a zero index factor where we start the indexing of the elements from zero. So the elements go from a zero up to y three. And this is a bit like the arrays of some primary languages where the arrays can either be indexed starting from one. The first element of an array is sometimes a Y1, this is sequence notation I guess, and sometimes it\'s zero index depending on what programming language you use. So it turns out that in most of math, the one index version is more common For a lot of machine learning applications, zero index vectors gives us a more convenient notation. So what you should usually do is, unless otherwised specified, you should assume we are using one index vectors. In fact, throughout the rest of these videos on linear algebra review, I will be using one index vectors. But just be aware that when we are talking about machine learning applications, sometimes I will explicitly say when we need to switch to, when we need to use the zero index vectors as well. Finally, by convention, usually when writing matrices and vectors, most people will use upper case to refer to matrices. So we\'re going to use capital letters like A, B, C, you know, X, to refer to matrices, and usually we\'ll use lowercase, like a, b, x, y, to refer to either numbers, or just raw numbers or scalars or to vectors. This isn\'t always true but this is the more common notation where we use lower case ""Y"" for referring to vector and we usually use upper case to refer to a matrix. So, you now know what are matrices and vectors. Next, we\'ll talk about some of the things you can do with them'",13,0,1
coursera,stanford_university,machine-learning,addition-and-scalar-multiplication,"b""In this video we'll talk about matrix addition and subtraction, as well as how to multiply a matrix by a number, also called Scalar Multiplication. Let's start an example. Given two matrices like these, let's say I want to add them together. How do I do that? And so, what does addition of matrices mean? It turns out that if you want to add two matrices, what you do is you just add up the elements of these matrices one at a time. So, my result of adding two matrices is going to be itself another matrix and the first element again just by taking one and four and multiplying them and adding them together, so I get five. The second element I get by taking two and two and adding them, so I get four; three plus three plus zero is three, and so on. I'm going to stop changing colors, I guess. And, on the right is open five, ten and two. And it turns out you can add only two matrices that are of the same dimensions. So this example is a three by two matrix, because this has 3 rows and 2 columns, so it's 3 by 2. This is also a 3 by 2 matrix, and the result of adding these two matrices is a 3 by 2 matrix again. So you can only add matrices of the same dimension, and the result will be another matrix that's of the same dimension as the ones you just added. Where as in contrast, if you were to take these two matrices, so this one is a 3 by 2 matrix, okay, 3 rows, 2 columns. This here is a 2 by 2 matrix. And because these two matrices are not of the same dimension, you know, this is an error, so you cannot add these two matrices and, you know, their sum is not well-defined. So that's matrix addition. Next, let's talk about multiplying matrices by a scalar number. And the scalar is just a, maybe a overly fancy term for, you know, a number or a real number. Alright, this means real number. So let's take the number 3 and multiply it by this matrix. And if you do that, the result is pretty much what you'll expect. You just take your elements of the matrix and multiply them by 3, one at a time. So, you know, one times three is three. What, two times three is six, 3 times 3 is 9, and let's see, I'm going to stop changing colors again. Zero times 3 is zero. Three times 5 is 15, and 3 times 1 is three. And so this matrix is the result of multiplying that matrix on the left by 3. And you notice, again, this is a 3 by 2 matrix and the result is a matrix of the same dimension. This is a 3 by 2, both of these are 3 by 2 dimensional matrices. And by the way, you can write multiplication, you know, either way. So, I have three times this matrix. I could also have written this matrix and 0, 2, 5, 3, 1, right. I just copied this matrix over to the right. I can also take this matrix and multiply this by three. So whether it's you know, 3 times the matrix or the matrix times three is the same thing and this thing here in the middle is the result. You can also take a matrix and divide it by a number. So, turns out taking this matrix and dividing it by four, this is actually the same as taking the number one quarter, and multiplying it by this matrix. 4, 0, 6, 3 and so, you can figure the answer, the result of this product is, one quarter times four is one, one quarter times zero is zero. One quarter times six is, what, three halves, about six over four is three halves, and one quarter times three is three quarters. And so that's the results of computing this matrix divided by four. Vectors give you the result. Finally, for a slightly more complicated example, you can also take these operations and combine them together. So in this calculation, I have three times a vector plus a vector minus another vector divided by three. So just make sure we know where these are, right. This multiplication. This is an example of scalar multiplication because I am taking three and multiplying it. And this is, you know, another scalar multiplication. Or more like scalar division, I guess. It really just means one zero times this. And so if we evaluate these two operations first, then what we get is this thing is equal to, let's see, so three times that vector is three, twelve, six, plus my vector in the middle which is a 005 minus one, zero, two-thirds, right? And again, just to make sure we understand what is going on here, this plus symbol, that is matrix addition, right? I really, since these are vectors, remember, vectors are special cases of matrices, right? This, you can also call this vector addition This minus sign here, this is again a matrix subtraction, but because this is an n by 1, really a three by one matrix, that this is actually a vector, so this is also vector, this column. We call this matrix a vector subtraction, as well. OK? And finally to wrap this up. This therefore gives me a vector, whose first element is going to be 3+0-1, so that's 3-1, which is 2. The second element is 12+0-0, which is 12. And the third element of this is, what, 6+5-(2/3), which is 11-(2/3), so that's 10 and one-third and see, you close this square bracket. And so this gives me a 3 by 1 matrix, which is also just called a 3 dimensional vector, which is the outcome of this calculation over here. So that's how you add and subtract matrices and vectors and multiply them by scalars or by row numbers. So far I have only talked about how to multiply matrices and vectors by scalars, by row numbers. In the next video we will talk about a much more interesting step, of taking 2 matrices and multiplying 2 matrices together.""",14,0,1
coursera,stanford_university,machine-learning,matrix-vector-multiplication,"b'In this video, I\'d like to start talking about how to multiply together two matrices. We\'ll start with a special case of that, of matrix vector multiplication -  multiplying a matrix together with a vector. Let\'s start with an example. Here is a matrix, and here is a vector, and let\'s say we want to multiply together this matrix with this vector, what\'s the result? Let me just work through this example and then we can step back and look at just what the steps were. It turns out the result of this multiplication process is going to be, itself, a vector. And I\'m just going work with this first and later we\'ll come back and see just what I did here. To get the first element of this vector I am going to take these two numbers and multiply them with the first row of the matrix and add up the corresponding numbers. Take one multiplied by one, and take three and multiply it by five, and that\'s what, that\'s one plus fifteen so that gives me sixteen. I\'m going to write sixteen here. then for the second row, second element, I am going to take the second row and multiply it by this vector, so I have four times one, plus zero times five, which is equal to four, so you\'ll have four there. And finally for the last one I have two one times one five, so two by one, plus one by 5, which is equal to a 7, and so I get a 7 over there. It turns out that the results of multiplying that\'s a 3x2 matrix by a 2x1 matrix is also just a two-dimensional vector. The result of this is going to be a 3x1 matrix, so that\'s why three by one 3x1 matrix, in other words a 3x1 matrix is just a three dimensional vector. So I realize that I did that pretty quickly, and you\'re probably not sure that you can repeat this process yourself, but let\'s look in more detail at what just happened and what this process of multiplying a matrix by a vector looks like. Here\'s the details of how to multiply a matrix by a vector. Let\'s say I have a matrix A and want to multiply it by a vector x. The result is going to be some vector y. So the matrix A is a m by n dimensional matrix, so m rows and n columns and we are going to multiply that by a n by 1 matrix, in other words an n dimensional vector. It turns out this ""n"" here has to match this ""n"" here. In other words, the number of columns in this matrix, so it\'s the number of n columns. The number of columns here has to match the number of rows here. It has to match the dimension of this vector. And the result of this product is going to be an n-dimensional vector y.  Rows here. ""M"" is going to be equal to the number of rows in this matrix ""A"". So how do you actually compute this vector ""Y""? Well it turns out to compute this vector ""Y"", the process is to get ""Y""""I"", multiply ""A\'s"" ""I\'th"" row with the elements of the vector ""X"" and add them up. So here\'s what I mean. In order to get the first element of ""Y"", that first number--whatever that turns out to be--we\'re gonna take the first row of the matrix ""A"" and multiply them one at a time with the elements of this vector ""X"". So I take this first number multiply it by this first number. Then take the second number multiply it by this second number. Take this third number whatever that is, multiply it the third number and so on until you get to the end. And I\'m gonna add up the results of these products and the result of paying that out is going to give us this first element of ""Y"". Then when we want to get the second element of ""Y"", let\'s say this element. The way we do that is we take the second row of A and we repeat the whole thing. So we take the second row of A, and multiply it elements-wise, so the elements of X and add up the results of the products and that would give me the second element of Y. And you keep going to get and we going to take the third row of A, multiply element Ys with the vector x, sum up the results and then I get the third element and so on, until I get down to the last row like so, okay? So that\'s the procedure. Let\'s do one more example. Here\'s the example:  So let\'s look at the dimensions. Here, this is a three by four dimensional matrix. This is a four-dimensional vector, or a 4 x 1 matrix, and so the result of this, the result of this product is going to be a three-dimensional vector. Write, you know, the vector, with room for three elements. Let\'s do the, let\'s carry out the products. So for the first element, I\'m going to take these four numbers and multiply them with the vector X. So I have 1x1, plus 2x3, plus 1x2, plus 5x1, which is equal to - that\'s 1+6, plus 2+6, which gives me 14. And then for the second element, I\'m going to take this row now and multiply it with this vector  (0x1)+3. All right, so 0x1+  3x3 plus 0x2 plus 4x1, which is equal to, let\'s see that\'s 9+4, which is 13. And finally, for the last element, I\'m going to take this last row, so I have minus one times one. You have minus two, or really there\'s a plus next to a two I guess. Times three plus zero times two plus zero times one, and so that\'s going to be minus one minus six, which is going to make this seven, and so that\'s vector seven. Okay? So my final answer is this vector fourteen, just to write to that without the colors, fourteen, thirteen, negative seven. And as promised, the result here is a three by one matrix. So that\'s how you multiply a matrix and a vector. I know that a lot just happened on this slide, so if you\'re not quite sure where all these numbers went, you know, feel free to pause the video you know, and so take a slow careful look at this big calculation that we just did and try to make sure that you understand the steps of what just happened to get us these numbers,fourteen, thirteen and eleven. Finally, let me show you a neat trick. Let\'s say we have a set of four houses so 4 houses with 4 sizes like these. And let\'s say I have a hypotheses for predicting what is the price of a house, and let\'s say I want to compute, you know, H of X for each of my 4 houses here. It turns out there\'s neat way of posing this, applying this hypothesis to all of my houses at the same time. It turns out there\'s a neat way to pose this as a Matrix Vector multiplication. So, here\'s how I\'m going to do it. I am going to construct a matrix as follows. My matrix is going to be 1111 times, and I\'m going to write down the sizes of my four houses here and I\'m going to construct a vector as well, And my vector is going to this vector of two elements, that\'s minus 40 and 0.25. That\'s these two co-efficients; data 0 and data 1. And what I am going to do is to take matrix and that vector and multiply them together, that times is that multiplication symbol. So what do I get? Well this is a four by two matrix. This is a two by one matrix. So the outcome is going to be a four by one vector, all right. So, let me, so this is going to be a 4 by 1 matrix is the outcome or really a four diminsonal vector, so let me write it as one of my four elements in my four real numbers here. Now it turns out and so this first element of this result, the way I am going to get that is, I am going to take this and multiply it by the vector. And so this is going to be -40 x 1 + 4.25 x 2104. By the way, on the earlier slides I was writing 1 x -40 and 2104 x 0.25, but the order doesn\'t matter, right? -40 x 1 is the same as 1 x -40. And this first element, of course, is ""H"" applied to 2104. So it\'s really the predicted price of my first house. Well, how about the second element? Hope you can see where I am going to get the second element. Right? I\'m gonna take this and multiply it by my vector. And so that\'s gonna be -40 x 1 + 0.25 x 1416. And so this is going be ""H"" of 1416. Right? And so on for the third and the fourth elements of this 4 x 1 vector. And just there, right? This thing here that I just drew the green box around, that\'s a real number, OK? That\'s a single real number, and this thing here that I drew the magenta box around--the purple, magenta color box around--that\'s a real number, right? And so this thing on the right--this thing on the right overall, this is a 4 by 1 dimensional matrix, was a 4 dimensional vector. And, the neat thing about this is that when you\'re actually implementing this in software--so when you have four houses and when you want to use your hypothesis to predict the prices, predict the price ""Y"" of all of these four houses. What this means is that, you know, you can write this in one line of code. When we talk about octave and program languages later, you can actually, you\'ll actually write this in one line of code. You write prediction equals my, you know, data matrix times parameters, right? Where data matrix is this thing here, and parameters is this thing here, and this times is a matrix vector multiplication. And if you just do this then this variable prediction - sorry for my bad handwriting - then just implement this one line of code assuming you have an appropriate library to do matrix vector multiplication. If you just do this, then prediction becomes this 4 by 1 dimensional vector, on the right, that just gives you all the predicted prices. And your alternative to doing this as a matrix vector multiplication would be to write eomething like , you know, for I equals 1 to 4, right? And you have say a thousand houses it would be for I equals 1 to a thousand or whatever. And then you have to write a prediction, you know, if I equals. and then do a bunch more work over there and it turns out that When you have a large number of houses, if you\'re trying to predict the prices of not just four but maybe of a thousand houses then it turns out that when you implement this in the computer, implementing it like this, in any of the various languages. This is not only true for Octave, but for Supra Server Java or Python, other high-level, other languages as well. It turns out, that, by writing code in this style on the left, it allows you to not only simplify the code, because, now, you\'re just writing one line of code rather than the form of a bunch of things inside. But, for subtle reasons, that we will see later, it turns out to be much more computationally efficient to make predictions on all of the prices of all of your houses doing it the way on the left than the way on the right than if you were to write your own formula. I\'ll say more about this later when we talk about vectorization, but, so, by posing a prediction this way, you get not only a simpler piece of code, but a more efficient one. So, that\'s it for matrix vector multiplication and we\'ll make good use of these sorts of operations as we develop the living regression in other models further. But, in the next video we\'re going to take this and generalize this to the case of matrix matrix multiplication.'",15,0,1
coursera,stanford_university,machine-learning,matrix-matrix-multiplication,"b""In this video we'll talk about\nmatrix-matrix multiplication, or how to multiply two matrices together. When we talk about the method in\nlinear regression for how to solve for the parameters theta 0 and\ntheta 1 all in one shot, without needing an iterative\nalgorithm like gradient descent. When we talk about that algorithm,\nit turns out that matrix-matrix multiplication is one\nof the key steps that you need to know. So let's, as usual, start with an example. Let's say I have two matrices and\nI want to multiply them together. Let me again just run\nthrough this example and then I'll tell you a little\nbit of what happened. So the first thing I'm gonna do is\nI'm going to pull out the first column of this matrix on the right. And I'm going to take this\nmatrix on the left and multiply it by a vector that\nis just this first column. And it turns out, if I do that,\nI'm going to get the vector 11, 9. So this is the same\nmatrix-vector multiplication as you saw in the last video. I worked this out in advance,\nso I know it's 11, 9. And then the second thing\nI want to do is I'm going to pull out the second column\nof this matrix on the right. And I'm then going to take\nthis matrix on the left, so take that matrix, and multiply it\nby that second column on the right. So again, this is a matrix-vector\nmultiplication step which you saw from the previous video. And it turns out that if you multiply this\nmatrix and this vector you get 10, 14. And by the way, if you want to practice\nyour matrix-vector multiplication, feel free to pause the video and\ncheck this product yourself. Then I'm just gonna take these two\nresults and put them together, and that'll be my answer. So it turns out the outcome of this\nproduct is gonna be a two by two matrix. And the way I'm gonna fill in this\nmatrix is just by taking my elements 11, 9, and plugging them here. And taking 10, 14 and plugging\nthem into the second column, okay? So that was the mechanics of how to\nmultiply a matrix by another matrix. You basically look at the second\nmatrix one column at a time and you assemble the answers. And again, we'll step through this\nmuch more carefully in a second. But I just want to point out also,\nthis first example is a 2x3 matrix. Multiply that by a 3x2 matrix,\nand the outcome of this product turns\nout to be a 2x2 matrix. And again, we'll see in\na second why this was the case. All right,\nthat was the mechanics of the calculation. Let's actually look at the details and\nlook at what exactly happened. Here are the details. I have a matrix A and I want to\nmultiply that with a matrix B and the result will be some new matrix C. It turns out you can only multiply\ntogether matrices whose dimensions match. So A is an m x n matrix,\nso m rows, n columns. And we multiply with an n x o matrix. And it turns out this n here\nmust match this n here. So the number of columns\nin the first matrix must equal to the number of\nrows in the second matrix. And the result of this product will be\na m x o matrix, like the matrix C here. And in the previous video everything we\ndid corresponded to the special case of o being equal to 1. That was to the case of B being a vector. But now we're gonna deal with the case\nof values of o larger than 1. So here's how you multiply\ntogether the two matrices. What I'm going to do is I'm going\nto take the first column of B and treat that as a vector, and multiply\nthe matrix A by the first column of B. And the result of that will be a n by 1\nvector, and I'm gonna put that over here. Then I'm gonna take the second\ncolumn of B, right? So this is another n by 1 vector. So this column here, this is n by 1. It's an n-dimensional vector. Gonna multiply this matrix\nwith this n by 1 vector. The result will be a m-dimensional vector,\nwhich we'll put there, and so on. And then I'm gonna take the third column,\nmultiply it by this matrix. I get a m-dimensional vector. And so on,\nuntil you get to the last column. The matrix times the last column\ngives you the last column of C. Just to say that again, the ith column of the matrix C is obtained\nby taking the matrix A and multiplying the matrix A with\nthe ith column of the matrix B for the values of i = 1, 2, up through o. So this is just a summary of what we did\nup there in order to compute the matrix C. Let's look at just one more example. Let's say I want to multiply\ntogether these two matrices. So what I'm going to do is first pull out\nthe first column of my second matrix. That was my matrix B on\nthe previous slide and I therefore have this\nmatrix times that vector. And so, oh,\nlet's do this calculation quickly. This is going to be equal to the 1, 3 x 0, 3, so that gives 1 x 0 + 3 x 3. And the second element is going to be 2,\n5 x 0, 3, so that's gonna be 2 x 0 + 5 x 3. And that is 9, 15. Oh, actually let me write that in green. So this is 9, 15. And then next I'm going to pull\nout the second column of this and do the corresponding calculations. So that's this matrix times this vector 1,\n2. Let's also do this quickly, so that's 1 x 1 + 3 x 2,\nso that was that row. And let's do the other one. So let's see, that gives me 2 x 1 + 5 x 2 and so that is going to be equal to, lets see, 1 x 1 + 3 x 1 is 7 and 2 x 1 + 5 x 2 is 12. So now I have these two and so my outcome, the product of these two matrices, is going to be this goes here and\nthis goes here. So I get 9, 15 and 4, 12.\n[It should be 7,12] And you may notice also that the result of\nmultiplying a 2x2 matrix with another 2x2 matrix, the resulting dimension is going\nto be that first 2 times that second 2. So the result is itself also a 2x2 matrix. Finally, let me show you one more neat\ntrick that you can do with matrix-matrix multiplication. Let's say, as before, that we have four\nhouses whose prices we wanna predict. Only now, we have three competing\nhypotheses shown here on the right. So if you want to apply all three competing hypotheses to all four\nof your houses, it turns out you can do that very efficiently using\na matrix-matrix multiplication. So here on the left is my usual matrix,\nsame as from the last video where these values are my housing prices\n[he means housing sizes]\nand I've put 1s here on the left as well. And what I am going to do is\nconstruct another matrix where here, the first column is this -40 and 0.25 and the second column is this 200,\n0.1 and so on. And it turns out that if you\nmultiply these two matrices, what you find is that this first column,\nI'll draw that in blue. Well, how do you get this first column? Our procedure for matrix-matrix\nmultiplication is, the way you get this first column is you take this matrix and\nyou multiply it by this first column. And we saw in the previous video that this\nis exactly the predicted housing prices of the first hypothesis, right,\nof this first hypothesis here. And how about the second column? Well, [INAUDIBLE] second column. The way you get the second column is,\nwell, you take this matrix and\nyou multiply it by this second column. And so the second column turns out to be the predictions of the second\nhypothesis up there, and similarly for the third column. And so\nI didn't step through all the details, but hopefully you can just feel free to pause\nthe video and check the math yourself and check that what I just\nclaimed really is true. But it turns out that by constructing\nthese two matrices, what you can therefore do is very quickly apply all 3\nhypotheses to all 4 house sizes to get all 12 predicted prices output\nby your 3 hypotheses on your 4 houses. So with just one matrix multiplication\nstep you managed to make 12 predictions. And even better, it turns out that in\norder to do that matrix multiplication, there are lots of good\nlinear algebra libraries in order to do this multiplication step for\nyou. And so pretty much any reasonable\nprogramming language that you might be using. Certainly all the top ten most\npopular programming languages will have great linear algebra libraries. And there'll be good linear algebra\nlibraries that are highly optimized in order to do that matrix-matrix\nmultiplication very efficiently. Including taking advantage of any sort of\nparallel computation that your computer may be capable of, whether your computer\nhas multiple cores or multiple processors. Or within a processor sometimes\nthere's parallelism as well called SIMD parallelism that\nyour computer can take care of. And there are very good\nfree libraries that you can use to do this matrix-matrix\nmultiplication very efficiently, so that you can very efficiently make lots\nof predictions with lots of hypotheses.""",16,0,1
coursera,stanford_university,machine-learning,matrix-multiplication-properties,"b""Matrix multiplication is really useful,\nsince you can pack a lot of computation into just one matrix\nmultiplication operation. But you should be careful\nof how you use them. In this video, I wanna tell you about\na few properties of matrix multiplication. When working with just real numbers or when working with scalars,\nmultiplication is commutative. And what I mean by that is\nthat if you take 3 times 5, that is equal to 5 times 3. And the ordering of this\nmultiplication doesn't matter. And this is called\nthe commutative property of multiplication of real numbers. It turns out this property, they can reverse the order in\nwhich you multiply things. This is not true for\nmatrix multiplication. So concretely, if A and B are matrices. Then in general,\nA times B is not equal to B times A. So, just be careful of that. Its not okay to arbitrarily reverse\nthe order in which you multiply matrices. Matrix multiplication in not commutative,\nis the fancy way of saying it. As a concrete example,\nhere are two matrices. This matrix 1 1 0 0 times 0 0 2 0 and if you multiply these two matrices\nyou get this result on the right. Now let's swap around the order\nof these two matrices. So I'm gonna take this two matrices and\njust reverse them. It turns out if you multiply\nthese two matrices, you get the second answer on the right. And well clearly, right, these two\nmatrices are not equal to each other. So, in fact, in general if you have\na matrix operation like A times B, if A is an m by n matrix, and B is an n by m matrix,\njust as an example. Then, it turns out that\nthe matrix A times B, right, is going to be an m by m matrix. Whereas the matrix B times A is\ngoing to be an n by n matrix. So the dimensions don't even match, right? So if A x B and\nB x A may not even be the same dimension. In the example on the left,\nI have all two by two matrices. So the dimensions were the same, but\nin general, reversing the order of the matrices can even change\nthe dimension of the outcome. So, matrix multiplication\nis not commutative. Here's the next property\nI want to talk about. So, when talking about real numbers or\nscalars, let's say I have 3 x 5 x 2. I can either multiply 5 x 2 first. Then I can compute this as 3 x 10. Or, I can multiply 3 x 5 first,\nand I can compute this as 15 x 2. And both of these give you\nthe same answer, right? Both of these is equal to 30. So it doesn't matter whether\nI multiply 5 x 2 first or whether I multiply 3 x 5 first,\nbecause sort of, well, 3 x (5 x 2) = (3 x 5) x 2. And this is called the associative\nproperty of real number multiplication. It turns out that matrix\nmultiplication is associative. So concretely, let's say I have\na product of three matrices A x B x C. Then, I can compute this\neither as A x (B x C) or I can computer this as (A x B) x C, and these will actually\ngive me the same answer. I'm not gonna prove this but\nyou can just take my word for it I guess. So just be clear,\nwhat I mean by these two cases. Let's look at the first one, right. This first case. What I mean by that is if you\nactually wanna compute A x B x C. What you can do is you\ncan first compute B x C. So that D = B x C then compute A x D. And so\nthis here is really computing A x B x C. Or, for this second case,\nyou can compute this as, you can set E = A x B,\nthen compute E times C. And this is then the same as A x B x C,\nand it turns out that both of these options will give you this\nguarantee to give you the same answer. And so we say that matrix multiplication\nthus enjoy the associative property. Okay? And don't worry about the terminology\nassociative and commutative. That's what it's called, but I'm not\nreally going to use this terminology later in this class, so\ndon't worry about memorizing those terms. Finally, I want to tell you\nabout the Identity Matrix, which is a special matrix. So let's again make the analogy\nto what we know of real numbers. When dealing with real numbers or\nscalar numbers, the number 1, you can think of it as\nthe identity of multiplication. And what I mean by that is that for any number z, 1 x z = z x 1. And that's just equal to the number z for\nany real number z. So 1 is the identity operation and\nso it satisfies this equation. So it turns out,\nthat this in the space of matrices there's an identity matrix as well and\nit's usually denoted I or sometimes we write it as I of n x n if we\nwant to make it explicit to dimensions. So I subscript n x n is\nthe n x n identity matrix. And so that's a different identity\nmatrix for each dimension n. And here are few examples. Here's the 2 x 2 identity matrix,\nhere's the 3 x 3 identity matrix, here's the 4 x 4 matrix. So the identity matrix has the property\nthat it has ones along the diagonals. All right, and so on. And 0 everywhere else. And so, by the way, the 1 x 1\nidentity matrix is just a number 1, and so the 1 x 1 matrix with just 1 in it. So it's not a very\ninteresting identity matrix. And informally, when I or\nothers are being sloppy, very often we'll write the identity\nmatrices in fine notation. We'll draw square brackets, just write\none one one dot dot dot dot one, and then we'll maybe somewhat sloppily\nwrite a bunch of zeros there. And these zeroes on the,\nthis big zero and this big zero, that's meant to denote that this matrix is\nzero everywhere except for the diagonal. So this is just how I might swap\nyou the right D identity matrix. And it turns out that the identity\nmatrix has its property that for any matrix A, A times identity\nequals I times A equals A so that's a lot like this\nequation that we have up here. Right? So 1 times z equals z\ntimes 1 equals z itself. So I times A equals A times I equals A. Just to make sure we have\nthe dimensions right. So if A is an m by n matrix, then this identity matrix here,\nthat's an n by n identity matrix. And if is and by then,\nthen this identity matrix, right? For matrix multiplication to make sense,\nthat has to be an m by m matrix. Because this m has the match up that m,\nand in either case, the outcome of this process is you get\nback the matrix A which is m by n. So whenever we write\nthe identity matrix I, you know, very often the dimension Mention, right,\nwill be implicit from the content. So these two I's, they're actually\ndifferent dimension matrices. One may be n by n, the other is n by m. But when we want to make the dimension\nof the matrix explicit, then sometimes we'll write to this I subscript\nn by n, kind of like we had up here. But very often,\nthe dimension will be implicit. Finally, I just wanna\npoint out that earlier I said that AB is not,\nin general, equal to BA. Right? For most matrices A and\nB, this is not true. But when B is the identity matrix,\nthis does hold true, that A times the identity matrix\ndoes indeed equal to identity times A is just that you know this is not\ntrue for other matrices B in general. So, that's it for the properties\nof matrix multiplication and special matrices like the identity\nmatrix I want to tell you about. In the next and\nfinal video on our linear algebra review, I'm going to quickly tell you about\na couple of special matrix operations and after that everything you need to know\nabout linear algebra for this class.""",17,0,1
coursera,stanford_university,machine-learning,inverse-and-transpose,"b""In this video, I want to tell you about a couple of special matrix operations, called the matrix inverse and the matrix transpose operation. Let's start by talking about matrix inverse, and as usual we'll start by thinking about how it relates to real numbers. In the last video, I said that the number one plays the role of the identity in the space of real numbers because one times anything is equal to itself. It turns out that real numbers have this property that very number have an, that each number has an inverse, for example, given the number three, there exists some number, which happens to be three inverse so that that number times gives you back the identity element one. And so to me, inverse of course this is just one third. And given some other number, maybe twelve there is some number which is the inverse of twelve written as twelve to the minus one, or really this is just one twelve. So that when you multiply these two things together. the product is equal to the identity element one again. Now it turns out that in the space of real numbers, not everything has an inverse. For example the number zero does not have an inverse, right? Because zero's a zero inverse, one over zero that's undefined. Like this one over zero is not well defined. And what we want to do, in the rest of this slide, is figure out what does it mean to compute the inverse of a matrix. Here's the idea: If A is a n by n matrix, and it has an inverse, I will say a bit more about that later, then the inverse is going to be written A to the minus one and A times this inverse, A to the minus one, is going to equal to A inverse times A, is going to give us back the identity matrix. Okay? Only matrices that are m by m for some the idea of M having inverse. So, a matrix is M by M, this is also called a square matrix and it's called square because the number of rows is equal to the number of columns. Right and it turns out only square matrices have inverses, so A is a square matrix, is m by m, on inverse this equation over here. Let's look at a concrete example, so let's say I have a matrix, three, four, two, sixteen. So this is a two by two matrix, so it's a square matrix and so this may just could have an and it turns out that I happen to know the inverse of this matrix is zero point four, minus zero point one, minus zero point zero five, zero zero seven five. And if I take this matrix and multiply these together it turns out what I get is the two by two identity matrix, I, this is I two by two. Okay? And so on this slide, you know this matrix is the matrix A, and this matrix is the matrix A-inverse. And it turns out if that you are computing A times A-inverse, it turns out if you compute A-inverse times A you also get back the identity matrix. So how did I find this inverse or how did I come up with this inverse over here? It turns out that sometimes you can compute inverses by hand but almost no one does that these days. And it turns out there is very good numerical software for taking a matrix and computing its inverse. So again, this is one of those things where there are lots of open source libraries that you can link to from any of the popular programming languages to compute inverses of matrices. Let me show you a quick example. How I actually computed this inverse, and what I did was I used software called Optive. So let me bring that up. We will see a lot about Optive later. Let me just quickly show you an example. Set my matrix A to be equal to that matrix on the left, type three four two sixteen, so that's my matrix A right. This is matrix 34, 216 that I have down here on the left. And, the software lets me compute the inverse of A very easily. It's like P over A equals this. And so, this is right, this matrix here on my four minus, on my one, and so on. This given the numerical solution to what is the inverse of A. So let me just write, inverse of A equals P inverse of A over that I can now just verify that A times A inverse the identity is, type A times the inverse of A and the result of that is this matrix and this is one one on the diagonal and essentially ten to the minus seventeen, ten to the minus sixteen, so Up to numerical precision, up to a little bit of round off error that my computer had in finding optimal matrices and these numbers off the diagonals are essentially zero so A times the inverse is essentially the identity matrix. Can also verify the inverse of A times A is also equal to the identity, ones on the diagonals and values that are essentially zero except for a little bit of round dot error on the off diagonals. If a definition that the inverse of a matrix is, I had this caveat first it must always be a square matrix, it had this caveat, that if A has an inverse, exactly what matrices have an inverse is beyond the scope of this linear algebra for review that one intuition you might take away that just as the number zero doesn't have an inverse, it turns out that if A is say the matrix of all zeros, then this matrix A also does not have an inverse because there's no matrix there's no A inverse matrix so that this matrix times some other matrix will give you the identity matrix so this matrix of all zeros, and there are a few other matrices with properties similar to this. That also don't have an inverse. But it turns out that in this review I don't want to go too deeply into what it means matrix have an inverse but it turns out for our machine learning application this shouldn't be an issue or more precisely for the learning algorithms where this may be an to namely whether or not an inverse matrix appears and I will tell when we get to those learning algorithms just what it means for an algorithm to have or not have an inverse and how to fix it in case. Working with matrices that don't have inverses. But the intuition if you want is that you can think of matrices as not have an inverse that is somehow too close to zero in some sense. So, just to wrap up the terminology, matrix that don't have an inverse Sometimes called a singular matrix or degenerate matrix and so this matrix over here is an example zero zero zero matrix. is an example of a matrix that is singular, or a matrix that is degenerate. Finally, the last special matrix operation I want to tell you about is to do matrix transpose. So suppose I have matrix A, if I compute the transpose of A, that's what I get here on the right. This is a transpose which is written and A superscript T, and the way you compute the transpose of a matrix is as follows. To get a transpose I am going to first take the first row of A one to zero. That becomes this first column of this transpose. And then I'm going to take the second row of A, 3 5 9, and that becomes the second column. of the matrix A transpose. And another way of thinking about how the computer transposes is as if you're taking this sort of 45 degree axis and you are mirroring or you are flipping the matrix along that 45 degree axis. so here's the more formal definition of a matrix transpose. Let's say A is a m by n matrix. And let's let B equal A transpose and so BA transpose like so. Then B is going to be a n by m matrix with the dimensions reversed so here we have a 2x3 matrix. And so the transpose becomes a 3x2 matrix, and moreover, the BIJ is equal to AJI. So the IJ element of this matrix B is going to be the JI element of that earlier matrix A. So for example, B 1 2 is going to be equal to, look at this matrix, B 1 2 is going to be equal to this element 3 1st row, 2nd column. And that equal to this, which is a two one, second row first column, right, which is equal to two and some\n[It should be 3] of the example B 3 2, right, that's B 3 2 is this element 9, and that's equal to a two three which is this element up here, nine. And so that wraps up the definition of what it means to take the transpose of a matrix and that in fact concludes our linear algebra review. So by now hopefully you know how to add and subtract matrices as well as multiply them and you also know how, what are the definitions of the inverses and transposes of a matrix and these are the main operations used in linear algebra for this course. In case this is the first time you are seeing this material. I know this was a lot of linear algebra material all presented very quickly and it's a lot to absorb but if you there's no need to memorize all the definitions we just went through and if you download the copy of either these slides or of the lecture notes from the course website. and use either the slides or the lecture notes as a reference then you can always refer back to the definitions and to figure out what are these matrix multiplications, transposes and so on definitions. And the lecture notes on the course website also has pointers to additional resources linear algebra which you can use to learn more about linear algebra by yourself. And next with these new tools. We'll be able in the next few videos to develop more powerful forms of linear regression that can view of a lot more data, a lot more features, a lot more training examples and later on after the new regression we'll actually continue using these linear algebra tools to derive more powerful learning algorithims as well""",18,0,1
coursera,stanford_university,machine-learning,multiple-features,"b'in this video we will start to talk about a new version of linear regression that\'s more powerful. One that works with multiple variables or with multiple features. Here\'s what I mean. In the original version of linear regression that we developed, we have a single feature x, the size of the house, and we wanted to use that to predict why the price of the house and this was our form of our hypothesis. But now imagine, what if we had not only the size of the house as a feature or as a variable of which to try to predict the price, but that we also knew the number of bedrooms, the number of house and the age of the home and years. It seems like this would give us a lot more information with which to predict the price. To introduce a little bit of notation, we sort of started to talk about this earlier, I\'m going to use the variables X subscript 1 X subscript 2 and so on to denote my, in this case, four features and I\'m going to continue to use Y to denote the variable, the output variable price that we\'re trying to predict. Let\'s introduce a little bit more notation. Now that we have four features I\'m going to use lowercase ""n"" to denote the number of features. So in this example we have n4 because we have, you know, one, two, three, four features. And ""n"" is different from our earlier notation where we were using ""n"" to denote the number of examples. So if you have 47 rows  ""M"" is the number of rows on this table or the number of training examples. So I\'m also going to use X superscript ""I"" to denote the input features of the ""I"" training example. As a concrete example let say X2 is going to be a vector of the features for my second training example. And so X2 here is going to be a vector 1416, 3, 2, 40 since those are my four features that I have to try to predict the price of the second house. So, in this notation, the superscript 2 here. That\'s an index into my training set. This is not X to the power of 2. Instead, this is, you know, an index that says look at the second row of this table. This refers to my second training example. With this notation X2 is a four dimensional vector. In fact, more generally, this is an in-dimensional feature back there. With this notation, X2 is now a vector and so, I\'m going to use also Xi subscript J to denote the value of the J, of feature number J and the training example. So concretely X2 subscript 3, will refer to feature number three in the x factor which is equal to 2,right? That was a 3 over there, just fix my handwriting. So x2 subscript 3 is going to be equal to 2. Now that we have multiple features, let\'s talk about what the form of our hypothesis should be. Previously this was the form of our hypothesis, where x was our single feature, but now that we have multiple features, we aren\'t going to use the simple representation any more. Instead, a form of the hypothesis in linear regression is going to be this, can be theta 0 plus theta 1 x1 plus theta 2 x2 plus theta 3 x3 plus theta 4 X4. And if we have N features then rather than summing up over our four features, we would have a sum over our N features. Concretely for a particular setting of our parameters we may have H of X 80 + 0.1 X1 +  0.01x2 + 3x3 - 2x4. This would be one example of a hypothesis and you remember a hypothesis is trying to predict the price of the house in thousands of dollars, just saying that, you know, the base price of a house is maybe 80,000 plus another open 1, so that\'s an extra, what, hundred dollars per square feet, yeah, plus the price goes up a little bit for each additional floor that the house has. X two is the number of floors, and it goes up further for each additional bedroom the house has, because X three was the number of bedrooms, and the price goes down a little bit with each additional age of the house. With each additional year of the age of the house. Here\'s the form of a hypothesis rewritten on the slide. And what I\'m gonna do is introduce a little bit of notation to simplify this equation. For convenience of notation, let me define x subscript 0 to be equals one. Concretely, this means that for every example i I have a feature vector X superscript I and X superscript I subscript 0 is going to be equal to 1. You can think of this as defining an additional zero feature. So whereas previously I had n features because x1, x2 through xn, I\'m now defining an additional sort of zero feature vector that always takes on the value of one. So now my feature vector X becomes this N+1 dimensional vector that is zero index. So this is now a n+1 dimensional feature vector, but I\'m gonna index it from 0 and I\'m also going to think of my parameters as a vector. So, our parameters here, right that would be our theta zero, theta one, theta two, and so on all the way up to theta n, we\'re going to gather them up into a parameter vector written theta 0, theta 1, theta 2, and so on, down to theta n. This is another zero index vector. It\'s of index signed from zero. That is another n plus 1 dimensional vector. So, my hypothesis cannot be written theta 0x0 plus theta 1x1+ up to theta n Xn. And this equation is the same as this on top because, you know, eight zero is equal to one. Underneath and I now take this form of the hypothesis and write this as either transpose x, depending on how familiar you are with inner products of vectors if you write what theta transfers x is what theta transfer and this is theta zero, theta one, up to theta N. So this thing here is theta transpose and this is actually a N plus one by one matrix.\n[It should be a 1 by (n+1) matrix] It\'s also called a row vector and you take that and multiply it with the vector X which is X zero, X one, and so on, down to X n. And so, the inner product that is theta transpose X is just equal to this. This gives us a convenient way to write the form of the hypothesis as just the inner product between our parameter vector theta and our theta vector X. And it is this little bit of notation, this little excerpt of the notation convention that let us write this in this compact form. So that\'s the form of a hypthesis when we have multiple features. And, just to give this another name, this is also called multivariate linear regression. And the term multivariable that\'s just maybe a fancy term for saying we have multiple features, or multivariables with which to try to predict the value Y.'",19,0,1
coursera,stanford_university,machine-learning,gradient-descent-for-multiple-variables,"b""In the previous video, we talked about\nthe form of the hypothesis for linear regression with multiple features\nor with multiple variables. In this video, let's talk about how to\nfit the parameters of that hypothesis. In particular let's talk about how\nto use gradient descent for linear regression with multiple features. To quickly summarize our notation,\nthis is our formal hypothesis in multivariable linear regression where\nwe've adopted the convention that x0=1. The parameters of this model are theta0\nthrough theta n, but instead of thinking of this as n separate parameters, which\nis valid, I'm instead going to think of the parameters as theta where theta\nhere is a n+1-dimensional vector. So I'm just going to think of the\nparameters of this model as itself being a vector. Our cost function is J of theta0 through\ntheta n which is given by this usual sum of square of error term. But again\ninstead of thinking of J as a function of these n+1 numbers, I'm going to\nmore commonly write J as just a function of the parameter vector theta\nso that theta here is a vector. Here's what gradient descent looks like.\nWe're going to repeatedly update each parameter theta j according to theta j\nminus alpha times this derivative term. And once again we just write this as\nJ of theta, so theta j is updated as theta j minus the learning rate\nalpha times the derivative, a partial derivative of the cost function with\nrespect to the parameter theta j. Let's see what this looks like when\nwe implement gradient descent and, in particular, let's go see what that\npartial derivative term looks like. Here's what we have for gradient descent\nfor the case of when we had N=1 feature. We had two separate update rules for\nthe parameters theta0 and theta1, and hopefully these look familiar to you.\nAnd this term here was of course the partial derivative of the cost function\nwith respect to the parameter of theta0, and similarly we had a different\nupdate rule for the parameter theta1. There's one little difference which is\nthat when we previously had only one feature, we would call that feature x(i)\nbut now in our new notation we would of course call this \nx(i)<u>1 to denote our one feature.</u> So that was for when\nwe had only one feature. Let's look at the new algorithm for\nwe have more than one feature, where the number of features n\nmay be much larger than one. We get this update rule for gradient\ndescent and, maybe for those of you that know calculus, if you take the\ndefinition of the cost function and take the partial derivative of the cost\nfunction J with respect to the parameter theta j, you'll find that that partial\nderivative is exactly that term that I've drawn the blue box around. And if you implement this you will\nget a working implementation of gradient descent for\nmultivariate linear regression. The last thing I want to do on\nthis slide is give you a sense of why these new and old algorithms are\nsort of the same thing or why they're both similar algorithms or why they're\nboth gradient descent algorithms. Let's consider a case\nwhere we have two features or maybe more than two features,\nso we have three update rules for the parameters theta0, theta1, theta2\nand maybe other values of theta as well. If you look at the update rule for\ntheta0, what you find is that this update rule here is the same as\nthe update rule that we had previously for the case of n = 1. And the reason that they are\nequivalent is, of course, because in our notational convention we\nhad this x(i)<u>0 = 1 convention, which is</u> why these two term that I've drawn the\nmagenta boxes around are equivalent. Similarly, if you look the update\nrule for theta1, you find that this term here is equivalent to\nthe term we previously had, or the equation or the update\nrule we previously had for theta1, where of course we're just using\nthis new notation x(i)<u>1 to denote</u> our first feature, and now that we have\nmore than one feature we can have similar update rules for the other\nparameters like theta2 and so on. There's a lot going on on this slide\nso I definitely encourage you if you need to to pause the video\nand look at all the math on this slide slowly to make sure you understand\neverything that's going on here. But if you implement the algorithm\nwritten up here then you have a working implementation of linear\nregression with multiple features.""",20,0,1
coursera,stanford_university,machine-learning,gradient-descent-in-practice-i-feature-scaling,"b""In this video and in the video after this one, I wanna tell you about some of the practical tricks for making gradient descent work well. In this video, I want to tell you about an idea called feature skill. Here's the idea. If you have a problem where you have multiple features, if you make sure that the features are on a similar scale, by which I mean make sure that the different features take on similar ranges of values, then gradient descents can converge more quickly. Concretely let's say you have a problem with two features where X1 is the size of house and takes on values between say zero to two thousand and two is the number of bedrooms, and maybe that takes on values between one and five. If you plot the contours of the cos function J of theta, then the contours may look like this, where, let's see, J of theta is a function of parameters theta zero, theta one and theta two. I'm going to ignore theta zero, so let's about theta 0 and pretend as a function of only theta 1 and theta 2, but if x1 can take on them, you know, much larger range of values and x2 It turns out that the contours of the cause function J of theta can take on this very very skewed elliptical shape, except that with the so 2000 to 5 ratio, it can be even more secure. So, this is very, very tall and skinny ellipses, or these very tall skinny ovals, can form the contours of the cause function J of theta. And if you run gradient descents on this cos-function, your gradients may end up taking a long time and can oscillate back and forth and take a long time before it can finally find its way to the global minimum. In fact, you can imagine if these contours are exaggerated even more when you draw incredibly skinny, tall skinny contours, and it can be even more extreme than, then, gradient descent just have a much harder time taking it's way, meandering around, it can take a long time to find this way to the global minimum. In these settings, a useful thing to do is to scale the features. Concretely if you instead define the feature X one to be the size of the house divided by two thousand, and define X two to be maybe the number of bedrooms divided by five, then the count well as of the cost function J can become much more, much less skewed so the contours may look more like circles. And if you run gradient descent on a cost function like this, then gradient descent, you can show mathematically, you can find a much more direct path to the global minimum rather than taking a much more convoluted path where you're sort of trying to follow a much more complicated trajectory to get to the global minimum. So, by scaling the features so that there are, the consumer ranges of values. In this example, we end up with both features, X one and X two, between zero and one. You can wind up with an implementation of gradient descent. They can convert much faster. More generally, when we're performing feature scaling, what we often want to do is get every feature into approximately a  -1 to +1 range and concretely, your feature x0 is always equal to 1. So, that's already in that range, but you may end up dividing other features by different numbers to get them to this range. The numbers -1 and +1 aren't too important. So, if you have a feature, x1 that winds up being between zero and three, that's not a problem. If you end up having a different feature that winds being between -2 and  + 0.5, again, this is close enough to minus one and plus one that, you know, that's fine, and that's fine. It's only if you have a different feature, say X 3 that is between, that ranges from -100 tp +100 , then, this is a very different values than minus 1 and plus 1. So, this might be a less well-skilled feature and similarly, if your features take on a very, very small range of values so if X 4 takes on values between minus 0.0001 and positive 0.0001, then again this takes on a much smaller range of values than the minus one to plus one range. And again I would consider this feature poorly scaled. So you want the range of values, you know, can be bigger than plus or smaller than plus one, but just not much bigger, like plus 100 here, or too much smaller like 0.00 one over there. Different people have different rules of thumb. But the one that I use is that if a feature takes on the range of values from say minus three the plus 3 how you should think that should be just fine, but maybe it takes on much larger values than plus 3 or minus 3 unless not to worry and if it takes on values from say minus one-third to one-third. You know, I think that's fine too or 0 to one-third or minus one-third to 0. I guess that's typical range of value sector 0 okay. But it will take on a much tinier range of values like x4 here than gain on mine not to worry. So, the take-home message is don't worry if your features are not exactly on the same scale or exactly in the same range of values. But so long as they're all close enough to this gradient descent it should work okay. In addition to dividing by so that the maximum value when performing feature scaling sometimes people will also do what's called mean normalization. And what I mean by that is that you want to take a feature Xi and replace it with Xi minus new i to make your features have approximately 0 mean. And obviously we want to apply this to the future x zero, because the future x zero is always equal to one, so it cannot have an average value of zero. But it concretely for other features if the range of sizes of the house takes on values between 0 to 2000 and if you know, the average size of a house is equal to 1000 then you might use this formula. Size, set the feature X1 to the size minus the average value divided by 2000 and similarly, on average if your houses have one to five bedrooms and if on average a house has two bedrooms then you might use this formula to mean normalize your second feature x2. In both of these cases, you therefore wind up with features x1 and x2. They can take on values roughly between minus .5 and positive .5. Exactly not true - X2 can actually be slightly larger than .5 but, close enough. And the more general rule is that you might take a feature X1 and replace it with X1 minus mu1 over S1 where to define these terms mu1 is the average value of x1 in the training sets and S1 is the range of values of that feature and by range, I mean let's say the maximum value minus the minimum value or for those of you that understand the deviation of the variable is setting S1 to be the standard deviation of the variable would be fine, too. But taking, you know, this max minus min would be fine. And similarly for the second feature, x2, you replace x2 with this sort of subtract the mean of the feature and divide it by the range of values meaning the max minus min. And this sort of formula will get your features, you know, maybe not exactly, but maybe roughly into these sorts of ranges, and by the way, for those of you that are being super careful technically if we're taking the range as max minus min this five here will actually become a four. So if max is 5 minus 1 then the range of their own values is actually equal to 4, but all of these are approximate and any value that gets the features into anything close to these sorts of ranges will do fine. And the feature scaling doesn't have to be too exact, in order to get gradient descent to run quite a lot faster. So, now you know about feature scaling and if you apply this simple trick, it and make gradient descent run much faster and converge in a lot fewer other iterations. That was feature scaling. In the next video, I'll tell you about another trick to make gradient descent work well in practice.""",21,0,1
coursera,stanford_university,machine-learning,gradient-descent-in-practice-ii-learning-rate,"b""In this video,\nI want to give you more practical tips for getting gradient descent to work. The ideas in this video will center\naround the learning rate alpha. Concretely, here's the gradient\ndescent update rule. And what I want to do in\nthis video is tell you about what I think of as debugging,\nand some tips for making sure that gradient\ndescent is working correctly. And second, I wanna tell you how to\nchoose the learning rate alpha or at least how I go about choosing it. Here's something that I often do to make\nsure that gradient descent is working correctly. The job of gradient descent is\nto find the value of theta for you that hopefully minimizes\nthe cost function J(theta). What I often do is therefore plot\nthe cost function J(theta) as gradient descent runs. So the x axis here is a number of\niterations of gradient descent and as gradient descent runs you hopefully\nget a plot that maybe looks like this. Notice that the x axis\nis number of iterations. Previously we where looking at plots\nof J(theta) where the x axis, where the horizontal axis, was the parameter\nvector theta but this is not what this is. Concretely, what this point is, is I'm going to run gradient descent for\n100 iterations. And whatever value I get for\ntheta after 100 iterations, I'm going to get some value of\ntheta after 100 iterations. And I'm going to evaluate\nthe cost function J(theta). For the value of theta I\nget after 100 iterations, and this vertical height\nis the value of J(theta). For the value of theta I got after\n100 iterations of gradient descent. And this point here that corresponds\nto the value of J(theta) for the theta that I get after I've run\ngradient descent for 200 iterations. So what this plot is showing is, is it's\nshowing the value of your cost function after each iteration of gradient decent. And if gradient is working\nproperly then J(theta) should decrease after every iteration. And one useful thing that this sort of\nplot can tell you also is that if you look at the specific figure that I've drawn,\nit looks like by the time you've gotten out to maybe 300 iterations,\nbetween 300 and 400 iterations, in this segment it looks like\nJ(theta) hasn't gone down much more. So by the time you get to 400 iterations, it looks like this curve\nhas flattened out here. And so way out here 400 iterations,\nit looks like gradient descent has more or less converged because your cost\nfunction isn't going down much more. So looking at this figure can\nalso help you judge whether or not gradient descent has converged. By the way, the number of iterations\nthe gradient descent takes to converge for a physical application can vary a lot,\nso maybe for one application, gradient descent may\nconverge after just thirty iterations. For a different application, gradient\ndescent may take 3,000 iterations, for another learning algorithm,\nit may take 3 million iterations. It turns out to be very difficult to tell\nin advance how many iterations gradient descent needs to converge. And is usually by plotting this sort of\nplot, plotting the cost function as we increase in number in iterations,\nis usually by looking at these plots. But I try to tell if gradient\ndescent has converged. It's also possible to come up\nwith automatic convergence test, namely to have a algorithm try to tell\nyou if gradient descent has converged. And here's maybe a pretty typical example\nof an automatic convergence test. And such a test may declare convergence\nif your cost function J(theta) decreases by less than\nsome small value epsilon, some small value 10 to\nthe minus 3 in one iteration. But I find that usually choosing what\nthis threshold is is pretty difficult. And so in order to check your\ngradient descent's converge I actually tend to look at plots like\nthese, like this figure on the left, rather than rely on\nan automatic convergence test. Looking at this sort of figure can also\ntell you, or give you an advance warning, if maybe gradient descent\nis not working correctly. Concretely, if you plot J(theta) as\na function of the number of iterations. Then if you see a figure like this\nwhere J(theta) is actually increasing, then that gives you a clear sign that\ngradient descent is not working. And a theta like this usually means that\nyou should be using learning rate alpha. If J(theta) is actually increasing,\nthe most common cause for that is if you're trying to minimize\na function, that maybe looks like this. But if your learning rate is too\nbig then if you start off there, gradient descent may overshoot\nthe minimum and send you there. And if the learning rate is too big, you may overshoot again and\nit sends you there, and so on. So that, what you really wanted was for\nit to start here and for it to slowly go downhill, right? But if the learning rate is too big, then gradient descent can instead\nkeep on overshooting the minimum. So that you actually end\nup getting worse and worse instead of getting to higher\nvalues of the cost function J(theta). So you end up with a plot like this and\nif you see a plot like this, the fix is usually just to\nuse a smaller value of alpha. Oh, and also, of course, make sure\nyour code doesn't have a bug of it. But usually too large a value of\nalpha could be a common problem. Similarly sometimes you may also see\nJ(theta) do something like this, it may go down for a while then go up then\ngo down for a while then go up go down for a while go up and so on. And a fix for something like this is\nalso to use a smaller value of alpha. I'm not going to prove it here, but under other assumptions about the cost\nfunction J, that does hold true for linear regression, mathematicians\nhave shown that if your learning rate alpha is small enough, then J(theta)\nshould decrease on every iteration. So if this doesn't happen probably\nmeans the alpha's too big, you should set it smaller. But of course, you also don't want\nyour learning rate to be too small because if you do that then the gradient\ndescent can be slow to converge. And if alpha were too small,\nyou might end up starting out here, say, and end up taking just\nminuscule baby steps. And just taking a lot of iterations\nbefore you finally get to the minimum, and so if alpha is too small, gradient\ndescent can make very slow progress and be slow to converge. To summarize,\nif the learning rate is too small, you can have a slow convergence problem,\nand if the learning rate is too large, J(theta) may not decrease on every\niteration and it may not even converge. In some cases if the learning rate is too\nlarge, slow convergence is also possible. But the more common problem you see is just that J(theta) may not\ndecrease on every iteration. And in order to debug all of these things,\noften plotting that J(theta) as a function of the number of iterations\ncan help you figure out what's going on. Concretely, what I actually do\nwhen I run gradient descent is I would try a range of values. So just try running gradient\ndescent with a range of values for alpha, like 0.001 and 0.01. So these are factor of ten differences. And for these different values of alpha\nare just plot J(theta) as a function of number of iterations, and then pick the value of alpha that seems to\nbe causing J(theta) to decrease rapidly. In fact, what I do actually\nisn't these steps of ten. So this is a scale factor\nof ten of each step up. What I actually do is try\nthis range of values. And so on, where this is 0.001. I'll then increase the learning\nrate threefold to get 0.003. And then this step up, this is another roughly threefold\nincrease from 0.003 to 0.01. And so these are, roughly,\ntrying out gradient descents with each value I try being about 3x\nbigger than the previous value. So what I'll do is try a range\nof values until I've found one value that's too small and made sure that\nI've found one value that's too large. And then I'll sort of try to pick\nthe largest possible value, or just something slightly smaller than\nthe largest reasonable value that I found. And when I do that usually it just gives\nme a good learning rate for my problem. And if you do this too, maybe you'll be\nable to choose a good learning rate for your implementation of gradient descent.""",22,0,1
coursera,stanford_university,machine-learning,features-and-polynomial-regression,"b""You now know about linear regression with multiple variables. In this video, I wanna tell you a bit about the choice of features that you have and how you can get different learning algorithm, sometimes very powerful ones by choosing appropriate features. And in particular I also want to tell you about polynomial regression allows you to use the machinery of linear regression to fit very complicated, even very non-linear functions. Let's take the example of predicting the price of the house. Suppose you have two features, the frontage of house and the depth of the house. So, here's the picture of the house we're trying to sell. So, the frontage is defined as this distance is basically the width or the length of how wide your lot is if this that you own, and the depth of the house is how deep your property is, so there's a frontage, there's a depth. called frontage and depth. You might build a linear regression model like this where frontage is your first feature x1 and and depth is your second feature x2, but when you're applying linear regression, you don't necessarily have to use just the features x1 and x2 that you're given. What you can do is actually create new features by yourself. So, if I want to predict the price of a house, what I might do instead is decide that what really determines the size of the house is the area or the land area that I own. So, I might create a new feature. I'm just gonna call this feature x which is frontage, times depth. This is a multiplication symbol. It's a frontage x depth because this is the land area that I own and I might then select my hypothesis as that using just one feature which is my land area, right? Because the area of a rectangle is you know, the product of the length of the size So, depending on what insight you might have into a particular problem, rather than just taking the features [xx] that we happen to have started off with, sometimes by defining new features you might actually get a better model. Closely related to the idea of choosing your features is this idea called polynomial regression. Let's say you have a housing price data set that looks like this. Then there are a few different models you might fit to this. One thing you could do is fit a quadratic model like this. It doesn't look like a straight line fits this data very well. So maybe you want to fit a quadratic model like this where you think the size, where you think the price is a quadratic function and maybe that'll give you, you know, a fit to the data that looks like that. But then you may decide that your quadratic model doesn't make sense because of a quadratic function, eventually this function comes back down and well, we don't think housing prices should go down when the size goes up too high. So then maybe we might choose a different polynomial model and choose to use instead a cubic function, and where we have now a third-order term and we fit that, maybe we get this sort of model, and maybe the green line is a somewhat better fit to the data cause it doesn't eventually come back down. So how do we actually fit a model like this to our data? Using the machinery of multivariant linear regression, we can do this with a pretty simple modification to our algorithm. The form of the hypothesis we, we know how the fit looks like this, where we say H of x is theta zero plus theta one x one plus x two theta X3. And if we want to fit this cubic model that I have boxed in green, what we're saying is that to predict the price of a house, it's theta 0 plus theta 1 times the size of the house plus theta 2 times the square size of the house. So this term is equal to that term. And then plus theta 3 times the cube of the size of the house raises that third term. In order to map these two definitions to each other, well, the natural way to do that is to set the first feature x one to be the size of the house, and set the second feature x two to be the square of the size of the house, and set the third feature x three to be the cube of the size of the house. And, just by choosing my three features this way and applying the machinery of linear regression, I can fit this model and end up with a cubic fit to my data. I just want to point out one more thing, which is that if you choose your features like this, then feature scaling becomes increasingly important. So if the size of the house ranges from one to a thousand, so, you know, from one to a thousand square feet, say, then the size squared of the house will range from one to one million, the square of a thousand, and your third feature x cubed, excuse me you, your third feature x three, which is the size cubed of the house, will range from one two ten to the nine, and so these three features take on very different ranges of values, and it's important to apply feature scaling if you're using gradient descent to get them into comparable ranges of values. Finally, here's one last example of how you really have broad choices in the features you use. Earlier we talked about how a quadratic model like this might not be ideal because, you know, maybe a quadratic model fits the data okay, but the quadratic function goes back down and we really don't want, right, housing prices that go down, to predict that, as the size of housing freezes. But rather than going to a cubic model there, you have, maybe, other choices of features and there are many possible choices. But just to give you another example of a reasonable choice, another reasonable choice might be to say that the price of a house is theta zero plus theta one times the size, and then plus theta two times the square root of the size, right? So the square root function is this sort of function, and maybe there will be some value of theta one, theta two, theta three, that will let you take this model and, for the curve that looks like that, and, you know, goes up, but sort of flattens out a bit and doesn't ever come back down. And, so, by having insight into, in this case, the shape of a square root function, and, into the shape of the data, by choosing different features, you can sometimes get better models. In this video, we talked about polynomial regression. That is, how to fit a polynomial, like a quadratic function, or a cubic function, to your data. Was also throw out this idea, that you have a choice in what features to use, such as that instead of using the frontish and the depth of the house, maybe, you can multiply them together to get a feature that captures the land area of a house. In case this seems a little bit bewildering, that with all these different feature choices, so how do I decide what features to use. Later in this class, we'll talk about some algorithms were automatically choosing what features are used, so you can have an algorithm look at the data and automatically choose for you whether you want to fit a quadratic function, or a cubic function, or something else. But, until we get to those algorithms now I just want you to be aware that you have a choice in what features to use, and by designing different features you can fit more complex functions your data then just fitting a straight line to the data and in particular you can put polynomial functions as well and sometimes by appropriate insight into the feature simply get a much better model for your data.""",23,0,1
coursera,stanford_university,machine-learning,normal-equation,"b'In this video, we\'ll talk about the normal equation, which for some linear regression problems, will give us a much better way to solve for the optimal value of the parameters theta. Concretely, so far the algorithm that we\'ve been using for linear regression is gradient descent where in order to minimize the cost function J of Theta, we would take this iterative algorithm that takes many steps, multiple iterations of gradient descent to converge to the global minimum. In contrast, the normal equation would give us a method to solve for theta analytically, so that rather than needing to run this iterative algorithm, we can instead just solve for the optimal value for theta all at one go, so that in basically one step you get to the optimal value right there. It turns out the normal equation that has some advantages and some disadvantages, but before we get to that and talk about when you should use it, let\'s get some intuition about what this method does. For this week\'s planetary example, let\'s imagine, let\'s take a very simplified cost function J of Theta, that\'s just the function of a real number Theta. So, for now, imagine that Theta is just a scalar value or that Theta is just a row value. It\'s just a number, rather than a vector. Imagine that we have a cost function J\nthat\'s a quadratic function of this real value parameter Theta, so J of Theta\nlooks like that. Well, how do you minimize\na quadratic function? For those of you that know\na little bit of calculus, you may know that the way to minimize a function is to take derivatives and to set derivatives equal to zero. So, you take the derivative of J\nwith respect to the parameter of Theta. You get some formula\nwhich I am not going to derive, you set that derivative equal to zero, and this allows you to solve for the value of Theda that\nminimizes J of Theta. That was a simpler case of when data was just real number. In the problem that we are\ninterested in, Theta is no longer just a real number, but, instead, is this n+1-dimensional parameter vector, and, a cost function J is a function of this vector value or Theta 0 through Theta m. And, a cost function looks like this,\nsome square cost function on the right. How do we minimize this cost function J? Calculus actually tells us that, if you, that one way to do so, is to take the partial derivative of J, with respect to every parameter of Theta J in turn, and then, to set all of these to 0. If you do that, and you solve for the values of Theta 0, Theta 1, up to Theta N, then, this would give you that values of Theta to minimize the cost function J.  Where, if you actually work through the calculus and work through the solution to the parameters Theta 0 through Theta N, the derivation ends up being somewhat involved. And, what I am going to do in this video, is actually to not go through the derivation, which is kind of long and kind of involved, but what I want to do is just tell you what you need to know in order to implement this process so you can solve for the values of the thetas that corresponds to where the partial derivatives is equal to zero. Or alternatively, or equivalently, the values of Theta is that minimize the cost function J of Theta. I realize that some of the comments I made that made more sense only to those of you that are normally familiar with calculus. So, but if you don\'t know, if you\'re less familiar with calculus, don\'t worry about it. I\'m just going to tell you what you need to know in order to implement this algorithm and get it to work. For the example that I want to use as a running example let\'s say that I have m = 4 training examples. In order to implement this normal equation at big, what I\'m going to do\nis the following. I\'m going to take my data set, so here are my four training examples. In this case let\'s assume that, you know, these four examples is all the data I have. What I am going to do is take my data set and add an extra column that corresponds to my extra feature, x0, that is always takes on this value of 1. What I\'m going to do is I\'m then going to construct a matrix called X that\'s a matrix are basically contains all of the features from my training data, so completely here is my here are all my features and we\'re going to take all those numbers and put them into this matrix ""X"", okay? So just, you know, copy the data over one column at a time and then I am going to do\nsomething similar for y\'s. I am going to take the values that I\'m trying to predict and construct now a vector, like so and call that a vector y. So X is going to be a m by (n+1) - dimensional matrix, and Y is going to be a m-dimensional vector where m is the number of\ntraining examples and n is, n is a number of features, n+1, because of this extra feature X0 that I had. Finally if you take your matrix X and you take your vector Y, and if you just compute this, and set theta to be equal to X transpose X inverse times X transpose Y, this would give you the value of theta that minimizes your cost function. There was a lot that happened on the slides and I work through it using\none specific example of one dataset. Let me just write this out in a slightly more general form and then let me just,\nand later on in this video let me explain\nthis equation a little bit more. It is not yet entirely clear how to do this. In a general case, let us say we have M training examples so X1, Y1 up to Xn, Yn and n features. So, each of the training example x(i) may looks like a vector like this, that is a n+1 dimensional feature vector. The way I\'m going to construct the matrix ""X"", this is also called the design matrix is as follows. Each training example gives me a feature vector like this. say, sort of n+1 dimensional vector. The way I am going to construct my design matrix x is only construct the matrix like this. and what I\'m going to do is take the first training example, so that\'s a vector, take its transpose so it ends up being this, you know, long flat thing and make x1 transpose the first row of my design matrix. Then I am going to take my second training example, x2, take the transpose of that and put that as the second row of x and so on, down until my last training example. Take the transpose of that, and that\'s my last row of my matrix X. And, so, that makes my matrix X, an M by N +1 dimensional matrix. As a concrete example, let\'s say I have only one feature, really, only one feature other than X zero, which is always equal to 1. So if my feature vectors X-i are equal to this 1, which is X-0, then some real feature, like maybe the size of the house, then my design matrix, X, would be equal to this. For the first row, I\'m going to basically take this and take its transpose. So, I\'m going to end up with 1, and then X-1-1. For the second row, we\'re going to end up with 1 and then X-1-2 and so on down to 1, and then X-1-M. And thus, this will be a m by 2-dimensional matrix. So, that\'s how to construct the matrix X. And, the vector Y--sometimes I might write an arrow on top to denote that it is a vector, but very often I\'ll just write this as Y, either way. The vector Y is obtained by taking all all the labels, all the correct prices of houses in my training set, and just stacking them up into an M-dimensional vector, and that\'s Y.  Finally, having constructed the matrix X and the vector Y, we then just compute theta as X\'(1/X) x X\'Y. I just want to make I just want to make sure that\nthis equation makes sense to you and that you know how to implement it. So, you know, concretely, what is this X\'(1/X)? Well, X\'(1/X) is the inverse of the matrix X\'X. Concretely, if you were to say set A to be equal to X\' x X, so X\' is a matrix, X\' x X gives you another matrix, and we call that matrix A. Then, you know, X\'(1/X) is just you take this matrix A and you invert it, right! This gives, let\'s say 1/A. And so that\'s how you compute this thing. You compute X\'X and then you compute its inverse. We haven\'t yet talked about Octave. We\'ll do so in the later set of videos, but in the Octave programming language or a similar view, and also the matlab programming language is very similar. The command to compute this quantity, X transpose X inverse times X transpose Y, is as follows. In Octave X prime is the notation that you use to denote X transpose. And so, this expression that\'s boxed in red, that\'s computing X transpose times X. pinv is a function for computing the inverse of a matrix, so this computes X transpose X inverse, and then you multiply that by X transpose, and you multiply that by Y. So you end computing that formula which I didn\'t prove, but it is possible to show mathematically even though I\'m not going to do so here, that this formula gives you the optimal value of theta in the sense that if you set theta equal to this, that\'s the value of theta that minimizes the cost function J of theta for the new regression. One last detail in the earlier video. I talked about the feature skill and the idea of getting features to be on similar ranges of Scales of similar ranges of values of each other. If you are using this normal equation method then feature scaling isn\'t actually necessary and is actually okay if, say, some feature X one is between zero and one, and some feature X two is between ranges from zero to one thousand and some feature x three ranges from zero to ten to the minus five and if you are using the normal equation method this is okay and there is no need to do features scaling, although of course if you are using gradient descent, then, features scaling is still important. Finally, where should you use the gradient descent and when should you use the normal equation method. Here are some of the their advantages and disadvantages. Let\'s say you have m training examples and n features. One disadvantage of gradient descent is that, you need to choose the learning rate Alpha. And, often, this means running it few times with different learning rate alphas and then seeing what works best. And so that is sort of extra work and extra hassle. Another disadvantage with gradient descent is it needs many more iterations. So, depending on the details, that could make it slower, although there\'s more to the story as we\'ll see in a second. As for the normal equation, you don\'t need to\nchoose any learning rate alpha. So that, you know, makes it really convenient,\nmakes it simple to implement. You just run it and it usually just works. And you don\'t need to iterate, so, you don\'t need to plot J of Theta or check the convergence or take all those extra steps. So far, the balance seems to favor normal the normal equation. Here are some disadvantages of the normal equation, and some advantages of gradient descent. Gradient descent works pretty well, even when you have a very large number of features. So, even if you have millions of features you can run gradient descent and it will be reasonably efficient. It will do something reasonable. In contrast to normal equation, In, in order to solve for the parameters data, we need to solve for this term. We need to compute this term, X transpose, X inverse. This matrix X transpose X. That\'s an n by n matrix,\nif you have n features. Because, if you look at the dimensions of X transpose the dimension of X, you multiply, figure out what the dimension of the product is, the matrix X transpose X is an n by n matrix where n is the number of features, and for almost computed implementations the cost of inverting the matrix, rose roughly as the cube of the dimension of the matrix. So, computing this inverse costs, roughly order, and cube time. Sometimes, it\'s slightly faster than N cube but, it\'s, you know, close enough\nfor our purposes. So if n the number of features\nis very large, then computing this quantity can be slow and the normal equation method can actually be much slower. So if n is large then I might usually use gradient descent because we don\'t want to pay this all in q time. But, if n is relatively small, then the normal equation might give you a better way to solve the parameters. What does small and large mean? Well, if n is on the order of a hundred, then inverting a hundred-by-hundred matrix is no problem by modern computing standards. If n is a thousand, I would still use\nthe normal equation method. Inverting a thousand-by-thousand matrix is actually really fast on a modern computer. If n is ten thousand, then I might start to wonder. Inverting a ten-thousand-  by-ten-thousand matrix starts to get kind of slow, and I might then start to maybe lean in the direction of gradient descent, but maybe not quite. n equals ten thousand, you can sort of convert a ten-thousand-by-ten-thousand matrix. But if it gets much bigger than that, then,\nI would probably use gradient descent. So, if n equals ten to the sixth with a million features, then inverting a million-by-million matrix is going to be very expensive, and I would definitely favor gradient descent if you have that many features. So exactly how large set of features has to be before you convert a gradient descent,\nit\'s hard to give a strict number. But, for me, it is usually around ten thousand that I might start to consider switching over to gradient descents or maybe, some other algorithms that we\'ll talk about later in this class. To summarize, so long as the number of features is not too large, the normal equation gives us a great alternative method\nto solve for the parameter theta. Concretely, so long as the number of features is less than 1000, you know, I would use, I would usually is used in normal equation method rather than, gradient descent. To preview some ideas that we\'ll talk about later in this course, as we get to the more complex learning algorithm, for example, when we talk about classification algorithm, like a logistic regression algorithm, We\'ll see that those algorithm actually... The normal  equation method\nactually do not work for those more sophisticated learning algorithms, and, we will have to resort to gradient descent\nfor those algorithms. So, gradient descent is a very useful algorithm to know. The linear regression will have a large number of features and for some of the other algorithms that we\'ll see in this course, because, for them, the normal equation method just doesn\'t apply and doesn\'t work. But for this specific model of linear regression, the normal equation can give you a alternative that can be much faster, than gradient descent. So, depending on the detail of your algortithm, depending of the detail of the problems and how many features that you have, both of these algorithms are\nwell worth knowing about.'",24,0,1
coursera,stanford_university,machine-learning,normal-equation-noninvertibility,"b""In this video I want to talk about\nthe Normal equation and non-invertibility. This is a somewhat more advanced concept,\nbut it's something that I've\noften been asked about. And so I want to talk it here and\naddress it here. But this is a somewhat\nmore advanced concept, so feel free to consider\nthis optional material. And there's a phenomenon that you may\nrun into that may be somewhat useful to understand, but even if you don't\nunderstand the normal equation and linear progression,\nyou should really get that to work okay. Here's the issue. For those of you there are, maybe some\nare more familiar with linear algebra, what some students have asked me is, when computing this Theta equals X\ntranspose X inverse X transpose Y. What if the matrix X transpose\nX is non-invertible? So for those of you that know\na bit more linear algebra you may know that only some\nmatrices are invertible and some matrices do not have an inverse\nwe call those non-invertible matrices. Singular or degenerate matrices. The issue or the problem of x transpose x being non\ninvertible should happen pretty rarely. And in Octave if you implement\nthis to compute theta, it turns out that this will\nactually do the right thing. I'm getting a little technical now, and\nI don't want to go into the details, but Octave hast two functions for\ninverting matrices. One is called pinv, and\nthe other is called inv. And the differences between these\ntwo are somewhat technical. One's called the pseudo-inverse,\none's called the inverse. But you can show mathematically that so long as you use the pinv function\nthen this will actually compute the value of data that you want even\nif X transpose X is non-invertible. The specific details between inv. What is the difference between pinv? What is inv? That's somewhat advanced\nnumerical computing concepts, I don't really want to get into. But I thought in this optional video, I'll\ntry to give you little bit of intuition about what it means for\nX transpose X to be non-invertible. For those of you that know a bit more\nlinear Algebra might be interested. I'm not gonna prove this mathematically\nbut if X transpose X is non-invertible, there usually two most common causes for\nthis. The first cause is if somehow in your\nlearning problem you have redundant features. Concretely, if you're trying to predict\nhousing prices and if x1 is the size of the house in feet, in square feet and x2\nis the size of the house in square meters, then you know 1 meter is equal to\n3.28 feet Rounded to two decimals. And so your two features will\nalways satisfy the constraint x1 equals 3.28 squared times x2. And you can show for those of you that are\nsomewhat advanced in linear Algebra, but if you're explaining the algebra you can\nactually show that if your two features are related,\nare a linear equation like this. Then matrix X transpose X\nwould be non-invertable. The second thing that can cause X\ntranspose X to be non-invertable is if you are training, if you are trying to run the\nlearning algorithm with a lot of features. Concretely, if m is less than or\nequal to n. For example, if you imagine that\nyou have m = 10 training examples that you have n equals 100\nfeatures then you're trying to fit a parameter back to theta which is,\nyou know, n plus one dimensional. So this is 101 dimensional, you're trying to fit 101 parameters\nfrom just 10 training examples. This turns out to sometimes work but\nnot always be a good idea. Because as we'll see later,\nyou might not have enough data if you only have 10 examples to fit you know,\n100 or 101 parameters. We'll see later in this course why\nthis might be too little data to fit this many parameters. But commonly what we do\nthen if m is less than n, is to see if we can either\ndelete some features or to use a technique called regularization\nwhich is something that we'll talk about later in this class as well, that will\nkind of let you fit a lot of parameters, use a lot features, even if you have\na relatively small training set. But this regularization will be\na later topic in this course. But to summarize if ever you find\nthat x transpose x is singular or alternatively you find it non-invertable,\nwhat I would recommend you do is first look at your features and see if you\nhave redundant features like this x1, x2. You're being linearly dependent or being\na linear function of each other like so. And if you do have redundant features and\nif you just delete one of these features, you really don't need\nboth of these features. If you just delete one of these features, that would solve your\nnon-invertibility problem. And so I would first think through my\nfeatures and check if any are redundant. And if so then keep deleting\nredundant features until they're no longer redundant. And if your features are not redundant, I would check if I may\nhave too many features. And if that's the case, I would either\ndelete some features if I can bear to use fewer features or else I would\nconsider using regularization. Which is this topic that\nwe'll talk about later. So that's it for the normal equation and what it means for if the matrix X\ntranspose X is non-invertable but this is a problem that you should run that\nhopefully you run into pretty rarely and if you just implement it\nin octave using P and using the P n function which is\ncalled a pseudo inverse function so you could use a different linear out your\nalive in Is called a pseudo-inverse but that implementation should\njust do the right thing, even if X transpose X is non-invertable,\nwhich should happen pretty rarely anyways, so this should not be a problem for\nmost implementations of linear regression.""",25,0,1
coursera,stanford_university,machine-learning,working-on-and-submitting-programming-assignments,"b'In this video, I want to just quickly step you\nthrough the logistics of how to work on homeworks in this class and how to use the\nsubmission system which will let you verify right away that you got the right answer for\nyour machine learning program exercise. Here\'s my Octave window and\nlet\'s first go to my desktop. I saved the files for my first exercise,\nsome of the files on my desktop: in this directory, \'ml-class-ex1\'. And we provide a number files\nand ask you to edit some of them. So the first file should meet the details in\nthe pdf file for this programming exercise. But one of the files we ask you to edit is\nthis file called warmUpExercise.m, where the exercise is really just to make sure that\nyou\'re familiar with the submission system. And all you need to do is\nreturn the 5x5 identity matrix. So the solution to this exercise I just\nshowed you is to write A = eye(5). So that modifies this function to\ngenerate the 5x5 identity matrix. And this function warmUpExercise()\nnow returns the 5x5 identity matrix. And I\'m just going to save it. So I\'ve done the first part of this homework.\nGoing back to my Octave window, let\'s now go to my directory,\n\'C:\\Users\\ang\\Desktop\\ml-class-ex1\'. And if I want to make sure that I\'ve implemented\nthis, type \'warmUpExercise()\' like so. And yup, it returns the 5x5 identity matrix\nthat we just wrote the code to create. And I can now submit the code as follows.\nI\'m going to type \'submit()\' in this directory and I\'m ready to submit part 1\nso I\'m going to enter choice \'1\'. So it asks me for my email address.\nI\'m going go to the course website. This is an internal testing site, so your version\nof the website may look a little bit different. But that\'s my email address and this is my submission\npassword, and I\'m just going to type them in here. So I have ang@cs.stanford.edu and\nmy submission password is 9yC75USsGf. I\'m going to hit enter; it connects to the server\nand submits it, and right away it tells you ""Congratulations! You have\nsuccessfully completed Homework 1 Part 1"". And this gives you a verification\nthat you got this part right. And if you don\'t submit the right answer,\nthen it will give you a message indicating that you haven\'t quite gotten it right yet. And you can use this submission password and\nyou can generate new passwords; it doesn\'t matter. But you can also use your regular website\nlogin password, but because this password here is typed in clear text on your monitor,\nwe gave you this extra submission password in case you don\'t want to type in your\nwebsite\'s normal password onto a window that, depending on your operating system,\nmay or may not appear as text when you type it into the Octave submission script. So, that\'s how you submit the\nhomeworks after you\'ve done it. Good luck, and, when you get around to\nhomeworks, I hope you get all of them right. And finally, in the next and final Octave\ntutorial video, I want to tell you about vectorization, which is a way to get your\nOctave code to run much more efficiently.'",26,0,1
coursera,stanford_university,machine-learning,basic-operations,"b""You now know a bunch about machine learning. In this video, I like to teach you a programing language, Octave, in which you'll be able to very quickly implement the the learning algorithms we've seen already, and the learning algorithms we'll see later in this course. In the past, I've tried to teach machine learning using a large variety of different programming languages including C++ Java, Python, NumPy, and also Octave, and what I found was that students were able to learn the most productively learn the most quickly and prototype your algorithms most quickly using a relatively high level language like octave. In fact, what I often see in Silicon Valley is that if even if you need to build. If you want to build a large scale deployment of a learning algorithm, what people will often do is prototype and the language is Octave. Which is a great prototyping language. So you can sort of get your learning algorithms working quickly. And then only if you need to a very large scale deployment of it. Only then spend your time re-implementing the algorithm to C++ Java or some of the language like that. Because all the lessons we've learned is that a time or develop a time. That is your time. The machine learning's time is incredibly valuable. And if you can get your learning algorithms to work more quickly in Octave. Then overall you have a huge time savings by first developing the algorithms in Octave, and then implementing and maybe C++ Java, only after we have the ideas working. The most common prototyping language I see people use for machine learning are: Octave, MATLAB, Python, NumPy, and R. Octave is nice because open sourced. And MATLAB works well too, but it is expensive for to many people. But if you have access to a copy of MATLAB. You can also use MATLAB with this class. If you know Python, NumPy, or if you know R. I do see some people use it. But, what I see is that people usually end up developing somewhat more slowly, and you know, these languages. Because the Python, NumPy syntax is just slightly clunkier than the Octave syntax. And so because of that, and because we are releasing starter code in Octave. I strongly recommend that you not try to do the following exercises in this class in NumPy and R. But that I do recommend that you instead do the programming exercises for this class in octave instead. What I'm going to do in this video is go through a list of commands very, very quickly, and its goal is to quickly show you the range of commands and the range of things you can do in Octave. The course website will have a transcript of everything I do, and so after watching this video you can refer to the transcript posted on the course website when you want find a command. Concretely, what I recommend you do is first watch the tutorial videos. And after watching to the end, then install Octave on your computer. And finally, it goes to the course website, download the transcripts of the things you see in the session, and type in whatever commands seem interesting to you into Octave, so that it's running on your own computer, so you can see it run for yourself. And with that let's get started. Here's my Windows desktop, and I'm going to start up Octave. And I'm now in Octave. And that's my Octave prompt. Let me first show the elementary operations you can do in Octave. So you type in 5 + 6. That gives you the answer of 11. 3 - 2. 5 x 8, 1/2, 2^6 is 64. So those are the elementary math operations. You can also do logical operations. So one equals two. This evaluates to false. The percent command here means a comment. So, one equals two, evaluates to false. Which is represents by zero. One not equals to two. This is true. So that returns one. Note that a not equal sign is this tilde equals symbol. And not bang equals. Which is what some other programming languages use. Lets see logical operations one and zero use a double ampersand sign to the logical AND. And that evaluates false. One or zero is the OR operation. And that evaluates to true. And I can XOR one and zero, and that evaluates to one. This thing over on the left, this Octave 324.x equals 11, this is the default Octave prompt. It shows the, what, the version in Octave and so on. If you don't want that prompt, there's a somewhat cryptic command PF quote, greater than, greater than and so on, that you can use to change the prompt. And I guess this quote a string in the middle. Your quote, greater than, greater than, space. That's what I prefer my Octave prompt to look like. So if I hit enter. Oops, excuse me. Like so. PS1 like so. Now my Octave prompt has changed to the greater than, greater than sign.Which, you know, looks quite a bit better. Next let's talk about Octave variables. I can take the variable A and assign it to 3. And hit enter. And now A is equal to 3. You want to assign a variable, but you don't want to print out the result. If you put a semicolon, the semicolon suppresses the print output. So to do that, enter, it doesn't print anything. Whereas A equals 3. mix it, print it out, where A equals, 3 semicolon doesn't print anything. I can do string assignment. B equals hi Now if I just enter B it prints out the variable B. So B is the string hi C equals 3 greater than colon 1. So, now C evaluates the true. If you want to print out or display a variable, here's how you go about it. Let me set A equals Pi. And if I want to print A I can just type A like so, and it will print it out. For more complex printing there is also the DISP command which stands for Display. Display A just prints out A like so. You can also display strings so: DISP, sprintf, two decimals, percent 0.2, F, comma, A. Like so. And this will print out the string. Two decimals, colon, 3.14. This is kind of an old style C syntax. For those of you that have programmed C before, this is essentially the syntax you use to print screen. So the Sprintf generates a string that is less than the 2 decimals, 3.1 plus string. This percent 0.2 F means substitute A into here, showing the two digits after the decimal points. And DISP takes the string DISP generates it by the Sprintf command. Sprintf. The Sprintf command. And DISP actually displays the string. And to show you another example, Sprintf six decimals percent 0.6 F comma A. And, this should print Pi with six decimal places. Finally, I was saying, a like so, looks like this. There are useful shortcuts that type type formats long. It causes strings by default. Be displayed to a lot more decimal places. And format short is a command that restores the default of just printing a small number of digits. Okay, that's how you work with variables. Now let's look at vectors and matrices. Let's say I want to assign MAT A to the matrix. Let me show you an example: 1, 2, semicolon, 3, 4, semicolon, 5, 6. This generates a three by two matrix A whose first row is 1, 2. Second row 3, 4. Third row is 5, 6. What the semicolon does is essentially say, go to the next row of the matrix. There are other ways to type this in. Type A 1, 2 semicolon 3, 4, semicolon, 5, 6, like so. And that's another equivalent way of assigning A to be the values of this three by two matrix. Similarly you can assign vectors. So V equals 1, 2, 3. This is actually a row vector. Or this is a 3 by 1 vector. Where that is a fat Y vector, excuse me, not, this is a 1 by 3 matrix, right. Not 3   by 1. If I want to assign this to a column vector, what I would do instead is do v 1;2;3. And this will give me a 3 by 1. There's a 1 by 3 vector. So this will be a column vector. Here's some more useful notation. V equals 1: 0.1: 2. What this does is it sets V to the bunch of elements that start from 1. And increments and steps of 0.1 until you get up to 2. So if I do this, V is going to be this, you know, row vector. This is what one by eleven matrix really. That's 1, 1.1, 1.2, 1.3 and so on until we get up to two. Now, and I can also set V equals one colon six, and that sets V to be these numbers. 1 through 6, okay. Now here are some other ways to generate matrices. Ones 2.3 is a command that generates a matrix that is a two by three matrix that is the matrix of all ones. So if I set that c2 times ones two by three this generates a two by three matrix that is all two's. You can think of this as a shorter way of writing this and c2,2,2's and you can call them 2,2,2, which would also give you the same result. Let's say W equals one's, one by three, so this is going to be a row vector or a row of three one's and similarly you can also say w equals zeroes, one by three, and this generates a matrix. A one by three matrix of all zeros. Just a couple more ways to generate matrices . If I do W equals Rand one by three, this gives me a one by three matrix of all random numbers. If I do Rand three by three. This gives me a three by three matrix of all random numbers drawn from the uniform distribution between zero and one. So every time I do this, I get a different set of random numbers drawn uniformly between zero and one. For those of you that know what a Gaussian random variable is or for those of you that know what a normal random variable is, you can also set W equals Rand N, one by three. And so these are going to be three values drawn from a Gaussian distribution with mean zero and variance or standard deviation equal to one. And you can set more complex things like W equals minus six, plus the square root ten, times, lets say Rand N, one by ten thousand. And I'm going to put a semicolon at the end because I don't really want this printed out. This is going to be a what? Well, it's going to be a vector of, with a hundred thousand, excuse me, ten thousand elements. So, well, actually, you know what? Let's print it out. So this will generate a matrix like this. Right? With 10,000 elements. So that's what W is. And if I now plot a histogram of W with a hist command, I can now. And Octave's print hist command, you know, takes a couple seconds to bring this up, but this is a histogram of my random variable for W. There was minus 6 plus zero ten times this Gaussian random variable. And I can plot a histogram with more buckets, with more bins, with say, 50 bins. And this is my histogram of a Gaussian with mean minus 6. Because I have a minus 6 there plus square root 10 times this. So the variance of this Gaussian random variable is 10 on the standard deviation is square root of 10, which is about what? Three point one. Finally, one special command for generator matrix, which is the I command. So I stands for this is maybe a pun on the word identity. It's server set eye 4. This is the 4 by 4 identity matrix. So I equals eye 4. This gives me a 4 by 4 identity matrix. And I equals eye 5,  eye 6. That gives me a 6 by 6 identity matrix, i3 is the 3 by 3 identity matrix. Lastly, to wrap up this video, there's one more useful command. Which is the help command. So you can type help i and this brings up the help function for the identity matrix. Hit Q to quit. And you can also type help rand. Brings up documentation for the rand or the random number generation function. Or even help help, which shows you, you know help on the help function. So, those are the basic operations in Octave. And with this you should be able to generate a few matrices, multiply, add things. And use the basic operations in Octave. In the next video, I'd like to start talking about more sophisticated commands and how to use data around and start to process data in Octave.""",27,0,1
coursera,stanford_university,machine-learning,moving-data-around,"b""In this second tutorial video on Octave, I'd like to start to tell you how to move data around in Octave. So, if you have data for a machine learning problem, how do you load that data in Octave? How do you put it into matrix? How do you manipulate these matrices? How do you save the results? How do you move data around and operate with data? Here's my Octave window as before, picking up from where we left off in the last video. If I type A, that's the matrix so we generate it, right, with this command equals one, two, three, four, five, six, and this is a three by two matrix. The size command in Octave lets you, tells you what is the size of a matrix. So size A returns three, two. It turns out that this size command itself is actually returning a one by two matrix. So you can actually set SZ equals size of A and SZ is now a one by two matrix where the first element of this is three, and the second element of this is two. So, if you just type size of SZ. Does SZ is a one by two matrix whose two elements contain the dimensions of the matrix A. You can also type size A one to give you back the first dimension of A, size of the first dimension of A. So that's the number of rows and size A two to give you back two, which is the number of columns in the matrix A. \nIf you have a vector V, so let's say V equals one, two, three, four, and you type length V.  What this does is it gives you the size of the longest dimension. So you can also type length A and because A is a three by two matrix, the longer dimension is of size three, so this should print out three. But usually we apply length only to vectors. So you know, length one, two, three, four, five, rather than apply length to matrices because that's a little more confusing. Now, let's look at how the load data and find data on the file system. When we start an Octave we're usually, we're often in a path that is, you know, the location of where the Octave location is. So the PWD command shows the current directory, or the current path that Octave is in. So right now we're in this maybe somewhat off scale directory. The CD command stands for change directory, so I can go to C:/Users/Ang/Desktop, and now I'm in, you know, in my Desktop and if I type ls, ls is, it comes from a Unix or a Linux command. But, ls will list the directories on my desktop and so these are the files that are on my Desktop right now. In fact, on my desktop are two files: Features X and Price Y that's maybe come from a machine learning problem I want to solve. So, here's my desktop. Here's Features X, and Features X is this window, excuse me, is this file with two columns of data. This is actually my housing prices data. So I think, you know, I think I have forty-seven rows in this data set. And so the first house has size two hundred four square feet, has three bedrooms; second house has sixteen hundred square feet, has three bedrooms; and so on. And Price Y is this file that has the prices of the data in my training set. So, Features X and Price Y are just text files with my data. How do I load this data into Octave? Well, I just type the command load Features X dot dat and if I do that, I load the Features X and can load Price Y dot dat. And by the way, there are multiple ways to do this. This command if you put Features X dot dat on that in strings and load it like so. This is a typo there. This is an equivalent command. So you can, this way I'm just putting the file name of the string in the founding in a string and in an Octave use single quotes to represent strings, like so. So that's a string, and we can load the file whose name is given by that string. Now the WHO command now shows me what variables I have in my Octave workspace. So Who shows me whether the variables that Octave has in memory currently. Features X and Price Y are among them, as well as the variables that, you know, we created earlier in this session. So I can type Features X to display features X. And there's my data. And I can type size features X and that's my 47 by two matrix. And some of these size, press Y, that gives me my 47 by one vector. This is a 47 dimensional vector. This is all common vector that has all the prices Y in my training set. Now the who function shows you one of the variables that, in the current workspace. There's also the who S variable that gives you the detailed view. And so this also, with an S at the end this also lists my variables except that it now lists the sizes as well. So A is a three by two matrix and features X as a 47 by 2 matrix. Price Y is a 47 by one matrix. Meaning this is just a vector. And it shows, you know, how many bytes of memory it's taking up. As well as what type of data this is. Double means double position floating point so that just means that these are real values, the floating point numbers. Now if you want to get rid of a variable you can use the clear command. So clear features X and type whose again. You notice that the features X variable has now disappeared. And how do we save data? Let's see. Let's take the variable V and say that it's a price Y 1 colon 10. This sets V to be the first 10 elements of vector Y. So let's type who or whose. Whereas Y was a 47 by 1 vector. V is now 10 by 1. B equals price Y, one column ten that sets it to the just the first ten elements of Y. Let's say I wanna save this to date to disc the command save, hello.mat V. This will save the variable V into a file called hello.mat. So let's do that. And now a file has appeared on my Desktop, you know, called Hello.mat. I happen to have MATLAB installed in this window, which is why, you know, this icon looks like this because Windows is recognized as it's a MATLAB file,but don't worry about it if this file looks like it has a different icon on your machine and let's say I clear all my variables. So, if you type clear without anything then this actually deletes all of the variables in your workspace. So there's now nothing left in the workspace. And if I load hello.mat, I can now load back my variable v, which is the data that I previously saved into the hello.mat file. So, hello.mat, what we did just now to save hello.mat to view, this save the data in a binary format, a somewhat more compressed binary format. So if v is a lot of data, this, you know, will be somewhat more compressing. Will take off less the space. If you want to save your data in a human readable format then you type save hello.text the variable v and then -ascii. So, this will save it as a text or as ascii format of text. And now, once I've done that, I have this file. Hello.text has just appeared on my desktop, and if I open this up, we see that this is a text file with my data saved away. So that's how you load and save data. Now let's talk a bit about how to manipulate data. Let's set a equals to that matrix again so is my three by two matrix. So as indexing. So type A 3, 2. This indexes into the 3, 2 elements of the matrix A. So, this is what, you know, in normally, we will write this as a subscript 3, 2 or A subscript, you know, 3, 2 and so that's the element and third row and second column of A which is the element of six. I can also type A to comma colon to fetch everything in the second row. So, the colon means every element along that row or column. So, a of 2 comma colon is this second row of a. Right. And similarly, if I do a colon comma 2 then this means get everything in the second column of A. So, this gives me 2 4 6. Right this means of A.  everything, second column. So, this is my second column A, which is 2 4 6. Now, you can also use somewhat most of the sophisticated index in the operations. So So, we just click each of an example. You do this maybe less often, but let me do this A 1 3 comma colon. This means get all of the elements of A who's first indexes one or three. This means I get everything from the first and third rows of A and from all columns. So, this was the matrix A and so A 1 3 comma colon means get everything from the first row and from the second row and from the third row and the colon means, you know, one both of first and the second columns and so this gives me this 1 2 5 6. Although, you use the source of more subscript index operations maybe somewhat less often. To show you what else we can do. Here's the A matrix and this source A colon, to give me the second column. You can also use this to do assignments. So I can take the second column of A and assign that to 10, 11, 12, and if I do that I'm now, you know, taking the second column of a and I'm assigning this column vector 10, 11, 12 to it. So, now a is this matrix that's 1, 3, 5. And the second column has been replaced by 10, 11, 12. And here's another operation. Let's set A to be equal to A comma 100, 101, 102 like so and what this will do is depend another column vector to the right. So, now, oops. I think I made a little mistake. Should have put semicolons there and now A is equals to this. Okay? I hope that makes sense. So this 100, 101, 102. This is a column vector and what we did was we set A, take A and set it to the original definition. And then we put that column vector to the right and so, we ended up taking the matrix A and--which was these six elements on the left. So we took matrix A and we appended another column vector to the right; which is now why A is a three by three matrix that looks like that. And finally, one neat trick that I sometimes use if you do just a and just a colon like so. This is a somewhat special case syntax. What this means is that put all elements with A into a single column vector and this gives me a 9 by 1 vector. They adjust the other ones are combined together. Just a couple more examples. Let's see. Let's say I set A to be equal to 123456, okay? And let's say I set a B to B equal to 11, 12, 13, 14, 15, 16. I can create a new matrix C as A B. This just means my Matrix A. Here's my Matrix B and I've set C to be equal to AB. What I'm doing is I'm taking these two matrices and just concatenating onto each other. So the left, matrix A on the left. And I have the matrix B on the right. And that's how I formed this matrix C by putting them together. I can also do C equals A semicolon B. The semi colon notation means that I go put the next thing at the bottom. So, I'll do is a equals semicolon B. It also puts the matrices A and B together except that it now puts them on top of each other. so now I have A on top and B at the bottom and C here is now in 6 by 2 matrix. So, just say the semicolon thing usually means, you know, go to the next line. So, C is comprised by a and then go to the bottom of that and then put b in the bottom and by the way, this A B is the same as A, B and so you know, either of these gives you the same result. So, with that, hopefully you now know how to construct matrices and hopefully starts to show you some of the commands that you use to quickly put together matrices and take matrices and, you know, slam them together to form bigger matrices, and with just a few lines of code, Octave is very convenient in terms of how quickly we can assemble complex matrices and move data around. So that's it for moving data around. In the next video we'll start to talk about how to actually do complex computations on this, on our data. So, hopefully that gives you a sense of how, with just a few commands, you can very quickly move data around in Octave. You know, you load and save vectors and matrices, load and save data, put together matrices to create bigger matrices, index into or select specific elements on the matrices. I know I went through a lot of commands, so I think the best thing for you to do is afterward, to look at the transcript of the things I was typing. You know, look at it. Look at the coursework site and download the transcript of the session from there and look through the transcript and type some of those commands into Octave yourself and start to play with these commands and get it to work. And obviously, you know, there's no point at all to try to memorize all these commands. It's just, but what you should do is, hopefully from this video you have gotten a sense of the sorts of things you can do. So that when later on when you are trying to program a learning algorithms yourself, if you are trying to find a specific command that maybe you think Octave can do because you think you might have seen it here, you should refer to the transcript of the session and look through that in order to find the commands you wanna use. So, that's it for moving data around and in the next video what I'd like to do is start to tell you how to actually do complex computations on our data, and how to compute on the data, and actually start to implement learning algorithms.""",28,0,1
coursera,stanford_university,machine-learning,computing-on-data,"b""Now that you know how to load and\nsave data in Octave, put your data into matrices and so on. In this video, I'd like to show you how\nto do computational operations on data. And later on, we'll be using these source\nof computational operations to implement our learning algorithms. Let's get started. Here's my Octave window. Let me just quickly initialize some\nvariables to use for our example. So set A to be a three by two matrix,\nand set B to a three by two matrix, and let's set C to a two\nby two matrix like so. Now let's say I want to\nmultiply two of my matrices. So let's say I want to compute A*C, I just\ntype A*C, so it's a three by two matrix times a two by two matrix,\nthis gives me this three by two matrix. You can also do element wise operations\nand do A.* B and what this will do is it'll take each element of A and multiply\nit by the corresponding elements B, so that's A, that's B, that's A .* B. So for example, the first element\ngives 1 times 11, which gives 11. The second element gives 2 time\n12 Which gives 24, and so on. So this is element-wise\nmultiplication of two matrices. And in general, the period tends to, is usually used to denote\nelement-wise operations in Octave. So here's a matrix A, and if I do A .^ 2, this gives me the element\nwise squaring of A. So 1 squared is 1,\n2 squared is 4, and so on. Let's set v as a vector. Let's set v as one, two,\nthree as a column vector. You can also do one dot over v to do\nthe element-wise reciprocal of v, so this gives me one over one, one over\ntwo, and one over three, and this is where I do the matrices, so one dot over\na gives me the element wise inverse of a. And once again, the period here gives us a\nclue that this an element-wise operation. We can also do things like log(v),\nthis is a element-wise logarithm of the v E to the V is base E\nexponentiation of these elements, so this is E, this is E squared EQ,\nbecause this was V, and I can also do abs V to take\nthe element-wise absolute value of V. So here, V was our positive,\nabs, minus one, two minus 3, the element-wise absolute value gives\nme back these non-negative values. And negative v gives me the minus of v. This is the same as negative one times v,\nbut usually you just write\nnegative v instead of -1*v. And what else can you do? Here's another neat trick. So, let's see. Let's say I want to take v an increment\neach of its elements by one. Well one way to do it is\nby constructing a three by one vector that's all ones and\nadding that to v. So if I do that, this increments\nv by from 1, 2, 3 to 2, 3, 4. The way I did that was, length(v) is 3, so ones(length(v),1),\nthis is ones of 3 by 1, so that's ones(3,1) on the right and\nwhat I did was v plus ones v by one, which is adding this vector of our ones\nto v, and so this increments v by one, and another simpler way to do\nthat is to type v plus one. So she has v, and v plus one also means to add one element\nwise to each of my elements of v. Now, let's talk about more operations. So here's my matrix A, if you want to\nbuy A transposed, the way to do that is to write A prime, that's the apostrophe\nsymbol, it's the left quote, so it's on your keyboard,\nyou have a left quote and a right quote. So this is actually\nthe standard quotation mark. Just type A transpose, this gives\nme the transpose of my matrix A. And, of course, A transpose, if I transpose that again,\nthen I should get back my matrix A. Some more useful functions. Let's say lower case a is 1 15 2 0.5,\nso it's 1 by 4 matrix. Let's say val equals max of A this\nreturns the maximum value of A which in this case is 15 and\nI can do val, ind max(a) and this returns val and ind which are going to be the maximum value\nof A which is 15, as well as the index. So it was the element number two of A that\nwas 15 so ind is my index into this. Just as a warning, if you do max(A),\nwhere A is a matrix, what this does is this actually\ndoes the column wise maximum. But say a little more\nabout this in a second. Still using this example that there for\nlowercase a. If I do a < 3,\nthis does the element wise operation. Element wise comparison, so the first\nelement of A is less than three so this one. Second element of A is not less than three\nso this value says zero cuz it's false. The third and fourth elements of\nA are less than three, so that's just 1 1. So that's the element-wise comparison of\nall four elements of the variable a < 3. And it returns true or false depending on\nwhether or not there's less than three. Now, if I do find(a < 3), this will\ntell me which are the elements of a, the variable a, that are less than 3, and in this case, the first, third and\nfourth elements are less than 3. For our next example,\nlet me set a to be equal to magic(3). The magic function returns,\nlet's type help magic. The magic function returns these\nmatrices called magic squares. They have this, you know, mathematical\nproperty that all of their rows and columns and\ndiagonals sum up to the same thing. So, you know, it's not actually useful for\nmachine learning as far as I know, but I'm just using this as a convenient way\nto generate a three by three matrix. And these magic squares have\nthe property that each row, each column, and the diagonals all add\nup to the same thing, so it's kind of a mathematical construct. I use this magic function only when I'm\ndoing demos or when I'm teaching octave like those in, I don't actually use it for\nany useful machine learning application. But let's see,\nif I type RC = find(A > 7) this finds All the elements of A that\nare greater than equal to seven, and so r, c stands for row and column. So the 1,1 element is greater than 7,\nthe 3,2 element is greater than 7, and the 2,3 element is greater than 7. So let's see. The 2,3 element, for example, is A(2,3), is 7 is this element out here, and\nthat is indeed greater than equal seven. By the way, I actually don't even memorize\nmyself what these find functions do and what all of these things do myself. And whenever I use the find function,\nsometimes I forget myself exactly what it does, and now I would type help\nfind to look at the document. Okay, just two more things\nthat I'll quickly show you. One is the sum function, so\nhere's my a, and then type sum(a). This adds up all the elements of a, and\nif I want to multiply them together, I type prod(a) prod sends the product, and this returns the product of\nthese four elements of A. Floor(a) rounds down these elements of A,\nso 0.5 gets rounded down to 0. And ceil, or ceiling(A) gets\nrounded up to the nearest integer, so 0.5 gets rounded up to 1. You can also, let's see. Let me type rand(3),\nthis generates a three by three matrix. If i type max(rand(3),\nwhat this does is it takes the element-wise maximum of\n3 random 3 by 3 matrices. So you notice all of these numbers tend\nto be a bit on the large side because each of these is actually\nthe max of a element wise max of two randomly\ngenerated matrices. This is my magic number. This is my magic square, three by three A. Let's say I type max A, and\nthen this will be a [], 1, what this does is this\ntexts the column wise maximum. So the max of the first column is 8,\nmax of second column is 9, the max of the third column is 7. This 1 means to take the max\namong the first dimension of 8. In contrast, if I were to type max A,\nthis funny notation, two, then this takes the per row maximum. So the max of the first row is eight, max\nof second row is seven, max of the third row is nine, and so this allows you to\ntake maxes either per row or per column. And remember the default's\nto a column wise element. So if you want to find the maximum\nelement in the entire matrix A, you can type max(max(A)) like so,\nwhich is 9. Or you can turn A into a vector and\ntype max(A(:)) like so and this treats this as a vector and\ntakes the max element of that vector. Finally let's set A to be\na 9 by 9 magic square. So remember the magic square has\nthis property that every column and every row sums the same thing,\nand also the diagonals, so just a nine by nine matrix square. So let me just sum(A, 1). So this does a per column sum, so\nwe'll take each column of A and add them up and\nthis is verified that indeed for a nine by nine matrix square, every column\nadds up to 369, adds up to the same thing. Now let's do the row wide sum. So the sum(A,2), and this sums up each row of A, and\nindeed each row of A also sums up to 369. Now, let's sum the diagonal\nelements of A and make sure that also sums\nup to the same thing. So what I'm gonna do is construct a nine\nby nine identity matrix, that's eye nine. And let me take A and construct, multiply\nA element wise, so here's my matrix A. I'm going to do A .^ eye(9). What this will do is take the element wise\nproduct of these two matrices, and so this should Wipe out everything in A,\nexcept for the diagonal entries. And now,\nI'm gonna do sum sum of A of that and this gives me the sum of these diagonal\nelements, and indeed that is 369. You can sum up the other\ndiagonals as well. So this top left to bottom left, you can sum up the opposite diagonal\nfrom bottom left to top right. The commands for\nthis is somewhat more cryptic, you don't really need to know this. I'm just showing you this in\ncase any of you are curious. But let's see. Flipud stands for flip up down. But if you do that, that turns out to\nsum up the elements in the opposite. So the other diagram,\nthat also sums up to 369. Here, let me show you. Whereas eye(9) is this matrix. Flipup(eye(9)), takes the identity matrix, and flips it vertically, so\nyou end up with, excuse me, flip UD, end up with ones on\nthis opposite diagonal as well. Just one last command and then that's it,\nand then that'll be it for this video. Let's set A to be the three\nby three magic square game. If you want to invert a matrix,\nyou type pinv(A). This is typically called\nthe pseudo-inverse, but it does matter. Just think of it as basically the inverse\nof A, and that's the inverse of A. And so I can set temp = pinv(A) and\ntemp times A, this is indeed the identity matrix, where\nit's essentially ones on the diagonals, and zeroes on the off-diagonals,\nup to a numeric round off. So, that's it for how to do different computational\noperations on data and matrices. And after running a learning algorithm,\noften one of the most useful things is to be able to look at your results, so\nto plot or visualize your result. And in the next video, I'm going to very\nquickly show you how again with one or two lines of code using Octave. You can quickly visualize your data or\nplot your data and use that to better understand what\nyou're learning algorithms are doing.""",29,0,1
coursera,stanford_university,machine-learning,plotting-data,"b""When developing learning algorithms, very often a few simple plots can give you a better sense of what the algorithm is doing and just sanity check that everything is going okay and the algorithms doing what is supposed to. For example, in an earlier video, I talked about how plotting the cost function J of theta can help you make sure that gradient descent is converging. Often, plots of the data or of all the learning algorithm outputs will also give you ideas for how to improve your learning algorithm. Fortunately, Octave has very simple tools to generate lots of different plots and when I use learning algorithms, I find that plotting the data, plotting the learning algorithm and so on are often an important part of how I get ideas for improving the algorithms and in this video, I'd like to show you some of these Octave tools for plotting and visualizing your data. Here's my Octave window. Let's quickly generate some data for us to plot. So I'm going to set T to be equal to, you know, this array of numbers. Here's T, set of numbers going from 0 up to .98. Let's set y1 equals sine of 2 pie 40 and if I want to plot the sine function, it's very easy. I just type plot T comma Y 1 and hit enter. And up comes this plot where the horizontal axis is the T variable and the vertical axis is y1, which is the sine you saw in the function that we just computed. Let's set y2 to be equal to the cosine of two pi, four T, like so. And if I plot T comma y2, what octave will I do is I'll take my sine plot and it will replace with this cosine function and now, you know, cosine of xi of 1. Now, what if I want to have both the sine and the cosine plots on top of each other? What I'm going to do is I'm going to type plot t,y1. So here's my sine function, and then I'm going to use the function hold on. And what hold does it closes octaves to now figures on top of the old one and let me now plot t y2. I'm going to plot the cosine function in a different color. So, let me put there r in quotation marks there and instead of replacing the current figure, I'll plot the cosine function on top and the r indicates the what is an event color. And here additional commands - x label times, to label the X axis, or the horizontal axis. And Y label values A, to label the vertical axis value, and I can also label my two lines with this command: legend sine cosine and this puts this legend up on the upper right showing what the 2 lines are, and finally title my plot is the title at the top of this figure. Lastly, if you want to save this figure, you type print -dpng myplot .png. So PNG is a graphics file format, and if you do this it will let you save this as a file. If I do that, let me actually change directory to, let's see, like that, and then I will print that out. So this will take a while depending on how your Octave configuration is setup, may take a few seconds, but change directory to my desktop and Octave is now taking a few seconds to save this. If I now go to my desktop, Let's hide these windows. Here's myplot.png which Octave has saved, and you know, there's the figure saved as the PNG file. Octave can save thousand other formats as well. So, you can type help plot, if you want to see the other file formats, rather than PNG, that you can save figures in. And lastly, if you want to get rid of the plot, the close command causes the figure to go away. As I figure if I type close, that figure just disappeared from my desktop. Octave also lets you specify a figure and numbers. You type figure 1 plots t, y1. That starts up first figure, and that plots t, y1. And then if you want a second figure, you specify a different figure number. So figure two, plot t, y2 like so, and now on my desktop, I actually have 2 figures. So, figure 1 and figure 2 thus 1 plotting the sine function, 1 plotting the cosine function. Here's one other neat command that I often use, which is the subplot command. So, we're going to use subplot 1 2 1. What it does it sub-divides the plot into a one-by-two grid with the first 2 parameters are, and it starts to access the first element. That's what the final parameter 1 is, right? So, divide my figure into a one by two grid, and I want to access the first element right now. And so, if I type that in, this product, this figure, is on the left. And if I plot t, y1, it now fills up this first element. And if I I'll do subplot 122. I'm going to start to access the second element and plot t, y2. Well, throw in y2 in the right hand side, or in the second element. And last command, you can also change the axis scales and change axis these to 1.51 minus 1 1 and this sets the x range and y range for the figure on the right, and concretely, it assess the horizontal major values in the figure on the right to make sure 0.5 to 1, and the vertical axis values use the range from minus one to one. And, you know, you don't need to memorize all these commands. If you ever need to change the access or you need to know is that, you know, there's an access command and you can already get the details from the usual octave help command. Finally, just a couple last commands CLF clear is a figure and here's one unique trait. Let's set a to be equal to a 5 by 5 magic squares a. So, a is now this 5 by 5 matrix does a neat trick that I sometimes use to visualize the matrix, which is I can use image sc of a what this will do is plot a five by five matrix, a five by five grid of color. where the different colors correspond to the different values in the A matrix. So concretely, I can also do color bar. Let me use a more sophisticated command, and image sc A color bar color map gray. This is actually running three commands at a time. I'm running image sc then running color bar, then running color map gray. And what this does, is it sets a color map, so a gray color map, and on the right it also puts in this color bar. And so this color bar shows what the different shades of color correspond to. Concretely, the upper left element of the A matrix is 17, and so that corresponds to kind of a mint shade of gray. Whereas in contrast the second element of A--sort of the 1 2 element of A--is 24. Right, so it's A 1 2 is 24. So that corresponds to this square out here, which is nearly a shade of white. And the small value, say A--what is that? A 4 5, you know, is a value 3 over here that corresponds-- you can see on my color bar that it corresponds to a much darker shade in this image. So here's another example, I can plot a larger, you know, here's a magic 15 that gives you a 15 by 15 magic square and this gives me a plot of what my 15 by 15 magic squares values looks like. And finally to wrap up this video, what you've seen me do here is use comma chaining of function calls. Here's how you actually do this. If I type A equals 1, B equals 2, C equals 3, and hit Enter, then this is actually carrying out three commands at the same time. Or really carrying out three commands, one after another, and it prints out all three results. And this is a lot like A equals 1, B equals 2, C equals 3, except that if I use semicolons instead of a comma, it doesn't print out anything. So, this, you know, this thing here we call comma chaining of commands, or comma chaining of function calls. And, it's just another convenient way in Octave to put multiple commands like image sc color bar, colon map to put multi-commands on the same line. So, that's it. You now know how to plot different figures and octave, and in next video the next main piece that I want to tell you about is how to write control statements like if, while, for statements and octave as well as hard to define and use functions""",30,0,1
coursera,stanford_university,machine-learning,control-statements-for-while-if-statement,"b'In this video, I\'d like to tell you how to write control statements for your Octave programs, so things like ""for"", ""while"" and ""if"" statements and also how to define and use functions. Here\'s my Octave window. Let me first show you how to use a ""for"" loop. I\'m going to start by setting v to be a 10 by 1 vector 0. Now, here\'s I write a ""for"" loop for I equals 1 to 10. That\'s for I equals Y colon 10. And let\'s see, I\'m going to set V of I equals two to the power of I, and finally end. The white space does not matter, so I am putting the spaces just to make it look nicely indented, but you know spacing doesn\'t matter. But if I do this, then the result is that V gets set to, you know, two to the power one, two to the power two, and so on. So this is syntax for I equals one colon 10 that makes I loop through the values one through 10. And by the way, you can also do this by setting your indices equals one to 10, and so the indices in the array from one to 10. You can also write for I equals indices. And this is actually the same as if I equals one to 10. You can do, you know, display I and this would do the same thing. So, that is a ""for"" loop, if you are familiar with ""break"" and ""continue"", there\'s ""break"" and ""continue"" statements, you can also use those inside loops in octave, but first let me show you how a while loop works. So, here\'s my vector V. Let\'s write the while loop. I equals 1, while I is less than or equal to 5, let\'s set V I equals one hundred and increment I by one, end. So this says what? I starts off equal to one and then I\'m going to set V I equals one hundred and increment I by one until I is, you know, greater than five. And as a result of that, whereas previously V was this powers of two vector. I\'ve now taken the first five elements of my vector and overwritten them with this value one hundred. So that\'s a syntax for a while loop. Let\'s do another example. Y equals one while true and here I wanted to show you how to use a break statement. Let\'s say V I equals 999 and I equals i+1 if i equals 6 break and end. And this is also our first use of an if statement, so I hope the logic of this makes sense. Since I equals one and, you know, increment loop. While repeatedly set V I equals 1 and increment i by 1, and then when 1 i gets up to 6, do a break which breaks here although the while do and so, the effective is should be to take the first five elements of this vector V and set them to 999. And yes, indeed, we\'re taking V and overwritten the first five elements with 999. So, this is the syntax for ""if"" statements, and for ""while"" statement, and notice the end. We have two ends here. This ends here ends the if statement and the second end here ends the while statement. Now let me show you the more general syntax for how to use an if-else statement. So, let\'s see, V 1 is equal to 999, let\'s type V1 equals to 2 for this example. So, let me type if V 1 equals 1 display the value as one. Here\'s how you write an else statement, or rather here\'s an else if: V 1 equals 2. This is, if in case that\'s true in our example, display the value as 2, else display, the value is not one or two. Okay, so that\'s a  if-else if-else statement it ends. And of course, here we\'ve just set v 1 equals 2, so hopefully, yup, displays that the value is 2. And finally, I don\'t think I talked about this earlier, but if you ever need to exit Octave, you can type the exit command and you hit enter that will cause Octave to quit or the \'q\'--quits command also works. Finally, let\'s talk about functions and how to define them and how to use them. Here\'s my desktop, and I have predefined a file or pre-saved on my desktop a file called ""squarethisnumber.m"". This is how you define functions in Octave. You create a file called, you know, with your function name and then ending in .m, and when Octave finds this file, it knows that this where it should look for the definition of the function ""squarethisnumber.m"". Let\'s open up this file. Notice that I\'m using the Microsoft program Wordpad to open up this file. I just want to encourage you, if your using Microsoft Windows, to use Wordpad rather than Notepad to open up these files, if you have a different text editor that\'s fine too, but notepad sometimes messes up the spacing. If you only have Notepad, that should work too, that could work too, but if you have Wordpad as well, I would rather use that or some other text editor, if you have a different text editor for editing your functions. So, here\'s how you define the function in Octave. Let me just zoom in a little bit. And this file has just three lines in it. The first line says function Y equals square root number of X, this tells Octave that I\'m gonna return the value Y, I\'m gonna return one value and that the value is going to be saved in the variable Y and moreover, it tells Octave that this function has one argument, which is the argument X, and the way the function body is defined, if Y equals X squared. So, let\'s try to call this function ""square"", this number 5, and this actually isn\'t going to work, and Octave says square this number it\'s undefined. That\'s because Octave doesn\'t know where to find this file. So as usual, let\'s use PWD, or not in my directory, so let\'s see this c:\\users\\ang\\desktop. That\'s where my desktop is. Oops, a little typo there. Users ANG desktop and if I now type square root number 5, it returns the answer 25. As kind of an advanced feature, this is only for those of you that know what the term search path means. But so if you want to modify the Octave search path and you could, you just think of this next part as advanced or optional material. Only for those who are either familiar with the concepts of search paths and permit languages, but you can use the term addpath, safety colon, slash users/ANG/desktop to add that directory to the Octave search path so that even if you know, go to some other directory I can still, Octave still knows to look in the users ANG desktop directory for functions so that even though I\'m in a different directory now, it still knows where to find the square this number function. Okay? But if you\'re not familiar with the concept of search path, don\'t worry about it. Just make sure as you use the CD command to go to the directory of your function before you run it and that actually works just fine. One concept that Octave has that many other programming languages don\'t is that it can also let you define functions that return multiple values or multiple arguments. So here\'s an example of that. Define the function called square and cube this number X and what this says is this function returns 2 values, y1 and y2. When I set down, this follows, y1 is squared, y2 is execute. And what this does is this really returns 2 numbers. So, some of you depending on what programming language you use, if you\'re familiar with, you know, CC++ your offer. Often, we think of the function as return in just one value. But just so the syntax in Octave that should return multiple values. Now back in the Octave window. If I type, you know, a, b equals square and cube this number 5 then a is now equal to 25 and b is equal to the cube of 5 equal to 125. So, this is often convenient if you needed to define a function that returns multiple values. Finally, I\'m going to show you just one more sophisticated example of a function. Let\'s say I have a data set that looks like this, with data points at 1, 1, 2, 2, 3, 3. And what I\'d like to do is to define an octave function to compute the cost function J of theta for different values of theta. First let\'s put the data into octave. So I set my design matrix to be 1,1,1,2,1,3. So, this is my design matrix x with x0, the first column being the said term and the second term being you know, my the x-values of my three training examples. And let me set y to be 1-2-3 as follows, which were the y axis values. So let\'s say theta is equal to 0 semicolon 1. Here at my desktop, I\'ve predefined does cost function j and if I bring up the definition of that function it looks as follows. So function j equals cost function j equals x y theta, some commons, specifying the inputs and then vary few steps set m to be the number trading examples thus the number of rows in x. Compute the predictions, predictions equals x times theta and so this is a common that\'s wrapped around, so this is probably the preceding comment line. Computer script errors by, you know, taking the difference between your predictions and the y values and taking the element of y squaring and then finally computing the cost function J. And Octave knows that J is a value I want to return because J appeared here in the function definition. Feel free by the way to pause this video if you want to look at this function definition for longer and kind of make sure that you understand the different steps. But when I run it in Octave, I run j equals cost function j x y theta. It computes. Oops, made a typo there. It should have been capital X. It computes J equals 0 because if my data set was, you know, 123, 123 then setting, theta 0 equals 0, theta 1 equals 1, this gives me exactly the 45-degree line that fits my data set perfectly. Whereas in contrast if I set theta equals say 0, 0, then this hypothesis is predicting zeroes on everything the same, theta 0 equals 0, theta 1 equals 0 and I compute the cost function then it\'s 2.333 and that\'s actually equal to 1 squared, which is my squared error on the first example, plus 2 squared, plus 3 squared and then divided by 2m, which is 2 times number of training examples, which is indeed 2.33 and so, that sanity checks that this function here is, you know, computing the correct cost function and these are the couple examples we tried out on our simple training example. And so that sanity tracks that the cost function J, as defined here, that it is indeed, you know, seeming to compute the correct cost function, at least on our simple training set that we had here with X and Y being this simple training example that we solved. So, now you know how to right control statements like for loops, while loops and if statements in octave as well as how to define and use functions. In the next video, I\'m going to just very quickly step you through the logistics of working on and submitting problem sets for this class and how to use our submission system. And finally, after that, in the final octave tutorial video, I wanna tell you about vectorization, which is an idea for how to make your octave programs run much fast.'",31,0,1
coursera,stanford_university,machine-learning,vectorization,"b""In this video I like to tell you\nabout the idea of Vectorization. So, whether you using Octave or\na similar language like MATLAB or whether you're using Python [INAUDIBLE],\nR, Java, C++, all of these languages have either\nbuilt into them or have regularly and easily accessible difference in\nnumerical linear algebra libraries. They're usually very well written,\nhighly optimized, often sort of developed by people that\nhave PhDs in numerical computing or they're really specialized\nin numerical computing. And when you're implementing machine\nlearning algorithms, if you're able to take advantage of these linear\nalgebra libraries or these numerical linear algebra libraries, and make some\nroutine calls to them rather than sort of write code yourself to do things\nthat these libraries could be doing. If you do that, then often you get code\nthat, first, is more efficient, so you just run more quickly and take better advantage of any parallel\nhardware your computer may have and so on. And second, it also means that you end up\nwith less code that you need to write, so it's a simpler implementation that is\ntherefore maybe also more likely to be by free. And as a concrete example,\nrather than writing code yourself to multiply matrices, if you let\nOctave do it by typing a times b, that would use a very efficient\nroutine to multiply the two matrices. And there's a bunch of examples like\nthese, where if you use appropriate vectorization implementations you get much\nsimpler code and much more efficient code. Let's look at some examples. Here's our usual hypothesis for\nlinear regression, and if you want to compute h(x),\nnotice that there's a sum on the right. And so one thing you could do is, compute\nthe sum from j = 0 to j = n yourself. Another way to think of this is to\nthink of h(x) as theta transpose x, and what you can do is, think of this\nas you are computing this inner product between two vectors where theta is your\nvector, say, theta 0, theta 1, theta 2. If you have two features,\nif n equals two, and if you think x as this vector,\nx0, x1, x2, and these two views can give you\ntwo different implementations. Here's what I mean. Here's an unvectorized implementation for\nhow to compute and by unvectorize, I mean without vectorization. We might first initialize\nprediction just to be 0.0. The prediction's going to eventually be\nh(x), and then I'm going to have a for loop for j=1 through n+1, prediction\ngets incremented by theta(j) * x(j). So it's kind of this expression over here. By the way, I should mention, in these vectors that I wrote over here,\nI had these vectors being 0 index. So I had theta 0, theta 1, theta 2. But because MATLAB is one index,\ntheta 0 in that MATLAB, we would end up representing as theta 1 and\nthe second element ends up as theta 2 and this third element may end up as theta 3,\njust because our vectors in MATLAB are indexed starting from 1,\neven though I wrote theta and x here, starting indexing from 0,\nwhich is why here I have a for loop. j goes from 1 through n+1 rather than\nj goes through 0 up to n, right? But so this is an unvectorized\nimplementation in that we have for loop that is summing up\nthe n elements of the sum. In contrast, here's how you would\nwrite a vectorized implementation, which is that you would think of a x and\ntheta as vectors. You just said prediction = theta' * x. You're just computing like so. So instead of writing all these\nlines of code with a for loop, you instead just have one line of code. And what this line of code\non the right will do is, it will use Octaves highly optimized\nnumerical linear algebra routines to compute this inner product between the two\nvectors, theta and X, and not only is the vectorized implementation simpler,\nit will also run much more efficiently. So that was octave, but the issue of vectorization applies to\nother programming language as well. Lets look on the example in C++. Here's what an unvectorized\nimplementation might look like. We again initialize prediction to 0.0 and then we now how a for\nloop for j = 0 up to n. Prediction += theta j * x[j],\nwhere again, you have this explicit for loop that you write yourself. In contrast, using a good numerical\nlinear algebra library in C++, you could write a function like,\nor rather. In contrast, using a good numerical\nlinear algebra library in C++, you can instead write code\nthat might look like this. So depending on the details of your\nnumerical linear algebra library, you might be able to have an object, this\nis a C++ object, which is vector theta, and a C++ object which is vector x,\nand you just take theta.transpose * x, where this times becomes a C++\nsort of overload operator so you can just multiply\nthese two vectors in C++. And depending on the details of your\nnumerical linear algebra library, you might end up using\na slightly different syntax, but by relying on the library\nto do this inner product, you can get a much simpler piece of\ncode and a much more efficient one. Let's now look at a more\nsophisticated example. Just to remind you,\nhere's our update rule for a gradient descent of a linear regression. And so we update theta j using this rule\nfor all values of j = 0, 1, 2, and so on. And if I just write out these\nequations for theta 0, theta 1, theta 2, assuming we have two features,\nso n = 2. Then these are the updates we perform for\ntheta 0, theta 1, theta 2, where you might remember my saying in an earlier video,\nthat these should be simultaneous updates. So, let's see if we can come up with\na vectorizing notation of this. Here are my same three equations\nwritten in a slightly smaller font, and you can imagine that one way to implement\nthese three lines of code is to have a for loop that says for j = 0, 1 through 2 to\nupdate theta j, or something like that. But instead, let's come up with\na vectorized implementation and see if we can have a simpler way to basically\ncompress these three lines of code or a for loop that effectively does\nthese three steps one set at a time. Let's see if we can take\nthese three steps and compress them into one\nline of vectorized code. Here's the idea. What I'm going to do is, I'm going to think of theta as a vector, and I'm gonna update theta as theta- alpha times some other vector delta, where delta's is going\nto be equal to 1 over m, sum from i = 1 through m. And then this term over on the right,\nokay? So, let me explain what's going on here. Here, I'm going to treat\ntheta as a vector, so this is n plus one dimensional vector, and I'm saying that theta gets here\nupdated as that's a vector, Rn + 1. Alpha is a real number, and\ndelta, here is a vector. So, this subtraction operation,\nthat's a vector subtraction, okay? Cuz alpha times delta is a vector, and so I'm saying theta gets this vector,\nalpha times delta subtracted from it. So, what is a vector delta? Well this vector delta,\nlooks like this, and what it's meant to be is really\nmeant to be this thing over here. Concretely, delta will be a n\nplus one dimensional vector, and the very first element of the vector\ndelta is going to be equal to that. So, if we have the delta,\nif we index it from 0, if it's delta 0, delta 1,\ndelta 2, what I want is that delta 0 is equal to this\nfirst box in green up above. And indeed,\nyou might be able to convince yourself that delta 0 is this 1\nof the m sum of ho(x), x(i) minus y(i) times x(i) 0. So, let's just make sure\nwe're on this same page about how delta really is computed. Delta is 1 over m times this sum\nover here, and what is this sum? Well, this term over here,\nthat's a real number, and the second term over here, x i, this term over there is a vector, right, because x(i) may be\na vector that would be, say, x(i)0, x(i)1, x(i)2, right, and what is the summation? Well, what the summation\nis saying is that, this term, that is this term over here, this is equal to, (h of(x(1))- y(1)) * x(1) + (h of(x(2))- y(2) x x(2) +, and so on, okay? Because this is summation of i, so\nas i ranges from i = 1 through m, you get these different terms, and\nyou're summing up these terms here. And the meaning of these terms, this\nis a lot like if you remember actually from the earlier quiz in this,\nright, you saw this equation. We said that in order to vectorize this\ncode we will instead said u = 2v + 5w. So we're saying that the vector u\nis equal to two times the vector v plus five times the vector w. So this is an example of how\nto add different vectors and this summation's the same thing. This is saying that the summation over\nhere is just some real number, right? That's kinda like the number two or\nsome other number times the vector, x1. So it's kinda like 2v or\nsay some other number times x1, and then plus instead of 5w we instead\nhave some other real number, plus some other vector, and\nthen you add on other vectors, plus dot, dot, dot, plus the other vectors,\nwhich is why, over all, this thing over here, that whole quantity,\nthat delta is just some vector. And concretely, the three elements\nof delta correspond if n = 2, the three elements of delta\ncorrespond exactly to this thing, to the second thing, and this third thing. Which is why when you update theta\naccording to theta- alpha delta, we end up carrying exactly\nthe same simultaneous updates as the update rules that we have up top. So, I know that there was a lot that\nhappened on this slide, but again, feel free to pause the video and\nif you aren't sure what just happened I'd encourage you\nto step through this slide to make sure you understand why is it that this\nupdate here with this definition of delta, right, why is it that that's\nequal to this update on top? And if it's still not clear,\none insight is that, this thing over here, that's exactly the vector x, and so we're just taking all three of these\ncomputations, and compressing them into one step with this vector delta,\nwhich is why we can come up with a vectorized implementation of this\nstep of the new refresh in this way. So, I hope this step makes sense and\ndo look at the video and see if you can understand it. In case you don't understand quite\nthe equivalence of this map, if you implement this, this turns\nout to be the right answer anyway. So, even if you didn't quite\nunderstand equivalence, if you just implement it this way, you'll\nbe able to get linear regression to work. But if you're able to figure out\nwhy these two steps are equivalent, then hopefully that will give you a better\nunderstanding of vectorization as well. And finally, if you are implementing\nlinear regression using more than one or two features, so sometimes we use\nlinear regression with 10's or 100's or 1,000's of features. But if you use the vectorized\nimplementation of linear regression, you'll see that will run much faster\nthan if you had, say, your old for loop that was updating theta zero,\nthen theta one, then theta two yourself. So, using a vectorized implementation,\nyou should be able to get a much more efficient\nimplementation of linear regression. And when you vectorize later algorithms\nthat we'll see in this class, there's good trick, whether in Octave or\nsome other language like C++, Java, for getting your code to\nrun more efficiently.""",32,0,1
coursera,stanford_university,machine-learning,classification,"b""In this and the next few videos, I want\nto start to talk about classification problems, where the variable y that\nyou want to predict is valued. We'll develop an algorithm\ncalled logistic regression, which is one of the most popular and most\nwidely used learning algorithms today. Here are some examples of\nclassification problems. Earlier we talked about\nemail spam classification as an example of a classification problem. Another example would be\nclassifying online transactions. So if you have a website\nthat sells stuff and if you want to know if a particular\ntransaction is fraudulent or not, whether someone is using a stolen credit\ncard or has stolen the user's password. There's another classification problem. And earlier we also talked about\nthe example of classifying tumors as cancerous,\nmalignant or as benign tumors. In all of these problems the variable that\nwe're trying to predict is a variable y that we can think of as taking\non two values either zero or one, either spam or not spam, fraudulent or not\nfraudulent, related malignant or benign. Another name for the class that we denote\nwith zero is the negative class, and another name for the class that we\ndenote with one is the positive class. So zero we denote as the benign tumor,\nand one, positive class we denote\na malignant tumor. The assignment of the two classes,\nspam not spam and so on. The assignment of the two classes to\npositive and negative to zero and one is somewhat arbitrary and it doesn't really matter but often there\nis this intuition that a negative class is conveying the absence of something\nlike the absence of a malignant tumor. Whereas one the positive class is\nconveying the presence of something that we may be looking for, but\nthe definition of which is negative and which is positive is somewhat arbitrary\nand it doesn't matter that much. For now we're going to start with\nclassification problems with just two classes zero and one. Later one we'll talk about multi\nclass problems as well where therefore y may take on four values zero,\none, two, and three. This is called a multiclass\nclassification problem. But for the next few videos, let's\nstart with the two class or the binary classification problem and we'll worry\nabout the multiclass setting later. So how do we develop\na classification algorithm? Here's an example of a training set for\na classification task for classifying a tumor as malignant or\nbenign. And notice that malignancy takes on\nonly two values, zero or no, one or yes. So one thing we could do\ngiven this training set is to apply the algorithm\nthat we already know. Linear regression to this data set and just try to fit the straight\nline to the data. So if you take this training set and\nfill a straight line to it, maybe you get a hypothesis\nthat looks like that, right. So that's my hypothesis. H(x) equals theta transpose x. If you want to make predictions one thing\nyou could try doing is then threshold the classifier outputs at\n0.5 that is at a vertical axis value 0.5 and if the hypothesis outputs a value that is greater than\nequal to 0.5 you can take y = 1. If it's less than 0.5 you can take y=0. Let's see what happens if we do that. So 0.5 and so\nthat's where the threshold is and that's using linear regression this way. Everything to the right of this\npoint we will end up predicting as the positive cross. Because the output values is greater than\n0.5 on the vertical axis and everything to the left of that point we will end\nup predicting as a negative value. In this particular example, it looks like linear regression is\nactually doing something reasonable. Even though this is a classification\ntoss we're interested in. But now let's try changing\nthe problem a bit. Let me extend out the horizontal\naccess a little bit and let's say we got one more training\nexample way out there on the right. Notice that that additional\ntraining example, this one out here, it doesn't\nactually change anything, right. Looking at the training set it's pretty\nclear what a good hypothesis is. Is that well everything to\nthe right of somewhere around here, to the right of this we\nshould predict this positive. Everything to the left we should probably\npredict as negative because from this training set, it looks like all the tumors\nlarger than a certain value around here are malignant, and all the tumors smaller\nthan that are not malignant, at least for this training set. But once we've added that extra example\nover here, if you now run linear regression, you instead get\na straight line fit to the data. That might maybe look like this. And if you know threshold\nhypothesis at 0.5, you end up with a threshold\nthat's around here, so that everything to the right of this\npoint you predict as positive and everything to the left of that\npoint you predict as negative. And this seems a pretty bad thing for\nlinear regression to have done, right, because you know these are our positive\nexamples, these are our negative examples. It's pretty clear we really should be\nseparating the two somewhere around there, but somehow by adding one example\nway out here to the right, this example really isn't\ngiving us any new information. I mean, there should be no surprise\nto the learning algorithm. That the example way out here\nturns out to be malignant. But somehow having that example out\nthere caused linear regression to change its straight-line fit to the data\nfrom this magenta line out here to this blue line over here, and\ncaused it to give us a worse hypothesis. So, applying linear regression\nto a classification problem often isn't a great idea. In the first example, before I\nadded this extra training example, previously linear regression was just\ngetting lucky and it got us a hypothesis that worked well for that particular\nexample, but usually applying linear regression to a data set, you might\nget lucky but often it isn't a good idea. So I wouldn't use linear regression for\nclassification problems. Here's one other funny thing about\nwhat would happen if we were to use linear regression for\na classification problem. For classification we know\nthat y is either zero or one. But if you are using linear\nregression where the hypothesis can output values that are much\nlarger than one or less than zero, even if all of your training examples\nhave labels y equals zero or one. And it seems kind of\nstrange that even though we know that the labels should be zero,\none it seems kind of strange if the algorithm can output values much\nlarger than one or much smaller than zero. So what we'll do in the next few videos\nis develop an algorithm called logistic regression, which has\nthe property that the output, the predictions of logistic regression\nare always between zero and one, and doesn't become bigger than one or\nbecome less than zero. And by the way,\nlogistic regression is, and we will use it as a classification\nalgorithm, is some, maybe sometimes confusing that the term\nregression appears in this name even though logistic regression is\nactually a classification algorithm. But that's just a name it was given for\nhistorical reasons. So don't be confused by that logistic\nregression is actually a classification algorithm that we apply to settings\nwhere the label y is discrete value, when it's either zero or one. So hopefully you now know why,\nif you have a classification problem, using linear regression isn't a good idea. In the next video, we'll start working out the details\nof the logistic regression algorithm.""",33,0,1
coursera,stanford_university,machine-learning,hypothesis-representation,"b""Let's start talking about\nlogistic regression. In this video, I'd like to show\nyou the hypothesis representation. That is, what is the function we're\ngoing to use to represent our hypothesis when we have a classification problem? Earlier, we said that we\nwould like our classifier to output values that are between 0 and 1. So we'd like to come up with a hypothesis\nthat satisfies this property, that is, predictions are maybe between 0 and 1. When we were using linear regression,\nthis was the form of a hypothesis, where h(x) is theta transpose x. For logistic regression,\nI'm going to modify this a little bit and make the hypothesis g\nof theta transpose x. Where I'm going to define\nthe function g as follows. G(z), z is a real number, is equal to\none over one plus e to the negative z. This is called the sigmoid function,\nor the logistic function, and the term logistic function, that's what gives rise to\nthe name logistic regression. And by the way,\nthe terms sigmoid function and logistic function are basically\nsynonyms and mean the same thing. So the two terms are basically\ninterchangeable, and either term can be used to\nrefer to this function g. And if we take these two equations and\nput them together, then here's just an alternative way of\nwriting out the form of my hypothesis. I'm saying that h(x) Is 1 over 1 plus\ne to the negative theta transpose x. And all I've do is I've\ntaken this variable z, z here is a real number, and\nplugged in theta transpose x. So I end up with theta transpose\nx in place of z there. Lastly, let me show you what\nthe sigmoid function looks like. We're gonna plot it on this figure here. The sigmoid function, g(z), also called\nthe logistic function, it looks like this. It starts off near 0 and\nthen it rises until it crosses 0.5 and the origin, and\nthen it flattens out again like so. So that's what the sigmoid\nfunction looks like. And you notice that the sigmoid function,\nwhile it asymptotes at one and asymptotes at zero, as a z axis,\nthe horizontal axis is z. As z goes to minus infinity,\ng(z) approaches zero. And as g(z) approaches infinity,\ng(z) approaches one. And so because g(z) upwards\nvalues are between zero and one, we also have that h(x)\nmust be between zero and one. Finally, given this hypothesis\nrepresentation, what we need to do, as before,\nis fit the parameters theta to our data. So given a training set we\nneed to a pick a value for the parameters theta and this hypothesis\nwill then let us make predictions. We'll talk about a learning algorithm\nlater for fitting the parameters theta, but first let's talk a bit about\nthe interpretation of this model. Here's how I'm going to interpret\nthe output of my hypothesis, h(x). When my hypothesis outputs some number,\nI am going to treat that number as the estimated probability that y is\nequal to one on a new input, example x. Here's what I mean, here's an example. Let's say we're using the tumor\nclassification example, so we may have a feature vector x,\nwhich is this x zero equals one as always. And then one feature is\nthe size of the tumor. Suppose I have a patient come in and\nthey have some tumor size and I feed their feature vector\nx into my hypothesis. And suppose my hypothesis\noutputs the number 0.7. I'm going to interpret my\nhypothesis as follows. I'm gonna say that this\nhypothesis is telling me that for a patient with features x,\nthe probability that y equals 1 is 0.7. In other words, I'm going to\ntell my patient that the tumor, sadly, has a 70 percent chance, or\na 0.7 chance of being malignant. To write this out slightly more formally,\nor to write this out in math, I'm going to interpret\nmy hypothesis output as. P of y=1 given x parameterized by theta. So for those of you that are familiar with\nprobability, this equation may make sense. If you're a little less\nfamiliar with probability, then here's how I read this expression. This is the probability\nthat y is equal to one. Given x,\ngiven that my patient has features x, so given my patient has a particular tumor\nsize represented by my features x. And this probability is\nparameterized by theta. So I'm basically going to count\non my hypothesis to give me estimates of the probability\nthat y is equal to 1. Now, since this is a classification task, we know that y must be either 0 or\n1, right? Those are the only two values\nthat y could possibly take on, either in the training set or for new\npatients that may walk into my office, or into the doctor's office in the future. So given h(x), we can therefore\ncompute the probability that y = 0 as well, completely\nbecause y must be either 0 or 1. We know that the probability of y = 0 plus\nthe probability of y = 1 must add up to 1. This first equation looks\na little bit more complicated. It's basically saying that\nprobability of y=0 for a particular patient with features x,\nand given our parameters theta. Plus the probability of y=1 for\nthat same patient with features x and given theta parameters\ntheta must add up to one. If this equation looks\na little bit complicated, feel free to mentally imagine\nit without that x and theta. And this is just saying that the product\nof y equals zero plus the product of y equals one, must be equal to one. And we know this to be true because y\nhas to be either zero or one, and so the chance of y equals zero,\nplus the chance that y is one. Those two must add up to one. And so if you just take this term and move it to the right hand side,\nthen you end up with this equation. That says probability that y equals zero\nis 1 minus probability of y equals 1, and thus if our hypothesis feature\nof x gives us that term. You can therefore quite simply\ncompute the probability or compute the estimated probability\nthat y is equal to 0 as well. So, you now know what the hypothesis\nrepresentation is for logistic regression and we're seeing\nwhat the mathematical formula is, defining the hypothesis for\nlogistic regression. In the next video, I'd like to\ntry to give you better intuition about what the hypothesis\nfunction looks like. And I wanna tell you about something\ncalled the decision boundary. And we'll look at some visualizations\ntogether to try to get a better sense of what this hypothesis function of\nlogistic regression really looks like.""",34,0,1
coursera,stanford_university,machine-learning,decision-boundary,"b""In the last video, we talked about\nthe hypothesis representation for logistic regression. What Id like to do now is tell you about\nsomething called the decision boundary, and this will give us a better sense\nof what the logistic regressions hypothesis function is computing. To recap, this is what we wrote\nout last time, where we said that the hypothesis is represented as h\nof x equals g of theta transpose x, where g is this function called the\nsigmoid function, which looks like this. It slowly increases from zero to one,\nasymptoting at one. What I want to do now is\ntry to understand better when this hypothesis will\nmake predictions that y is equal to 1 versus when it might\nmake predictions that y is equal to 0. And understand better what\nhypothesis function looks like particularly when we have\nmore than one feature. Concretely, this hypothesis\nis outputting estimates of the probability that y is equal to one,\ngiven x and parameterized by theta. So if we wanted to predict\nis y equal to one or is y equal to zero,\nhere's something we might do. Whenever the hypothesis outputs\nthat the probability of y being one is greater than or equal to 0.5, so\nthis means that if there is more likely to be y equals 1 than y equals 0,\nthen let's predict y equals 1. And otherwise, if the probability,\nthe estimated probability of y being over 1 is less than 0.5,\nthen let's predict y equals 0. And I chose a greater than or\nequal to here and less than here If h of x is equal to 0.5 exactly, then\nyou could predict positive or negative, but I probably created a loophole here, so\nwe default maybe to predicting positive if h of x is 0.5, but that's a detail\nthat really doesn't matter that much. What I want to do is understand better\nwhen is it exactly that h of x will be greater than or equal to 0.5, so that\nwe'll end up predicting y is equal to 1. If we look at this plot\nof the sigmoid function, we'll notice that the sigmoid function,\ng of z is greater than or equal to 0.5 whenever z is greater than or\nequal to zero. So is in this half of the figure that g\ntakes on values that are 0.5 and higher. This notch here, that's 0.5, and\nso when z is positive, g of z, the sigmoid function is greater than or\nequal to 0.5. Since the hypothesis for logistic\nregression is h of x equals g of theta and transpose x, this is therefore going\nto be greater than or equal to 0.5, whenever theta transpose x is\ngreater than or equal to 0. So what we're shown, right, because here\ntheta transpose x takes the role of z. So what we're shown is that a hypothesis\nis gonna predict y equals 1 whenever theta transpose x is\ngreater than or equal to 0. Let's now consider the other case of when\na hypothesis will predict y is equal to 0. Well, by similar argument,\nh(x) is going to be less than 0.5 whenever g(z) is less than 0.5\nbecause the range of values of z that cause g(z) to take on values less\nthan 0.5, well, that's when z is negative. So when g(z) is less than 0.5, a hypothesis will predict\nthat y is equal to 0. And by similar argument\nto what we had earlier, h(x) is equal to g of\ntheta transpose x and so we'll predict y equals 0 whenever this\nquantity theta transpose x is less than 0. To summarize what we just worked out, we\nsaw that if we decide to predict whether y=1 or y=0 depending on whether the\nestimated probability is greater than or equal to 0.5, or whether less than 0.5,\nthen that's the same as saying that when we predict y=1 whenever theta\ntranspose x is greater than or equal to 0. And we'll predict y is equal to 0 whenever\ntheta transpose x is less than 0. Let's use this to better understand how the hypothesis of logistic\nregression makes those predictions. Now, let's suppose we have a training\nset like that shown on the slide. And suppose a hypothesis is h of x equals g of theta zero plus\ntheta one x one plus theta two x two. We haven't talked yet about how to\nfit the parameters of this model. We'll talk about that in the next video. But suppose that via\na procedure to specified. We end up choosing the following\nvalues for the parameters. Let's say we choose theta 0 equals 3,\ntheta 1 equals 1, theta 2 equals 1. So this means that my parameter\nvector is going to be theta equals minus 3, 1, 1. So, when given this choice\nof my hypothesis parameters, let's try to figure out where a hypothesis\nwould end up predicting y equals one and where it would end up\npredicting y equals zero. Using the formulas that we were\ntaught on the previous slide, we know that y equals one is more likely,\nthat is the probability that y equals one is greater than or\nequal to 0.5, whenever theta transpose\nx is greater than zero. And this formula that I just underlined,\n-3 + x1 + x2, is, of course,\ntheta transpose x when theta is equal to this value of\nthe parameters that we just chose. So for any example, for\nany example which features x1 and x2 that satisfy this equation, that minus 3 plus x1 plus x2 is\ngreater than or equal to 0, our hypothesis will think that y equals 1, the\nsmall x will predict that y is equal to 1. We can also take -3 and\nbring this to the right and rewrite this as x1+x2 is greater than or\nequal to 3, so equivalently, we found that\nthis hypothesis would predict y=1 whenever x1+x2 is greater than or\nequal to 3. Let's see what that means on the figure,\nif I write down the equation, X1 + X2 = 3, this defines\nthe equation of a straight line and if I draw what that straight line looks\nlike, it gives me the following line which passes through 3 and\n3 on the x1 and the x2 axis. So the part of the infospace,\nthe part of the X1 X2 plane that corresponds to when X1 plus\nX2 is greater than or equal to 3, that's going to be this right half thing,\nthat is everything to the up and everything to the upper right portion\nof this magenta line that I just drew. And so, the region where our hypothesis\nwill predict y = 1, is this region, just really this huge region,\nthis half space over to the upper right. And let me just write that down,\nI'm gonna call this the y = 1 region. And, in contrast,\nthe region where x1 + x2 is less than 3, that's when we will predict\nthat y is equal to 0. And that corresponds to this region. And there's really a half plane, but that region on the left is the region\nwhere our hypothesis will predict y = 0. I wanna give this line,\nthis magenta line that I drew a name. This line, there,\nis called the decision boundary. And concretely, this straight line,\nX1 plus X equals 3. That corresponds to the set of points, so\nthat corresponds to the region where H of X is equal to 0.5 exactly and the decision\nboundary that is this straight line, that's the line that separates the region\nwhere the hypothesis predicts Y equals 1 from the region where the hypothesis\npredicts that y is equal to zero. And just to be clear, the decision\nboundary is a property of the hypothesis including the parameters theta zero,\ntheta one, theta two. And in the figure I drew a training set, I drew a data set,\nin order to help the visualization. But even if we take away the data set\nthis decision boundary and the region where we predict y =1 versus y = 0,\nthat's a property of the hypothesis and of the parameters of the hypothesis and\nnot a property of the data set. Later on, of course, we'll talk\nabout how to fit the parameters and there we'll end up using the training set,\nusing our data. To determine the value of the parameters. But once we have particular values for\nthe parameters theta0, theta1, theta2 then that completely defines\nthe decision boundary and we don't actually need to plot a training set\nin order to plot the decision boundary. Let's now look at a more complex\nexample where as usual, I have crosses to denote my positive examples and\nOs to denote my negative examples. Given a training set like this, how can I get logistic regression\nto fit the sort of data? Earlier when we were talking\nabout polynomial regression or when we're talking about linear\nregression, we talked about how we could add extra higher order polynomial\nterms to the features. And we can do the same for\nlogistic regression. Concretely, let's say my hypothesis\nlooks like this where I've added two extra features, x1 squared and\nx2 squared, to my features. So that I now have five parameters,\ntheta zero through theta four. As before, we'll defer to the next video,\nour discussion on how to automatically choose values for the\nparameters theta zero through theta four. But let's say that varied\nprocedure to be specified, I end up choosing theta zero equals\nminus one, theta one equals zero, theta two equals zero, theta three\nequals one and theta four equals one. What this means is that with this\nparticular choose of parameters, my parameter effect theta theta looks\nlike minus one, zero, zero, one, one. Following our earlier discussion, this\nmeans that my hypothesis will predict that y=1 whenever -1 + x1 squared + x2\nsquared is greater than or equal to 0. This is whenever theta transpose\ntimes my theta transfers, my features is greater than or\nequal to zero. And if I take minus 1 and\njust bring this to the right, I'm saying that my hypothesis will\npredict that y is equal to 1 whenever x1 squared plus x2 squared is greater than or\nequal to 1. So what does this decision\nboundary look like? Well, if you were to plot the curve for\nx1 squared plus x2 squared equals 1 Some of you will recognize that,\nthat is the equation for circle of radius one,\ncentered around the origin. So that is my decision boundary. And everything outside the circle,\nI'm going to predict as y=1. So out here is my y equals 1 region, we'll predict y equals 1 out here and inside the circle is where\nI'll predict y is equal to 0. So by adding these more complex, or these\npolynomial terms to my features as well, I can get more complex decision\nboundaries that don't just try to separate the positive and\nnegative examples in a straight line that I can get in this example,\na decision boundary that's a circle. Once again, the decision boundary is\na property, not of the trading set, but of the hypothesis under the parameters. So, so long as we're given\nmy parameter vector theta, that defines the decision boundary,\nwhich is the circle. But the training set is not what we\nuse to define the decision boundary. The training set may be used\nto fit the parameters theta. We'll talk about how to do that later. But, once you have the parameters theta, that is what defines\nthe decisions boundary. Let me put back the training set just for\nvisualization. And finally let's look at\na more complex example. So can we come up with even more\ncomplex decision boundaries then this? If I have even higher polynomial terms so\nthings like X1 squared, X1 squared X2,\nX1 squared equals squared and so on. And have much higher polynomials,\nthen it's possible to show that you can get even more complex decision\nboundaries and the regression can be used to find decision boundaries that may,\nfor example, be an ellipse like that or maybe a little bit different setting of\nthe parameters maybe you can get instead a different decision boundary which may\neven look like some funny shape like that. Or for even more complete examples\nmaybe you can also get this decision boundaries that could look like more\ncomplex shapes like that where everything in here you predict y = 1 and\neverything outside you predict y = 0. So this higher autopolynomial features you\ncan a very complex decision boundaries. So, with these visualizations, I hope that\ngives you a sense of what's the range of hypothesis functions we can represent\nusing the representation that we have for logistic regression. Now that we know what h(x) can represent,\nwhat I'd like to do next in the following video is talk about how to automatically\nchoose the parameters theta so that given a training set we can automatically\nfit the parameters to our data.""",35,0,1
coursera,stanford_university,machine-learning,cost-function,"b""In this video, we'll talk about how\nto fit the parameters of theta for the logistic compression. In particular, I'd like to define\nthe optimization objective, or the cost function that we'll\nuse to fit the parameters. Here's the supervised learning problem\nof fitting logistic regression model. We have a training set of m\ntraining examples and as usual, each of our examples is represented\nby a that's n plus one dimensional, and as usual we have x o equals one. First feature or\na zero feature is always equal to one. And because this is a computation problem, our training set has the property\nthat every label y is either 0 or 1. This is a hypothesis, and the parameters\nof a hypothesis is this theta over here. And the question that I want to talk\nabout is given this training set, how do we choose, or\nhow do we fit the parameter's theta? Back when we were developing\nthe linear regression model, we used the following cost function. I've written this slightly differently\nwhere instead of 1 over 2m, I've taken a one-half and\nput it inside the summation instead. Now I want to use an alternative way\nof writing out this cost function. Which is that instead of writing\nout this square of return here, let's write in here costs of h of x, y and I'm going to define that total cost\nof h of x, y to be equal to this. Just equal to this one-half\nof the squared error. So now we can see more clearly that the\ncost function is a sum over my training set, which is 1 over n times the sum of\nmy training set of this cost term here. And to simplify this\nequation a little bit more, it's going to be convenient to\nget rid of those superscripts. So just define cost of h\nof x comma y to be equal to one half of this squared error. And interpretation of this cost function\nis that, this is the cost I want my learning algorithm to have to\npay if it outputs that value, if its prediction is h of x,\nand the actual label was y. So just cross off the superscripts, right, and no surprise for linear regression\nthe cost we've defined is that or the cost of this is that is one-half\ntimes the square difference between what I predicted and\nthe actual value that we have, 0 for y. Now this cost function worked fine for\nlinear regression. But here,\nwe're interested in logistic regression. If we could minimize this cost\nfunction that is plugged into J here, that will work okay. But it turns out that if we use\nthis particular cost function, this would be a non-convex\nfunction of the parameter's data. Here's what I mean by non-convex. Have some cross function j of theta and for logistic regression,\nthis function h here has a nonlinearity that is one over one\nplus e to the negative theta transpose. So this is a pretty\ncomplicated nonlinear function. And if you take the function,\nplug it in here. And then take this cost function and\nplug it in there and then plot what j of theta looks like. You find that j of theta can look\nlike a function that's like this with many local optima. And the formal term for this is\nthat this is a non-convex function. And you can kind of tell, if you were\nto run gradient descent on this sort of function It is not guaranteed to\nconverge to the global minimum. Whereas in contrast what we would like is\nto have a cost function j of theta that is convex, that is a single bow-shaped\nfunction that looks like this so that if you run theta in the we\nwould be guaranteed that would converge to the global minimum. And the problem with using this great\ncost function is that because of this very nonlinear function that appears\nin the middle here, J of theta ends up being a nonconvex function if you were\nto define it as a square cost function. So what we'd like to do is, instead of\ncome up with a different cost function, that is convex, and so\nthat we can apply a great algorithm, like gradient descent and\nbe guaranteed to find the global minimum. Here's the cost function that we're\ngoing to use for logistic regression. We're going to say that the cost, or the penalty that the algorithm pays,\nif it upwards the value of h(x), so if this is some number like 0.7,\nit predicts the value h of x. And the actual cost\nlabel turns out to be y. The cost is going to be\n-log(h(x)) if y = 1 and -log(1- h(x)) if y = 0. This looks like a pretty\ncomplicated function, but let's plot this function to gain some\nintuition about what it's doing. Let's start off with the case of y = 1. If y = 1,\nthen the cost function is -log(h(x)). And if we plot that, so let's say\nthat the horizontal axis is h(x), so we know that a hypothesis is going\nto output a value between 0 and 1. Right, so h(x),\nthat varies between 0 and 1. If you plot what this cost function looks\nlike, you find that it looks like this. One way to see why the plot looks like\nthis is because if you were to plot log z with z on the horizontal axis,\nthen that looks like that. And it approaches minus infinity, right? So this is what the log\nfunction looks like. And this is 0, this is 1. Here, z is of course\nplaying the role of h of x. And so -log z will look like this. Just flipping the sign, minus log z,\nand we're interested only in the range of when this function goes between\nzero and one, so get rid of that. And so we're just left with,\nyou know, this part of the curve, and that's what this curve\non the left looks like. Now, this cost function has a few\ninteresting and desirable properties. First, you notice that if y is\nequal to 1 and h(x) is equal to 1, in other words, if the hypothesis\nexactly predicts h equals 1 and y is exactly equal to what it predicted,\nthen the cost = 0 right? That corresponds to the curve\ndoesn't actually flatten out. The curve is still going. First, notice that if h(x) = 1,\nif that hypothesis predicts that y = 1 and\nif indeed y = 1 then the cost = 0. That corresponds to this point down here,\nright? If h(x) = 1 and we're only\nconsidering the case of y = 1 here. But if h(x) = 1 then the cost\nis down here, is equal to 0. And that's where we'd like it to be\nbecause if we correctly predict the output y, then the cost is 0. But now notice also that\nas h(x) approaches 0, so as the output of a hypothesis approaches 0,\nthe cost blows up and it goes to infinity. And what this does is this captures\nthe intuition that if a hypothesis of 0, that's like saying a hypothesis saying\nthe chance of y equals 1 is equal to 0. It's kinda like our going to\nour medical patients and saying the probability that you have a malignant\ntumor, the probability that y=1, is zero. So, it's like absolutely impossible\nthat your tumor is malignant. But if it turns out that the tumor, the\npatient's tumor, actually is malignant, so if y is equal to one,\neven after we told them, that the probability of\nit happening is zero. So it's absolutely impossible for\nit to be malignant. But if we told them this with that level\nof certainty and we turn out to be wrong, then we penalize the learning\nalgorithm by a very, very large cost. And that's captured by\nhaving this cost go to infinity if y equals 1 and\nh(x) approaches 0. This slide consider\nthe case of y equals 1. Let's look at what the cost\nfunction looks like for y equals 0. If y is equal to 0,\nthen the cost looks like this, it looks like this expression over here,\nand if you plot the function, -log(1-z), what you get is the cost\nfunction actually looks like this. So it goes from 0 to 1,\nsomething like that and so if you plot the cost function for the case of y equals\n0, you find that it looks like this. And what this curve does\nis it now goes up and it goes to plus infinity as h of x\ngoes to 1 because as I was saying, that if y turns out to be equal to 0. But we predicted that y is equal to\n1 with almost certainly, probably 1, then we end up paying a very large cost. And conversely, if h of x is equal to 0 and y equals 0,\nthen the hypothesis melted. The protected y of z is equal to 0,\nand it turns out y is equal to 0, so at this point,\nthe cost function is going to be 0. In this video, we will define the cost\nfunction for a single train example. The topic of convexity analysis is now\nbeyond the scope of this course, but it is possible to show that with\na particular choice of cost function, this will give a convex\noptimization problem. Overall cost function j of theta will\nbe convex and local optima free. In the next video we're gonna take\nthese ideas of the cost function for a single training example and develop that\nfurther, and define the cost function for the entire training set. And we'll also figure out a simpler way\nto write it than we have been using so far, and based on that we'll\nwork out grading descent, and that will give us logistic\ncompression algorithm.""",36,0,1
coursera,stanford_university,machine-learning,simplified-cost-function-and-gradient-descent,"b""In this video, we'll figure out a slightly\nsimpler way to write the cost function than we have been using so far. And we'll also figure out how\nto apply gradient descent to fit the parameters\nof logistic regression. So, by the end of this, video you know how to implement a fully\nworking version of logistic regression. Here's our cost function for\nlogistic regression. Our overall cost function is 1 over m\ntimes the sum over the trading set of the cost of making different predictions\non the different examples of labels y i. And this is the cost of a single\nexample that we worked out earlier. And just want to remind you that for classification problems in our training\nsets, and in fact even for examples, now that our training set y is\nalways equal to zero or one, right? That's sort of part of\nthe mathematical definition of y. Because y is either zero or one, we'll be able to come up with\na simpler way to write this cost function. And in particular,\nrather than writing out this cost function on two separate lines with two separate\ncases, so y equals one and y equals zero. I'm going to show you a way\nto take these two lines and compress them into one equation. And this would make it more convenient\nto write out a cost function and derive gradient descent. Concretely, we can write out\nthe cost function as follows. We say that cost of H(x), y. I'm gonna write this as -y times log h(x)- (1-y) times log (1-h(x)). And I'll show you in a second that\nthis expression, no, this equation, is an equivalent way, or more compact way, of writing out this definition of\nthe cost function that we have up here. Let's see why that's the case. We know that there are only\ntwo possible cases. Y must be zero or one. So let's suppose Y equals one. If y is equal to 1, than this equation\nis saying that the cost is equal to, well if y is equal to 1,\nthen this thing here is equal to 1. And 1 minus y is going to be equal to 0,\nright. So if y is equal to 1, then 1 minus\ny is 1 minus 1, which is therefore 0. So the second term gets\nmultiplied by 0 and goes away. And we're left with only this first term,\nwhich is y times log- y times log (h(x)). Y is 1 so that's equal to -log h(x). And this equation is exactly what\nwe have up here for if y = 1. The other case is if y = 0. And if that's the case, then our writing\nof the cos function is saying that, well, if y is equal to 0, then this\nterm here would be equal to zero. Whereas 1 minus y,\nif y is equal to zero would be equal to 1, because 1 minus y becomes 1 minus\nzero which is just equal to 1. And so the cost function simplifies\nto just this last term here, right? Because the fist term over here gets\nmultiplied by zero, and so it disappears, and so it's just left with this last term,\nwhich is -log (1- h(x)). And you can verify that this term\nhere is just exactly what we had for when y is equal to 0. So this shows that this definition for\nthe cost is just a more compact way of taking both of these expressions,\nthe cases y =1 and y = 0, and writing them in a more\nconvenient form with just one line. We can therefore write all\nour cost functions for logistic regression as follows. It is this 1 over m of the sum\nof these cost functions. And plugging in the definition for the cost that we worked out earlier,\nwe end up with this. And we just put the minus sign outside. And why do we choose\nthis particular function, while it looks like there could be other\ncost functions we could have chosen. Although I won't have time to go into\ngreat detail of this in this course, this cost function can be\nderived from statistics using the principle of maximum\nlikelihood estimation. Which is an idea in statistics for how to efficiently find parameters'\ndata for different models. And it also has a nice\nproperty that it is convex. So this is the cost function that essentially everyone uses when\nfitting logistic regression models. If you don't understand the terms that\nI just said, if you don't know what the principle of maximum likelihood\nestimation is, don't worry about it. But it's just a deeper rationale and\njustification behind this particular cost function than\nI have time to go into in this class. Given this cost function,\nin order to fit the parameters, what we're going to do then is try to find the\nparameters theta that minimize J of theta. So if we try to minimize this, this would\ngive us some set of parameters theta. Finally, if we're given a new example with\nsome set of features x, we can then take the thetas that we fit to our training\nset and output our prediction as this. And just to remind you, the output of\nmy hypothesis I'm going to interpret as the probability that y is equal to one. And given the input x and\nparameterized by theta. But just, you can think of this as just my\nhypothesis as estimating the probability that y is equal to one. So all that remains to be done is figure\nout how to actually minimize J of theta as a function of theta so that we can actually fit\nthe parameters to our training set. The way we're going to minimize the cost\nfunction is using gradient descent. Here's our cost function and if we want\nto minimize it as a function of theta, here's our usual template for graded descent where we repeatedly\nupdate each parameter by taking, updating it as itself minus learning\nray alpha times this derivative term. If you know some calculus, feel free\nto take this term and try to compute the derivative yourself and see if you can\nsimplify it to the same answer that I get. But even if you don't know\ncalculus don't worry about it. If you actually compute this, what you get is this equation,\nand just write it out here. It's sum from i equals one through m\nof essentially the error times xij. So if you take this partial derivative\nterm and plug it back in here, we can then write out our gradient\ndescent algorithm as follows. And all I've done is I took the derivative\nterm for the previous slide and plugged it in there. So if you have n features,\nyou would have a parameter vector theta, which with parameters theta 0,\ntheta 1, theta 2, down to theta n. And you will use this update\nto simultaneously update all of your values of theta. Now, if you take this update rule and compare it to what we were doing for\nlinear regression. You might be surprised to realize that,\nwell, this equation was exactly what we had for\nlinear regression. In fact,\nif you look at the earlier videos, and look at the update rule, the Gradient\nDescent rule for linear regression. It looked exactly like what I\ndrew here inside the blue box. So are linear regression and logistic\nregression different algorithms or not? Well, this is resolved by observing\nthat for logistic regression, what has changed is that the definition\nfor this hypothesis has changed. So as whereas for linear regression,\nwe had h(x) equals theta transpose X, now this definition of h(x) has changed. And is instead now one over one\nplus e to the negative transpose x. So even though the update rule\nlooks cosmetically identical, because the definition of\nthe hypothesis has changed, this is actually not the same thing as\ngradient descent for linear regression. In an earlier video, when we were\ntalking about gradient descent for linear regression, we had talked about how\nto monitor a gradient descent to make sure that it is converging. I usually apply that same\nmethod to logistic regression, too to monitor a gradient descent,\nto make sure it's converging correctly. And hopefully, you can figure out how to apply that\ntechnique to logistic regression yourself. When implementing logistic\nregression with gradient descent, we have all of these\ndifferent parameter values, theta zero down to theta n, that we\nneed to update using this expression. And one thing we could do is have a for\nloop. So for i equals zero to n, or\nfor i equals one to n plus one. So update each of these\nparameter values in turn. But of course rather than using a for\nloop, ideally we would also use\na vector rise implementation. So that a vector rise implementation\ncan update all of these m plus one parameters all in one fell swoop. And to check your own understanding, you\nmight see if you can figure out how to do the vector rise implementation\nwith this algorithm as well. So, now you you know how to implement\ngradient descents for logistic regression. There was one last idea that we\nhad talked about earlier, for linear regression,\nwhich was feature scaling. We saw how feature scaling can help\ngradient descent converge faster for linear regression. The idea of feature scaling also\napplies to gradient descent for logistic regression. And yet we have features that are on very\ndifferent scale, then applying feature scaling can also make grading descent\nrun faster for logistic regression. So that's it, you now know how to\nimplement logistic regression and this is a very powerful, and probably the most widely used,\nclassification algorithm in the world. And you now know how we get it to work for\nyourself.""",37,0,1
coursera,stanford_university,machine-learning,advanced-optimization,"b""In the last video, we talked about gradient descent for minimizing the cost function J of theta for logistic regression. In this video, I'd like to tell you about some advanced optimization algorithms and some advanced optimization concepts. Using some of these ideas, we'll be able to get logistic regression to run much more quickly than it's possible with gradient descent. And this will also let the algorithms scale much better to very large machine learning problems, such as if we had a very large number of features. Here's an alternative view of what gradient descent is doing. We have some cost function J and we want to minimize it. So what we need to is, we need to write code that can take as input the parameters theta and they can compute two things: J of theta and these partial derivative terms for, you know, J equals 0, 1 up to N.  Given code that can do these two things, what gradient descent does is it repeatedly performs the following update. Right? So given the code that we wrote to compute these partial derivatives, gradient descent plugs in here and uses that to update our parameters theta. So another way of thinking about gradient descent is that we need to supply code to compute J of theta and these derivatives, and then these get plugged into gradient descents, which can then try to minimize the function for us. For gradient descent, I guess technically you don't actually need code to compute the cost function J of theta. You only need code to compute the derivative terms. But if you think of your code as also monitoring convergence of some such, we'll just think of ourselves as providing code to compute both the cost function and the derivative terms. So, having written code to compute these two things, one algorithm we can use is gradient descent. But gradient descent isn't the only algorithm we can use. And there are other algorithms, more advanced, more sophisticated ones, that, if we only provide them a way to compute these two things, then these are different approaches to optimize the cost function for us. So conjugate gradient BFGS and L-BFGS are examples of more sophisticated optimization algorithms that need a way to compute J of theta, and need a way to compute the derivatives, and can then use more sophisticated strategies than gradient descent to minimize the cost function. The details of exactly what these three algorithms is well beyond the scope of this course. And in fact you often end up spending, you know, many days, or a small number of weeks studying these algorithms. If you take a class and advance the numerical computing. But let me just tell you about some of their properties. These three algorithms have a number of advantages. One is that, with any of this algorithms you usually do not need to manually pick the learning rate alpha. So one way to think of these algorithms is that given is the way to compute the derivative and a cost function. You can think of these algorithms as having a clever inter-loop. And, in fact, they have a clever inter-loop called a line search algorithm that automatically tries out different values for the learning rate alpha and automatically picks a good learning rate alpha so that it can even pick a different learning rate for every iteration. And so then you don't need to choose it yourself. These algorithms actually do more sophisticated things than just pick a good learning rate, and so they often end up converging much faster than gradient descent. These algorithms actually do more sophisticated things than just pick a good learning rate, and so they often end up converging much faster than gradient descent, but detailed discussion of exactly what they do is beyond the scope of this course. In fact, I actually used to have used these algorithms for a long time, like maybe over a decade, quite frequently, and it was only, you know, a few years ago that I actually figured out for myself the details of what conjugate gradient, BFGS and O-BFGS do. So it is actually entirely possible to use these algorithms successfully and apply to lots of different learning problems without actually understanding the inter-loop of what these algorithms do. If these algorithms have a disadvantage, I'd say that the main disadvantage is that they're quite a lot more complex than gradient descent. And in particular, you probably should not implement these algorithms - conjugate gradient, L-BGFS, BFGS - yourself unless you're an expert in numerical computing. Instead, just as I wouldn't recommend that you write your own code to compute square roots of numbers or to compute inverses of matrices, for these algorithms also what I would recommend you do is just use a software library. So, you know, to take a square root what all of us do is use some function that someone else has written to compute the square roots of our numbers. And fortunately, Octave and the closely related language MATLAB - we'll be using that - Octave has a very good. Has a pretty reasonable library implementing some of these advanced optimization algorithms. And so if you just use the built-in library, you know, you get pretty good results. I should say that there is a difference between good and bad implementations of these algorithms. And so, if you're using a different language for your machine learning application, if you're using C, C++, Java, and so on, you might want to try out a couple of different libraries to make sure that you find a good library for implementing these algorithms. Because there is a difference in performance between a good implementation of, you know, contour gradient or LPFGS versus less good implementation of contour gradient or LPFGS. So now let's explain how to use these algorithms, I'm going to do so with an example. Let's say that you have a problem with two parameters equals theta zero and theta one. And let's say your cost function is J of theta equals theta one minus five squared, plus theta two minus five squared. So with this cost function. You know the value for theta 1 and theta 2. If you want to minimize J of theta as a function of theta. The value that minimizes it is going to be theta 1 equals 5, theta 2 equals equals five. Now, again, I know some of you know more calculus than others, but the derivatives of the cost function J turn out to be these two expressions. I've done the calculus. So if you want to apply one of the advanced optimization algorithms to minimize cost function J. So, you know, if we didn't know the minimum was at 5, 5, but if you want to have a cost function 5 the minimum numerically using something like gradient descent but preferably more advanced than gradient descent, what you would do is implement an octave function like this, so we implement a cost function, cost function theta function like that, and what this does is that it returns two arguments, the first J-val, is how we would compute the cost function J. And so this says J-val equals, you know, theta one minus five squared plus theta two minus five squared. So it's just computing this cost function over here. And the second argument that this function returns is gradient. So gradient is going to be a two by one vector, and the two elements of the gradient vector correspond to the two partial derivative terms over here. Having implemented this cost function, you would, you can then call the advanced optimization function called the fminunc - it stands for function minimization unconstrained in Octave -and the way you call this is as follows. You set a few options. This is a options as a data structure that stores the options you want. So grant up on, this sets the gradient objective parameter to on. It just means you are indeed going to provide a gradient to this algorithm. I'm going to set the maximum number of iterations to, let's say, one hundred. We're going give it an initial guess for theta. There's a 2 by 1 vector. And then this command calls fminunc. This at symbol presents a pointer to the cost function that we just defined up there. And if you call this, this will compute, you know, will use one of the more advanced optimization algorithms. And if you want to think it as just like gradient descent. But automatically choosing the learning rate alpha for so you don't have to do so yourself. But it will then attempt to use the sort of advanced optimization algorithms. Like gradient descent on steroids. To try to find the optimal value of theta for you. Let me actually show you what this looks like in Octave. So I've written this cost function of theta function exactly as we had it on the previous line. It computes J-val which is the cost function. And it computes the gradient with the two elements being the partial derivatives of the cost function with respect to, you know, the two parameters, theta one and theta two. Now let's switch to my Octave window. I'm gonna type in those commands I had just now. So, options equals optimset. This is the notation for setting my parameters on my options, for my optimization algorithm. Grant option on, maxIter, 100 so that says 100 iterations, and I am going to provide the gradient to my algorithm. Let's say initial theta equals zero's two by one. So that's my initial guess for theta. And now I have of theta, function val exit flag equals fminunc constraint. A pointer to the cost function. and provide my initial guess. And the options like so. And if I hit enter this will run the optimization algorithm. And it returns pretty quickly. This funny formatting that's because my line, you know, my code wrapped around. So, this funny thing is just because my command line had wrapped around. But what this says is that numerically renders, you know, think of it as gradient descent on steroids, they found the optimal value of a theta is theta 1 equals 5, theta 2 equals 5, exactly as we're hoping for. The function value at the optimum is essentially 10 to the minus 30. So that's essentially zero, which is also what we're hoping for. And the exit flag is 1, and this shows what the convergence status of this. And if you want you can do help fminunc to read the documentation for how to interpret the exit flag. But the exit flag let's you verify whether or not this algorithm thing has converged. So that's how you run these algorithms in Octave. I should mention, by the way, that for the Octave implementation, this value of theta, your parameter vector of theta, must be in rd for d greater than or equal to 2. So if theta is just a real number. So, if it is not at least a two-dimensional vector or some higher than two-dimensional vector, this fminunc may not work, so and if in case you have a one-dimensional function that you use to optimize, you can look in the octave documentation for fminunc for additional details. So, that's how we optimize our trial example of this simple quick driving cost function. However, we apply this to let's just say progression. In logistic regression we have a parameter vector theta, and I'm going to use a mix of octave notation and sort of math notation. But I hope this explanation will be clear, but our parameter vector theta comprises these parameters theta 0 through theta n because octave indexes, vectors using indexing from 1, you know, theta 0 is actually written theta 1 in octave, theta 1 is gonna be written. So, if theta 2 in octave and that's gonna be a written theta n+1, right? And that's because Octave indexes is vectors starting from index of 1 and so the index of 0. So what we need to do then is write a cost function that captures the cost function for logistic regression. Concretely, the cost function needs to return J-val, which is, you know, J-val as you need some codes to compute J of theta and we also need to give it the gradient. So, gradient 1 is going to be some code to compute the partial derivative in respect to theta 0, the next partial derivative respect to theta 1 and so on. Once again, this is gradient 1, gradient 2 and so on, rather than gradient 0, gradient 1 because octave indexes is vectors starting from one rather than from zero. But the main concept I hope you take away from this slide is, that what you need to do, is write a function that returns the cost function and returns the gradient. And so in order to apply this to logistic regression or even to linear regression, if you want to use these optimization algorithms for linear regression. What you need to do is plug in the appropriate code to compute these things over here. So, now you know how to use these advanced optimization algorithms. Because, using, because for these algorithms, you're using a sophisticated optimization library, it makes the just a little bit more opaque and so just maybe a little bit harder to debug. But because these algorithms often run much faster than gradient descent, often quite typically whenever I have a large machine learning problem, I will use these algorithms instead of using gradient descent. And with these ideas, hopefully, you'll be able to get logistic progression and also linear regression to work on much larger problems. So, that's it for advanced optimization concepts. And in the next and final video on Logistic Regression, I want to tell you how to take the logistic regression algorithm that you already know about and make it work also on multi-class classification problems.""",38,0,1
coursera,stanford_university,machine-learning,multiclass-classification-one-vs-all,"b""In this video we'll talk about how to\nget logistic regression to work for multiclass classification problems. And in particular I want to tell you\nabout an algorithm called one-versus-all classification. What's a multiclass\nclassification problem? Here are some examples. Lets say you want a learning algorithm\nto automatically put your email into different folders or\nto automatically tag your emails so you might have different folders or\ndifferent tags for work email, email from your friends, email from your\nfamily, and emails about your hobby. And so here we have a classification\nproblem with four classes which we might assign to the classes y = 1,\ny =2, y =3, and y = 4 too. And another example,\nfor medical diagnosis, if a patient comes into your\noffice with maybe a stuffy nose, the possible diagnosis could\nbe that they're not ill. Maybe that's y = 1. Or they have a cold, 2. Or they have a flu. And a third and final example if you\nare using machine learning to classify the weather, you know maybe you want to\ndecide that the weather is sunny, cloudy, rainy, or snow, or if it's gonna be snow,\nand so in all of these examples, y can take on a small number of values,\nmaybe one to three, one to four and so on, and these are multiclass\nclassification problems. And by the way, it doesn't really\nmatter whether we index is at 0, 1, 2, 3, or as 1, 2, 3, 4. I tend to index my classes starting\nfrom 1 rather than starting from 0, but either way we're off and\nit really doesn't matter. Whereas previously for a binary classification problem,\nour data sets look like this. For a multi-class classification\nproblem our data sets may look like this where here I'm using three different\nsymbols to represent our three classes. So the question is given the data\nset with three classes where this is an example of one class, that's\nan example of a different class, and that's an example of yet a third class. How do we get a learning\nalgorithm to work for the setting? We already know how to do binary\nclassification using a regression. We know how to you know maybe fit a\nstraight line to set for the positive and negative classes. You see an idea called\none-vs-all classification. We can then take this and make it work for\nmulti-class classification as well. Here's how a one-vs-all\nclassification works. And this is also sometimes\ncalled one-vs-rest. Let's say we have a training set\nlike that shown on the left, where we have three classes of y equals 1,\nwe denote that with a triangle, if y equals 2, the square, and\nif y equals three, then the cross. What we're going to do is\ntake our training set and turn this into three separate\nbinary classification problems. I'll turn this into three separate\ntwo class classification problems. So let's start with class\none which is the triangle. We're gonna essentially create a new sort\nof fake training set where classes two and three get assigned to the negative class. And class one gets assigned\nto the positive class. You want to create a new training set\nlike that shown on the right, and we're going to fit a classifier which\nI'm going to call h subscript theta superscript one of x where here the triangles are the positive examples\nand the circles are the negative examples. So think of the triangles being\nassigned the value of one and the circles assigned the value of zero. And we're just going to train a standard\nlogistic regression classifier and maybe that will give us a position\nboundary that looks like that. Okay? This superscript one here stands for\nclass one, so we're doing this for the triangles of class one. Next we do the same thing for class two. Gonna take the squares and assign\nthe squares as the positive class, and assign everything else, the triangles and\nthe crosses, as a negative class. And then we fit a second logistic\nregression classifier and call this h of x superscript two, where the superscript\ntwo denotes that we're now doing this, treating the square class\nas the positive class. And maybe we get classified like that. And finally, we do the same thing for\nthe third class and fit a third classifier h\nsuper script three of x, and maybe this will give us a decision\nbounty of the visible cross fire. This separates the positive and\nnegative examples like that. So to summarize, what we've done is,\nwe've fit three classifiers. So, for i = 1, 2, 3, we'll fit a classifier x super\nscript i subscript theta of x. Thus trying to estimate what is the\nprobability that y is equal to class i, given x and parametrized by theta. Right?\nSo in the first instance for this first one up here, this classifier\nwas learning to recognize the triangles. So it's thinking of the triangles\nas a positive clause, so x superscript one is essentially trying\nto estimate what is the probability that the y is equal to one,\ngiven that x is parametrized by theta. And similarly, this is treating\nthe square class as a positive class and so it's trying to estimate\nthe probability that y = 2 and so on. So we now have three classifiers, each of which was trained to\nrecognize one of the three classes. Just to summarize,\nwhat we've done is we want to train a logistic regression\nclassifier h superscript i of x for each class i to predict\nthe probability that y is equal to i. Finally to make a prediction,\nwhen we're given a new input x, and we want to make a prediction. What we do is we just run all three of our classifiers on the input x and we then\npick the class i that maximizes the three. So we just basically pick the classifier, I think whichever one of the three\nclassifiers is most confident and so the most enthusiastically says that\nit thinks it has the right clause. So whichever value of i gives\nus the highest probability we then predict y to be that value. So that's it for multi-class\nclassification and one-vs-all method. And with this little method you can\nnow take the logistic regression classifier and make it work on multi-class\nclassification problems as well""",39,0,1
coursera,stanford_university,machine-learning,the-problem-of-overfitting,"b""By now, you've seen a couple different learning algorithms, linear regression and logistic regression. They work well for many problems, but when you apply them to certain machine learning applications, they can run into a problem called overfitting that can cause them to perform very poorly. What I'd like to do in this video is explain to you what is this overfitting problem, and in the next few videos after this, we'll talk about a technique called regularization, that will allow us to ameliorate or to reduce this overfitting problem and get these learning algorithms to maybe work much better. So what is overfitting? Let's keep using our running example of predicting housing prices with linear regression where we want to predict the price as a function of the size of the house. One thing we could do is fit a linear function to this data, and if we do that, maybe we get that sort of straight line fit to the data. But this isn't a very good model. Looking at the data, it seems pretty clear that as the size of the housing increases, the housing prices plateau, or kind of flattens out as we move to the right and so this algorithm does not fit the training and we call this problem underfitting, and another term for this is that this algorithm has high bias. Both of these roughly mean that it's just not even fitting the training data very well. The term is kind of a historical or technical one, but the idea is that if a fitting a straight line to the data, then, it's as if the algorithm has a very strong preconception, or a very strong bias that housing prices are going to vary linearly with their size and despite the data to the contrary. Despite the evidence of the contrary is preconceptions still are bias, still closes it to fit a straight line and this ends up being a poor fit to the data. Now, in the middle, we could fit a quadratic functions enter and, with this data set, we fit the quadratic function, maybe, we get that kind of curve and, that works pretty well. And, at the other extreme, would be if we were to fit, say a fourth other polynomial to the data. So, here we have five parameters, theta zero through theta four, and, with that, we can actually fill a curve that process through all five of our training examples. You might get a curve that looks like this. That, on the one hand, seems to do a very good job fitting the training set and, that is processed through all of my data, at least. But, this is still a very wiggly curve, right? So, it's going up and down all over the place, and, we don't actually think that's such a good model for predicting housing prices. So, this problem we call overfitting, and, another term for this is that this algorithm has high variance.. The term high variance is another historical or technical one. But, the intuition is that, if we're fitting such a high order polynomial, then, the hypothesis can fit, you know, it's almost as if it can fit almost any function and this face of possible hypothesis is just too large, it's too variable. And we don't have enough data to constrain it to give us a good hypothesis so that's called overfitting. And in the middle, there isn't really a name but I'm just going to write, you know, just right. Where a second degree polynomial, quadratic function seems to be just right for fitting this data. To recap a bit the problem of over fitting comes when if we have too many features, then to learn hypothesis may fit the training side very well. So, your cost function may actually be very close to zero or may be even zero exactly, but you may then end up with a curve like this that, you know tries too hard to fit the training set, so that it even fails to generalize to new examples and fails to predict prices on new examples as well, and here the term generalized refers to how well a hypothesis applies even to new examples. That is to data to houses that it has not seen in the training set. On this slide, we looked at over fitting for the case of linear regression. A similar thing can apply to logistic regression as well. Here is a logistic regression example with two features X1 and x2. One thing we could do, is fit logistic regression with just a simple hypothesis like this, where, as usual, G is my sigmoid function. And if you do that, you end up with a hypothesis, trying to use, maybe, just a straight line to separate the positive and the negative examples. And this doesn't look like a very good fit to the hypothesis. So, once again, this is an example of underfitting or of the hypothesis having high bias. In contrast, if you were to add to your features these quadratic terms, then, you could get a decision boundary that might look more like this. And, you know, that's a pretty good fit to the data. Probably, about as good as we could get, on this training set. And, finally, at the other extreme, if you were to fit a very high-order polynomial, if you were to generate lots of high-order polynomial terms of speeches, then, logistical regression may contort itself, may try really hard to find a decision boundary that fits your training data or go to great lengths to contort itself, to fit every single training example well. And, you know, if the features X1 and X2 offer predicting, maybe, the cancer to the, you know, cancer is a malignant, benign breast tumors. This doesn't, this really doesn't look like a very good hypothesis, for making predictions. And so, once again, this is an instance of overfitting and, of a hypothesis having high variance and not really, and, being unlikely to generalize well to new examples. Later, in this course, when we talk about debugging and diagnosing things that can go wrong with learning algorithms, we'll give you specific tools to recognize when overfitting and, also, when underfitting may be occurring. But, for now, lets talk about the problem of, if we think overfitting is occurring, what can we do to address it? In the previous examples, we had one or two dimensional data so, we could just plot the hypothesis and see what was going on and select the appropriate degree polynomial. So, earlier for the housing prices example, we could just plot the hypothesis and, you know, maybe see that it was fitting the sort of very wiggly function that goes all over the place to predict housing prices. And we could then use figures like these to select an appropriate degree polynomial. So plotting the hypothesis, could be one way to try to decide what degree polynomial to use. But that doesn't always work. And, in fact more often we may have learning problems that where we just have a lot of features. And there is not just a matter of selecting what degree polynomial. And, in fact, when we have so many features, it also becomes much harder to plot the data and it becomes much harder to visualize it, to decide what features to keep or not. So concretely, if we're trying predict housing prices sometimes we can just have a lot of different features. And all of these features seem, you know, maybe they seem kind of useful. But, if we have a lot of features, and, very little training data, then, over fitting can become a problem. In order to address over fitting, there are two main options for things that we can do. The first option is, to try to reduce the number of features. Concretely, one thing we could do is manually look through the list of features, and, use that to try to decide which are the more important features, and, therefore, which are the features we should keep, and, which are the features we should throw out. Later in this course, where also talk about model selection algorithms. Which are algorithms for automatically deciding which features to keep and, which features to throw out. This idea of reducing the number of features can work well, and, can reduce over fitting. And, when we talk about model selection, we'll go into this in much greater depth. But, the disadvantage is that, by throwing away some of the features, is also throwing away some of the information you have about the problem. For example, maybe, all of those features are actually useful for predicting the price of a house, so, maybe, we don't actually want to throw some of our information or throw some of our features away. The second option, which we'll talk about in the next few videos, is regularization. Here, we're going to keep all the features, but we're going to reduce the magnitude or the values of the parameters theta J. And, this method works well, we'll see, when we have a lot of features, each of which contributes a little bit to predicting the value of Y, like we saw in the housing price prediction example. Where we could have a lot of features, each of which are, you know, somewhat useful, so, maybe, we don't want to throw them away. So, this subscribes the idea of regularization at a very high level. And, I realize that, all of these details probably don't make sense to you yet. But, in the next video, we'll start to formulate exactly how to apply regularization and, exactly what regularization means. And, then we'll start to figure out, how to use this, to make how learning algorithms work well and avoid overfitting.""",40,0,1
coursera,stanford_university,machine-learning,cost-function,"b""In this video, I'd like to convey to you, the main intuitions behind how regularization works. And, we'll also write down the cost function that we'll use, when we were using regularization. With the hand drawn examples that we have on these slides, I think I'll be able to convey part of the intuition. But, an even better way to see for yourself, how regularization works, is if you implement it, and, see it work for yourself. And, if you do the appropriate exercises after this, you get the chance to self see regularization in action for yourself. So, here is the intuition. In the previous video, we saw that, if we were to fit a quadratic function to this data, it gives us a pretty good fit to the data. Whereas, if we were to fit an overly high order degree polynomial, we end up with a curve that may fit the training set very well, but, really not be a, but overfit the data poorly, and, not generalize well. Consider the following, suppose we were to penalize, and, make the parameters theta 3 and theta 4 really small. Here's what I mean, here is our optimization objective, or here is our optimization problem, where we minimize our usual squared error cause function. Let's say I take this objective and modify it and add to it, plus 1000 theta 3 squared, plus 1000 theta 4 squared. 1000 I am just writing down as some huge number. Now, if we were to minimize this function, the only way to make this new cost function small is if theta 3 and data 4 are small, right? Because otherwise, if you have a thousand times theta 3, this new cost functions gonna be big. So when we minimize this new function we are going to end up with theta 3 close to 0 and theta 4 close to 0, and as if we're getting rid of these two terms over there. And if we do that, well then, if theta 3 and theta 4 close to 0 then we are being left with a quadratic function, and, so, we end up with a fit to the data, that's, you know, quadratic function plus maybe, tiny contributions from small terms, theta 3, theta 4, that they may be very close to 0. And, so, we end up with essentially, a quadratic function, which is good. Because this is a much better hypothesis. In this particular example, we looked at the effect of penalizing two of the parameter values being large. More generally, here is the idea behind regularization. The idea is that, if we have small values for the parameters, then, having small values for the parameters, will somehow, will usually correspond to having a simpler hypothesis. So, for our last example, we penalize just theta 3 and theta 4 and when both of these were close to zero, we wound up with a much simpler hypothesis that was essentially a quadratic function. But more broadly, if we penalize all the parameters usually that, we can think of that, as trying to give us a simpler hypothesis as well because when, you know, these parameters are as close as you in this example, that gave us a quadratic function. But more generally, it is possible to show that having smaller values of the parameters corresponds to usually smoother functions as well for the simpler. And which are therefore, also, less prone to overfitting. I realize that the reasoning for why having all the parameters be small. Why that corresponds to a simpler hypothesis; I realize that reasoning may not be entirely clear to you right now. And it is kind of hard to explain unless you implement yourself and see it for yourself. But I hope that the example of having theta 3 and theta 4 be small and how that gave us a simpler hypothesis, I hope that helps explain why, at least give some intuition as to why this might be true. Lets look at the specific example. For housing price prediction we may have our hundred features that we talked about where may be x1 is the size, x2 is the number of bedrooms, x3 is the number of floors and so on. And we may we may have a hundred features. And unlike the polynomial example, we don't know, right, we don't know that theta 3, theta 4, are the high order polynomial terms. So, if we have just a bag, if we have just a set of a hundred features, it's hard to pick in advance which are the ones that are less likely to be relevant. So we have a hundred or a hundred one parameters. And we don't know which ones to pick, we don't know which parameters to try to pick, to try to shrink. So, in regularization, what we're going to do, is take our cost function, here's my cost function for linear regression. And what I'm going to do is, modify this cost function to shrink all of my parameters, because, you know, I don't know which one or two to try to shrink. So I am going to modify my cost function to add a term at the end. Like so we have square brackets here as well. When I add an extra regularization term at the end to shrink every single parameter and so\nthis term we tend to shrink all of my parameters theta 1, theta 2, theta 3 up to theta 100. By the way, by convention the summation here starts from one so I am not actually going penalize theta zero being large. That sort of the convention that, the sum I equals one through N, rather than I equals zero through N. But in practice, it makes very little difference, and, whether you include, you know, theta zero or not, in practice, make very little difference to the results. But by convention, usually, we regularize only theta  through theta 100. Writing down our regularized optimization objective, our regularized cost function again. Here it is. Here's J of theta where, this term on the right is a regularization term and lambda here is called the regularization parameter and what lambda does, is it controls a trade off between two different goals. The first goal, capture it by the first goal objective, is that we would like to train, is that we would like to fit the training data well. We would like to fit the training set well. And the second goal is, we want to keep the parameters small, and that's captured by the second term, by the regularization objective. And by the regularization term. And what lambda, the regularization parameter does is the controls the trade of between these two goals, between the goal of fitting the training set well and the goal of keeping the parameter plan small and therefore keeping the hypothesis relatively simple to avoid overfitting. For our housing price prediction example, whereas, previously, if we had fit a very high order polynomial, we may have wound up with a very, sort of wiggly or curvy function like this. If you still fit a high order polynomial with all the polynomial features in there, but instead, you just make sure, to use this sole of regularized objective, then what you can get out is in fact a curve that isn't quite a quadratic function, but is much smoother and much simpler and maybe a curve like the magenta line that, you know, gives a much better hypothesis for this data. Once again, I realize it can be a bit difficult to see why strengthening the parameters can have this effect, but if you implement yourselves with regularization you will be able to see this effect firsthand. In regularized linear regression, if the regularization parameter monitor is set to be very large, then what will happen is we will end up penalizing the parameters theta 1, theta 2, theta 3, theta 4 very highly. That is, if our hypothesis is this is one down at the bottom. And if we end up penalizing theta 1, theta 2, theta 3, theta 4 very heavily, then we end up with all of these parameters close to zero, right? Theta 1 will be close to zero; theta 2 will be close to zero. Theta three and theta four will end up being close to zero. And if we do that, it's as if we're getting rid of these terms in the hypothesis so that we're just left with a hypothesis that will say that. It says that, well, housing prices are equal to theta zero, and that is akin to fitting a flat horizontal straight line to the data. And this is an example of underfitting, and in particular this hypothesis, this straight line it just fails to fit the training set well. It's just a fat straight line, it doesn't go, you know, go near. It doesn't go anywhere near most of the training examples. And another way of saying this is that this hypothesis has too strong a preconception or too high bias that housing prices are just equal to theta zero, and despite the clear data to the contrary, you know chooses to fit a sort of, flat line, just a flat horizontal line. I didn't draw that very well. This just a horizontal flat line to the data. So for regularization to work well, some care should be taken, to choose a good choice for the regularization parameter lambda as well. And when we talk about multi-selection later in this course, we'll talk about a way, a variety of ways for automatically choosing the regularization parameter lambda as well. So, that's the idea of the high regularization and the cost function reviews in order to use regularization In the next two videos, lets take these ideas and apply them to linear regression and to logistic regression, so that we can then get them to avoid overfitting.""",41,0,1
coursera,stanford_university,machine-learning,regularized-linear-regression,"b""For linear regression, we have previously\nworked out two learning algorithms. One based on gradient descent and\none based on the normal equation. In this video,\nwe'll take those two algorithms and generalize them to the case of\nregularized linear regression. Here's the optimization objective\nthat we came up with last time for regularized linear regression. This first part is our usual objective for\nlinear regression. And we now have this additional\nregularization term, where lambda is our regularization parameter, and\nwe like to find parameters theta that minimizes this cost function, this\nregularized cost function, J of theta. Previously, we were using\ngradient descent for the original cost function\nwithout the regularization term. And we had the following algorithm,\nfor regular linear regression, without regularization, we would repeatedly update\nthe parameters theta J as follows for J equals 0, 1, 2, up through n. Let me take this and just write\nthe case for theta 0 separately. So I'm just going to write the update for\ntheta 0 separately than for the update for the parameters 1,\n2, 3, and so on up to n. And so this is,\nI haven't changed anything yet, right. This is just writing the update for theta\n0 separately from the updates for theta 1, theta 2, theta 3, up to theta n. And the reason I want to do this\nis you may remember that for our regularized linear regression, we\npenalize the parameters theta 1, theta 2, and so on up to theta n. But we don't penalize theta 0. So, when we modify this algorithm for\nregularized linear regression, we're going to end up treating\ntheta zero slightly differently. Concretely, if we want to\ntake this algorithm and modify it to use\nthe regular rise objective, all we need to do is take this term at\nthe bottom and modify it as follows. We'll take this term and add minus lambda over m times theta j. And if you implement this,\nthen you have gradient descent for trying to minimize the regularized\ncost function, j of theta. And concretely, I'm not gonna\ndo the calculus to prove it, but concretely if you look at this term, this\nterm hat I've written in square brackets, if you know calculus it's\npossible to prove that that term is the partial derivative\nwith respect to J of theta using the new definition of J of\ntheta with the regularization term. And similarly, this term up on top\nwhich I'm drawing the cyan box, that's still the partial derivative\nrespect of theta zero of J of theta. If you look at the update for theta j, it's possible to show\nsomething very interesting. Concretely, theta j gets updated\nas theta j minus alpha times, and then you have this other term\nhere that depends on theta J. So if you group all the terms\ntogether that depend on theta j, you can show that this update can\nbe written equivalently as follows. And all I did was add theta j here is,\nso theta j times 1. And this term is, right, lambda over m,\nthere's also an alpha here, so you end up with alpha lambda\nover m multiplied into theta j. And this term here, 1 minus alpha times\nlambda m, is a pretty interesting term. It has a pretty interesting effect. Concretely this term,\n1 minus alpha times lambda over m, is going to be a number that is\nusually a little bit less than one, because alpha times lambda over m is\ngoing to be positive, and usually if your learning rate is small and if m\nis large, this is usually pretty small. So this term here is gonna be a number\nthat's usually a little bit less than 1, so think of it as a number like 0.99,\nlet's say. And so\nthe effect of our update to theta j is, we're going to say that theta j gets\nreplaced by theta j times 0.99, right? So theta j times 0.99 has the effect of shrinking theta j a little\nbit towards zero. So this makes theta j a bit smaller. And more formally, this makes the square\nnorm of theta j a little bit smaller. And then after that,\nthe second term here, that's actually exactly the same as the original\ngradient descent update that we had, before we added all this\nregularization stuff. So, hopefully this gradient descent,\nhopefully this update makes sense. When we're using a regularized\nlinear regression and what we're doing is on every iteration\nwe're multiplying theta j by a number that's a little bit less then one, so its\nshrinking the parameter a little bit, and then we're performing\na similar update as before. Of course that's just the intuition behind\nwhat this particular update is doing. Mathematically what it's doing\nis it's exactly gradient descent on the cost function J of theta that we\ndefined on the previous slide that uses the regularization term. Gradient descent was just one\nof our two algorithms for fitting a linear regression model. The second algorithm was the one\nbased on the normal equation, where what we did was we created\nthe design matrix X where each row corresponded to\na separate training example. And we created a vector y, so this is\na vector, that's an m dimensional vector. And that contained the labels\nfrom my training set. So whereas X is an m by (n+1) dimensional\nmatrix, y is an m dimensional vector. And in order to minimize the cost\nfunction J, we found that one way to do so\nis to set theta to be equal to this. Right, you have X transpose X,\ninverse, X transpose Y. I'm leaving room here to\nfill in stuff of course. And what this value for theta does is this minimizes the cost function J of theta,\nwhen we were not using regularization. Now that we are using regularization,\nif you were to derive what the minimum is, and just to give you a sense of how to\nderive the minimum, the way you derive it is you take partial derivatives\nwith respect to each parameter. Set this to zero, and\nthen do a bunch of math and you can then show that it's a formula like\nthis that minimizes the cost function. And concretely,\nif you are using regularization, then this formula changes as follows. Inside this parenthesis,\nyou end up with a matrix like this. 0, 1, 1, 1, and so on,\n1, until the bottom. So this thing over here is a matrix\nwhose upper left-most entry is 0. There are ones on the diagonals, and\nthen zeros everywhere else in this matrix. Because I'm drawing this rather sloppily. But as a example, if n = 2, then this matrix is going to\nbe a three by three matrix. More generally, this matrix is\nan (n+1) by (n+1) dimensional matrix. So if n = 2, then that matrix becomes\nsomething that looks like this. It would be 0, and\nthen 1s on the diagonals, and then 0s on the rest of the diagonals. And once again, I'm not going to show this\nderivation, which is frankly somewhat long and involved, but it is possible to prove\nthat if you are using the new definition of J of theta, with the regularization\nobjective, then this new formula for theta is the one that we give you,\nthe global minimum of J of theta. So finally I want to just quickly\ndescribe the issue of non-invertibility. This is relatively advanced material,\nso you should consider this as optional. And feel free to skip it,\nor if you listen to it and positive it doesn't really make sense,\ndon't worry about it either. But earlier when I talked about\nthe normal equation method, we also had an optional video\non the non-invertibility issue. So this is another optional part to this, sort of an add-on to that earlier\noptional video on non-invertibility. Now, consider a setting where m,\nthe number of examples, is less than or equal to n, the number of features. If you have fewer examples than features,\nthan this matrix, X transpose X will be non-invertible,\nor singular. Or the other term for\nthis is the matrix will be degenerate. And if you implement this in\nOctave anyway and you use the pinv function to take the pseudo inverse,\nit will kind of do the right thing, but it's not clear that it would give you\na very good hypothesis, even though numerically the Octave pinv function will\ngive you a result that kinda makes sense. But if you were doing this\nin a different language, and if you were taking just the regular\ninverse, which in Octave denoted with the function inv, we're trying to take\nthe regular inverse of X transpose X. Then in this setting,\nyou find that X transpose X is singular, is non-invertible, and if you're doing\nthis in different program language and using some linear algebra library to\ntry to take the inverse of this matrix, it just might not work because that\nmatrix is non-invertible or singular. Fortunately, regularization\nalso takes care of this for us. And concretely, so long as\nthe regularization parameter lambda is strictly greater than 0, it is actually\npossible to prove that this matrix, X transpose X plus lambda times this funny\nmatrix here, it is possible to prove that this matrix will not be singular and\nthat this matrix will be invertible. So using regularization also takes\ncare of any non-invertibility issues of the X transpose\nX matrix as well. So you now know how to implement\nregularized linear regression. Using this you'll be\nable to avoid overfitting even if you have lots of features\nin a relatively small training set. And this should let you get linear\nregression to work much better for many problems. In the next video we'll take\nthis regularization idea and apply it to logistic regression. So that you'd be able to get logistic\nregression to avoid overfitting and perform much better as well.""",42,0,1
coursera,stanford_university,machine-learning,regularized-logistic-regression,"b""For logistic regression, we previously talked about two types of optimization algorithms. We talked about how to use gradient descent to optimize as cost function J of theta. And we also talked about advanced optimization methods. Ones that require that you provide a way to compute your cost function J of theta and that you provide a way to compute the derivatives. In this video, we'll show how you can adapt both of those techniques, both gradient descent and the more advanced optimization techniques in order to have them work for regularized logistic regression. So, here's the idea. We saw earlier that Logistic Regression can also be prone to overfitting if you fit it with a very, sort of, high order polynomial features like this. Where G is the sigmoid function and in particular you end up with a hypothesis, you know, whose decision bound to be just sort of an overly complex and extremely contortive function that really isn't such a great hypothesis for this training set, and more generally if you have logistic regression with a lot of features. Not necessarily polynomial ones, but just with a lot of features you can end up with overfitting. This was our cost function for logistic regression. And if we want to modify it to use regularization, all we need to do is add to it the following term plus londer over 2M, sum from J equals 1, and as usual sum from J equals 1. Rather than the sum from J equals 0, of theta J squared. And this has to effect therefore, of penalizing the parameters theta 1 theta 2 and so on up to theta N from being too large. And if you do this, then it will the have the effect that even though you're fitting a very high order polynomial with a lot of parameters. So long as you apply regularization and keep the parameters small you're more likely to get a decision boundary. You know, that maybe looks more like this. It looks more reasonable for separating the positive and the negative examples. So, when using regularization even when you have a lot of features, the regularization can help take care of the overfitting problem. How do we actually implement this? Well, for the original gradient descent algorithm, this was the update we had. We will repeatedly perform the following update to theta J. This slide looks a lot like the previous one for linear regression. But what I'm going to do is write the update for theta 0 separately. So, the first line is for update for theta 0 and a second line is now my update for theta 1 up to theta N. Because I'm going to treat theta 0 separately. And in order to modify this algorithm, to use a regularized cos function, all I need to do is pretty similar to what we did for linear regression is actually to just modify this second update rule as follows. And, once again, this, you know, cosmetically looks identical what we had for linear regression. But of course is not the same algorithm as we had, because now the hypothesis is defined using this. So this is not the same algorithm as regularized linear regression. Because the hypothesis is different. Even though this update that I wrote down. It actually looks cosmetically the same as what we had earlier. We're working out gradient descent for regularized linear regression. And of course, just to wrap up this discussion, this term here in the square brackets, so this term here, this term is, of course, the new partial derivative for respect of theta J of the new cost function J of theta. Where J of theta here is the cost function we defined on a previous slide that does use regularization. So, that's gradient descent for regularized linear regression. Let's talk about how to get regularized linear regression to work using the more advanced optimization methods. And just to remind you for those methods what we needed to do was to define the function that's called the cost function, that takes us input the parameter vector theta and once again in the equations we've been writing here we used 0 index vectors. So we had theta 0 up to theta N. But because Octave indexes the vectors starting from 1. Theta 0 is written in Octave as theta 1. Theta 1 is written in Octave as theta 2, and so on down to theta N plus 1. And what we needed to do was provide a function. Let's provide a function called cost function that we would then pass in to what we have, what we saw earlier. We will use the fminunc and then you know at cost function, and so on, right. But the F min, u and c was the F min unconstrained and this will work with fminunc was what will take the cost function and minimize it for us. So the two main things that the cost function needed to return were first J-val. And for that, we need to write code to compute the cost function J of theta. Now, when we're using regularized logistic regression, of course the cost function j of theta changes and, in particular, now a cost function needs to include this additional regularization term at the end as well. So, when you compute j of theta be sure to include that term at the end. And then, the other thing that this cost function thing needs to derive with a gradient. So gradient one needs to be set to the partial derivative of J of theta with respect to theta zero, gradient two needs to be set to that, and so on. Once again, the index is off by one. Right, because of the indexing from one Octave users. And looking at these terms. This term over here. We actually worked this out on a previous slide is actually equal to this. It doesn't change. Because the derivative for theta zero doesn't change. Compared to the version without regularization. And the other terms do change. And in particular the derivative respect to theta one. We worked this out on the previous slide as well. Is equal to, you know, the original term and then minus londer M times theta 1. Just so we make sure we pass this correctly. And we can add parentheses here. Right, so the summation doesn't extend. And similarly, you know, this other term here looks like this, with this additional term that we had on the previous slide, that corresponds to the gradient from their regularization objective. So if you implement this cost function and pass this into fminunc or to one of those advanced optimization techniques, that will minimize the new regularized cost function J of theta. And the parameters you get out will be the ones that correspond to logistic regression with regularization. So, now you know how to implement regularized logistic regression. When I walk around Silicon Valley, I live here in Silicon Valley, there are a lot of engineers that are frankly, making a ton of money for their companies using machine learning algorithms. And I know we've only been, you know, studying this stuff for a little while. But if you understand linear regression, the advanced optimization algorithms and regularization, by now, frankly, you probably know quite a lot more machine learning than many, certainly now, but you probably know quite a lot more machine learning right now than frankly, many of the Silicon Valley engineers out there having very successful careers. You know, making tons of money for the companies. Or building products using machine learning algorithms. So, congratulations. You've actually come a long ways. And you can actually, you actually know enough to apply this stuff and get to work for many problems. So congratulations for that. But of course, there's still a lot more that we want to teach you, and in the next set of videos after this, we'll start to talk about a very powerful cause of non-linear classifier. So whereas linear regression, logistic regression, you know, you can form polynomial terms, but it turns out that there are much more powerful nonlinear quantifiers that can then sort of polynomial regression. And in the next set of videos after this one, I'll start telling you about them. So that you have even more powerful learning algorithms than you have now to apply to different problems.""",43,0,1
coursera,stanford_university,machine-learning,non-linear-hypotheses,"b'In this and in the next set of videos, I\'d like to tell you about a learning algorithm called a Neural Network. We\'re going to first talk about the representation and then in the next set of videos talk about learning algorithms for it. Neutral networks is actually a pretty old idea, but had fallen out of favor for a while. But today, it is the state of the art technique for many different machine learning problems. So why do we need yet another learning algorithm? We already have linear regression and we have logistic regression, so why do we need, you know, neural networks? In order to motivate the discussion of neural networks, let me start by showing you a few examples of machine learning problems where we need to learn complex non-linear hypotheses. Consider a supervised learning classification problem where you have a training set like this. If you want to apply logistic regression to this problem, one thing you could do is apply logistic regression with a lot of nonlinear features like that. So here, g as usual is the sigmoid function, and we can include lots of polynomial terms like these. And, if you include enough polynomial terms then, you know, maybe you can get a hypotheses that separates the positive and negative examples. This particular method works well when you have only, say, two features - x1 and x2 - because you can then include all those polynomial terms of x1 and x2. But for many interesting machine learning problems would have a lot more features than just two. We\'ve been talking for a while about housing prediction, and suppose you have a housing classification problem rather than a regression problem, like maybe if you have different features of a house, and you want to predict what are the odds that your house will be sold within the next six months, so that will be a classification problem. And as we saw we can come up with quite a lot of features, maybe a hundred different features of different houses. For a problem like this, if you were to include all the quadratic terms, all of these, even all of the quadratic that is the second or the polynomial terms, there would be a lot of them. There would be terms like x1 squared, x1x2, x1x3, you know, x1x4 up to x1x100 and then you have x2 squared, x2x3 and so on. And if you include just the second order terms, that is, the terms that are a product of, you know, two of these terms, x1 times x1 and so on, then, for the case of n equals 100, you end up with about five thousand features. And, asymptotically, the number of quadratic features grows roughly as order n squared, where n is the number of the original features, like x1 through x100 that we had. And its actually closer to n squared over two. So including all the quadratic features doesn\'t seem like it\'s maybe a good idea, because that is a lot of features and you might up overfitting the training set, and it can also be computationally expensive, you know, to be working with that many features. One thing you could do is include only a subset of these, so if you include only the features x1 squared, x2 squared, x3 squared, up to maybe x100 squared, then the number of features is much smaller. Here you have only 100 such quadratic features, but this is not enough features and certainly won\'t let you fit the data set like that on the upper left. In fact, if you include only these quadratic features together with the original x1, and so on, up to x100 features, then you can actually fit very interesting hypotheses. So, you can fit things like, you know, access a line of the ellipses like these, but you certainly cannot fit a more complex data set like that shown here. So 5000 features seems like a lot, if you were to include the cubic, or third order known of each others, the x1, x2, x3. You know, x1 squared, x2, x10 and x11, x17 and so on. You can imagine there are gonna be a lot of these features. In fact, they are going to be order and cube such features and if any is 100 you can compute that, you end up with on the order of about 170,000 such cubic features and so including these higher auto-polynomial features when your original feature set end is large this really dramatically blows up your feature space and this doesn\'t seem like a good way to come up with additional features with which to build none many classifiers when n is large. For many machine learning problems, n will be pretty large. Here\'s an example. Let\'s consider the problem of computer vision. And suppose you want to use machine learning to train a classifier to examine an image and tell us whether or not the image is a car. Many people wonder why computer vision could be difficult. I mean when you and I look at this picture it is so obvious what this is. You wonder how is it that a learning algorithm could possibly fail to know what this picture is. To understand why computer vision is hard let\'s zoom into a small part of the image like that area where the little red rectangle is. It turns out that where you and I see a car, the computer sees that. What it sees is this matrix, or this grid, of pixel intensity values that tells us the brightness of each pixel in the image. So the computer vision problem is to look at this matrix of pixel intensity values, and tell us that these numbers represent the door handle of a car. Concretely, when we use machine learning to build a car detector, what we do is we come up with a label training set, with, let\'s say, a few label examples of cars and a few label examples of things that are not cars, then we give our training set to the learning algorithm trained a classifier and then, you know, we may test it and show the new image and ask, ""What is this new thing?"". And hopefully it will recognize that that is a car. To understand why we need nonlinear hypotheses, let\'s take a look at some of the images of cars and maybe non-cars that we might feed to our learning algorithm. Let\'s pick a couple of pixel locations in our images, so that\'s pixel one location and pixel two location, and let\'s plot this car, you know, at the location, at a certain point, depending on the intensities of pixel one and pixel two. And let\'s do this with a few other images. So let\'s take a different example of the car and you know, look at the same two pixel locations and that image has a different intensity for pixel one and a different intensity for pixel two. So, it ends up at a different location on the figure. And then let\'s plot some negative examples as well. That\'s a non-car, that\'s a non-car  . And if we do this for more and more examples using the pluses to denote cars and minuses to denote non-cars, what we\'ll find is that the cars and non-cars end up lying in different regions of the space, and what we need therefore is some sort of non-linear hypotheses to try to separate out the two classes. What is the dimension of the feature space? Suppose we were to use just 50 by 50 pixel images. Now that suppose our images were pretty small ones, just 50 pixels on the side. Then we would have 2500 pixels, and so the dimension of our feature size will be N equals 2500 where our feature vector x is a list of all the pixel testings, you know, the pixel brightness of pixel one, the brightness of pixel two, and so on down to the pixel brightness of the last pixel where, you know, in a typical computer representation, each of these may be values between say 0 to 255 if it gives us the grayscale value. So we have n equals 2500, and that\'s if we were using grayscale images. If we were using RGB images with separate red, green and blue values, we would have n equals 7500. So, if we were to try to learn a nonlinear hypothesis by including all the quadratic features, that is all the terms of the form, you know, Xi times Xj, while with the 2500 pixels we would end up with a total of three million features. And that\'s just too large to be reasonable; the computation would be very expensive to find and to represent all of these three million features per training example. So, simple logistic regression together with adding in maybe the quadratic or the cubic features - that\'s just not a good way to learn complex nonlinear hypotheses when n is large because you just end up with too many features. In the next few videos, I would like to tell you about Neural Networks, which turns out to be a much better way to learn complex hypotheses, complex nonlinear hypotheses even when your input feature space, even when n is large. And along the way I\'ll also get to show you a couple of fun videos of historically important applications of Neural networks as well that I hope those videos that we\'ll see later will be fun for you to watch as well.'",44,0,1
coursera,stanford_university,machine-learning,neurons-and-the-brain,"b""Neural Networks are a pretty old algorithm that was originally motivated by the goal of having machines that can mimic the brain. Now in this class, of course I'm teaching Neural Networks to you because they work really well for different machine learning problems and not, certainly not because they're logically motivated. In this video, I'd like to give you some of the background on Neural Networks. So that we can get a sense of what we can expect them to do. Both in the sense of applying them to modern day machinery problems, as well as for those of you that might be interested in maybe the big AI dream of someday building truly intelligent machines. Also, how Neural Networks might pertain to that. The origins of Neural Networks was as algorithms that try to mimic the brain and those a sense that if we want to build learning systems while why not mimic perhaps the most amazing learning machine we know about, which is perhaps the brain. Neural Networks came to be very widely used throughout the 1980's and 1990's and for various reasons as popularity diminished in the late 90's. But more recently, Neural Networks  have had a major recent resurgence. One of the reasons for this resurgence is that Neural Networks are computationally some what more expensive algorithm and so, it was only, you know, maybe somewhat more recently that computers became fast enough to really run large scale Neural Networks and because of that as well as a few other technical reasons which we'll talk about later, modern Neural Networks today are the state of the art technique for many applications. So, when you think about mimicking the brain while one of the human brain does tell me same things, right? The brain can learn to see process images than to hear, learn to process our sense of touch. We can, you know, learn to do math, learn to do calculus, and the brain does so many different and amazing things. It seems like if you want to mimic the brain it seems like you have to write lots of different pieces of software to mimic all of these different fascinating, amazing things that the brain tell us, but does is this fascinating hypothesis that the way the brain does all of these different things is not worth like a thousand different programs, but instead, the way the brain does it is worth just a single learning algorithm. This is just a hypothesis but let me share with you some of the evidence for this. This part of the brain, that little red part of the brain, is your auditory cortex and the way you're understanding my voice now is your ear is taking the sound signal and routing the sound signal to your auditory cortex and that's what's allowing you to understand my words. Neuroscientists have done the following fascinating experiments where you cut the wire from the ears to the auditory cortex and you re-wire, in this case an animal's brain, so that the signal from the eyes to the optic nerve eventually gets routed to the auditory cortex. If you do this it turns out, the auditory cortex will learn to see. And this is in every single sense of the word see as we know it. So, if you do this to the animals, the animals can perform visual discrimination task and as they can look at images and make appropriate decisions based on the images and they're doing it with that piece of brain tissue. Here's another example. That red piece of brain tissue is your somatosensory cortex. That's how you process your sense of touch. If you do a similar re-wiring process then the somatosensory cortex will learn to see. Because of this and other similar experiments, these are called neuro-rewiring experiments. There's this sense that if the same piece of physical brain tissue can process sight or sound or touch then maybe there is one learning algorithm that can process sight or sound or touch. And instead of needing to implement a thousand different programs or a thousand different algorithms to do, you know, the thousand wonderful things that the brain does, maybe what we need to do is figure out some approximation or to whatever the brain's learning algorithm is and implement that and that the brain learned by itself how to process these different types of data. To a surprisingly large extent, it seems as if we can plug in almost any sensor to almost any part of the brain and so, within the reason, the brain will learn to deal with it. Here are a few more examples. On the upper left is an example of learning to see with your tongue. The way it works is--this is actually a system called BrainPort undergoing FDA trials now to help blind people see--but the way it works is, you strap a grayscale camera to your forehead, facing forward, that takes the low resolution grayscale image of what's in front of you and you then run a wire to an array of electrodes that you place on your tongue so that each pixel gets mapped to a location on your tongue where maybe a high voltage corresponds to a dark pixel and a low voltage corresponds to a bright pixel and, even as it does today, with this sort of system you and I will be able to learn to see, you know, in tens of minutes with our tongues. Here's a second example of human echo location or human sonar. So there are two ways you can do this. You can either snap your fingers, or click your tongue. I can't do that very well. But there are blind people today that are actually being trained in schools to do this and learn to interpret the pattern of sounds bouncing off your environment - that's sonar. So, if after you search on YouTube, there are actually videos of this amazing kid who tragically because of cancer had his eyeballs removed, so this is a kid with no eyeballs. But by snapping his fingers, he can walk around and never hit anything. He can ride a skateboard. He can shoot a basketball into a hoop and this is a kid with no eyeballs. Third example is the Haptic Belt where if you have a strap around your waist, ring up buzzers and always have the northmost one buzzing. You can give a human a direction sense similar to maybe how birds can, you know, sense where north is. And, some of the bizarre example, but if you plug a third eye into a frog, the frog will learn to use that eye as well. So, it's pretty amazing to what extent is as if you can plug in almost any sensor to the brain and the brain's learning algorithm will just figure out how to learn from that data and deal with that data. And there's a sense that if we can figure out what the brain's learning algorithm is, and, you know, implement it or implement some approximation to that algorithm on a computer, maybe that would be our best shot at, you know, making real progress towards the AI, the artificial intelligence dream of someday building truly intelligent machines. Now, of course, I'm not teaching Neural Networks, you know, just because they might give us a window into this far-off AI dream, even though I'm personally, that's one of the things that I personally work on in my research life. But the main reason I'm teaching Neural Networks in this class is because it's actually a very effective state of the art technique for modern day machine learning applications. So, in the next few videos, we'll start diving into the technical details of Neural Networks so that you can apply them to modern-day machine learning applications and get them to work well on problems. But for me, you know, one of the reasons the excite me  is that maybe they give us this window into what we might do if we're also thinking of what algorithms might someday be able to learn in a manner similar to humankind.""",45,0,1
coursera,stanford_university,machine-learning,model-representation-i,"b""In this video, I want to start telling you\nabout how we represent neural networks. In other words,\nhow we represent our hypothesis or how we represent our model\nwhen using neural networks. Neural networks were developed\nas simulating neurons or networks of neurons in the brain. So, to explain the hypothesis\nrepresentation let's start by looking at what a single\nneuron in the brain looks like. Your brain and mine is jam packed full of neurons like\nthese and neurons are cells in the brain. And two things to draw\nattention to are that first. The neuron has a cell body,\nlike so, and moreover, the neuron has a number of input wires,\nand these are called the dendrites. You think of them as input wires, and\nthese receive inputs from other locations. And a neuron also has an output\nwire called an Axon, and this output wire is what it uses\nto send signals to other neurons, so to send messages to other neurons. So, at a simplistic level what a neuron\nis, is a computational unit that gets a number of inputs through it input\nwires and does some computation and then it says outputs via its axon to other\nnodes or to other neurons in the brain. Here's a illustration\nof a group of neurons. The way that neurons communicate with\neach other is with little pulses of electricity, they are also called spikes\nbut that just means pulses of electricity. So here is one neuron and what it does\nis if it wants a send a message what it does is sends a little\npulse of electricity. Varis axon to some different neuron and\nhere, this axon that is this open wire, connects to the dendrites of\nthis second neuron over here, which then accepts this incoming\nmessage that some computation. And they, in turn, decide to send out this\nmessage on this axon to other neurons, and this is the process by which\nall human thought happens. It's these Neurons doing computations and passing messages to other neurons as\na result of what other inputs they've got. And, by the way, this is how our\nsenses and our muscles work as well. If you want to move one of your muscles\nthe way that where else in your neuron may send this electricity to your muscle and\nthat causes your muscles to contract and your eyes, some senses like your eye must\nsend a message to your brain while it does it senses hosts electricity entity\nto a neuron in your brain like so. In a neuro network, or rather,\nin an artificial neuron network that we've implemented on the computer,\nwe're going to use a very simple model of what a neuron does we're going to model\na neuron as just a logistic unit. So, when I draw a yellow circle like that,\nyou should think of that as a playing a role analysis,\nwho's maybe the body of a neuron, and we then feed the neuron a few inputs\nwho's various dendrites or input wiles. And the neuron does some computation. And output some value on this output wire,\nor in the biological neuron, this is an axon. And whenever I draw a diagram like this, what this means is that this\nrepresents a computation of h of x equals one over one plus e to\nthe negative theta transpose x, where as usual, x and\ntheta are our parameter vectors, like so. So this is a very simple,\nmaybe a vastly oversimplified model, of the computations that the neuron does,\nwhere it gets a number of inputs, x1, x2, x3 and\nit outputs some value computed like so. When I draw a neural network, usually I\ndraw only the input nodes x1, x2, x3. Sometimes when it's useful to do so,\nI'll draw an extra node for x0. This x0 now that's sometimes called\nthe bias unit or the bias neuron, but because x0 is already equal to 1,\nsometimes, I draw this, sometimes I won't just depending on whatever is more\nnotationally convenient for that example. Finally, one last bit of terminology\nwhen we talk about neural networks, sometimes we'll say that\nthis is a neuron or an artificial neuron with a Sigmoid or\nlogistic activation function. So this activation function in\nthe neural network terminology. This is just another term for\nthat function for that non-linearity g(z)\n= 1 over 1+e to the -z. And whereas so far I've been calling\ntheta the parameters of the model, I'll mostly continue to\nuse that terminology. Here, it's a copy to the parameters, but\nin neural networks, in the neural network literature sometimes you might hear\npeople talk about weights of a model and weights just means exactly the same\nthing as parameters of a model. But I'll mostly continue to use the\nterminology parameters in these videos, but sometimes, you might hear\nothers use the weights terminology. So, this little diagram\nrepresents a single neuron. What a neural network is, is just a group\nof this different neurons strong together. Completely, here we have input units x1,\nx2, x3 and once again, sometimes you can draw this extra note x0\nand Sometimes not, just flow that in here. And here we have three neurons\nwhich have written 81, 82, 83. I'll talk about those indices later. And once again we can if\nwe want add in just a0 and add the mixture bias unit there. There's always a value of 1. And then finally we have this third\nnode and the final layer, and there's this third node that outputs the\nvalue that the hypothesis h(x) computes. To introduce a bit more terminology,\nin a neural network, the first layer, this is also called\nthe input layer because this is where we Input our features, x1, x2, x3. The final layer is also called the output\nlayer because that layer has a neuron, this one over here, that outputs\nthe final value computed by a hypothesis. And then, layer 2 in between,\nthis is called the hidden layer. The term hidden layer isn't a great\nterminology, but this ideation is that, you know, you supervised early, where you get to see the inputs and\nget to see the correct outputs, where there's a hidden layer of values you don't\nget to observe in the training setup. It's not x, and it's not y,\nand so we call those hidden. And they try to see neural nets\nwith more than one hidden layer but in this example, we have one input layer,\nLayer 1, one hidden layer, Layer 2, and one output layer, Layer 3. But basically,\nanything that isn't an input layer and isn't an output layer is\ncalled a hidden layer. So I want to be really clear about\nwhat this neural network is doing. Let's step through the computational\nsteps that are and body represented by this diagram. To explain these specific computations\nrepresented by a neural network, here's a little bit more notation. I'm going to use a superscript j\nsubscript i to denote the activation of neuron i or of unit i in layer j. So completely this gave\nsuperscript to sub group one, that's the activation of the first unit\nin layer two, in our hidden layer. And by activation I just mean\nthe value that's computed by and as output by a specific. In addition, new network is\nparametrize by these matrixes, theta super script j Where theta j is going\nto be a matrix of weights controlling the function mapping form one layer,\nmaybe the first layer to the second layer, or from the second layer\nto the third layer. So here are the computations that\nare represented by this diagram. This first hidden unit here has\nit's value computed as follows, there's a is a21 is equal to the sigma\nfunction of the sigma activation function, also called the logistics\nactivation function, apply to this sort of linear\ncombination of these inputs. And then this second hidden\nunit has this activation value computer as sigmoid of this. And similarly for this third hidden\nunit is computed by that formula. So here we have 3 theta 1 which is matrix of parameters governing our mapping from our three different units,\nour hidden units. Theta 1 is going to be a 3. Theta 1 is going to be\na 3x4-dimensional matrix. And more generally,\nif a network has SJU units in there j and sj + 1 units and\nsj + 1 then the matrix theta j which governs the function\nmapping from there sj + 1. That will have to mention sj +1\nby sj + 1 I'll just be clear about this notation right. This is Subscript j + 1 and\nthat's s subscript j, and then this whole thing, plus 1,\nthis whole thing (sj + 1), okay? So that's s subscript j + 1 by, So that's s subscript\nj + 1 by sj + 1 where this plus one is not\npart of the subscript. Okay, so we talked about what the three\nhidden units do to compute their values. Finally, there's a loss of this final and after that we have one more\nunit which computer h of x and that's equal can also be written\nas a(3)1 and that's equal to this. And you notice that I've written\nthis with a superscript two here, because theta of superscript two\nis the matrix of parameters, or the matrix of weights that controls the\nfunction that maps from the hidden units, that is the layer two units to the one\nlayer three unit, that is the output unit. To summarize, what we've done is shown\nhow a picture like this over here defines an artificial neural network\nwhich defines a function h that maps with x's input values to\nhopefully to some space that provisions y. And these hypothesis\nare parameterized by parameters denoting with a capital theta so\nthat, as we vary theta, we get different hypothesis and\nwe get different functions. Mapping say from x to y. So this gives us a mathematical definition\nof how to represent the hypothesis in the neural network. In the next few videos what I would like\nto do is give you more intuition about what these hypothesis representations do,\nas well as go through a few examples and talk about how to compute\nthem efficiently.""",46,0,1
coursera,stanford_university,machine-learning,model-representation-ii,"b""In the last video, we gave a mathematical definition of how to represent or how to compute the hypotheses used by Neural Network. In this video, I like show you how to actually carry out that computation efficiently, and that is show you a vector rise implementation. And second, and more importantly, I want to start giving you intuition about why these neural network representations might be a good idea and how they can help us to learn complex nonlinear hypotheses. Consider this neural network. Previously we said that the sequence of steps that we need in order to compute the output of a hypotheses is these equations given on the left where we compute the activation values of the three hidden uses and then we use those to compute the final output of our hypotheses h of x. \nNow, I'm going to define a few extra terms. So, this term that I'm underlining here, I'm going to define that to be z superscript 2 subscript 1. So that we have that a(2)1, which is this term is equal to g of z to 1. And by the way, these superscript 2, you know, what that means is that the z2 and this a2 as well, the superscript 2 in parentheses means that these are values associated with layer 2, that is with the hidden layer in the neural network. Now this term here I'm going to similarly define as z(2)2. And finally, this last term here that I'm underlining, let me define that as z(2)3. So that similarly we have a(2)3 equals g of z(2)3. So these z values are just a linear combination, a weighted linear combination, of the input values x0, x1, x2, x3 that go into a particular neuron. Now if you look at this block of numbers, you may notice that that block of numbers corresponds suspiciously similar to the matrix vector operation, matrix vector multiplication of x1 times the vector x. Using this observation we're going to be able to vectorize this computation of the neural network. Concretely, let's define the feature vector x as usual to be the vector of x0, x1, x2, x3 where x0 as usual is always equal 1 and that defines z2 to be the vector of these z-values, you know, of z(2)1 z(2)2, z(2)3. And notice that, there, z2 this is a three dimensional vector. We can now vectorize the computation of a(2)1, a(2)2, a(2)3 as follows. We can just write this in two steps. We can compute z2 as theta 1 times x and that would give us this vector z2; and then a2 is g of z2 and just to be clear z2 here, This is a three-dimensional vector and a2 is also a three-dimensional vector and thus this activation g. This applies the sigmoid function element-wise to each of the z2's elements.\nAnd by the way, to make our notation a little more consistent with what we'll do later, in this input layer we have the inputs x, but we can also thing it is as in activations of the first layers. So, if I defined a1 to be equal to x. So, the a1 is vector, I can now take this x here and replace this with z2 equals theta1 times a1 just by defining a1 to be activations in my input layer. Now, with what I've written so far I've now gotten myself the values for a1, a2, a3, and really I should put the superscripts there as well. But I need one more value, which is I also want this a(0)2 and that corresponds to a bias unit in the hidden layer that goes to the output there. Of course, there was a bias unit here too that, you know, it just didn't draw under here but to take care of this extra bias unit, what we're going to do is add an extra a0 superscript 2, that's equal to one, and after taking this step we now have that a2 is going to be a four dimensional feature vector because we just added this extra, you know, a0 which is equal to 1 corresponding to the bias unit in the hidden layer.\nAnd finally, to compute the actual value output of our hypotheses, we then simply need to compute z3. So z3 is equal to this term here that I'm just underlining. This inner term there is z3. And z3 is stated 2 times a2 and finally my hypotheses output h of x which is a3 that is the activation of my one and only unit in the output layer. So, that's just the real number. You can write it as a3 or as a(3)1 and that's g of z3. This process of computing h of x is also called forward propagation and is called that because we start of with the activations of the input-units and then we sort of forward-propagate that to the hidden layer and compute the activations of the hidden layer and then we sort of forward propagate that and compute the activations of the output layer, but this process of computing the activations from the input then the hidden then the output layer, and that's also called forward propagation and what we just did is we just worked out a vector wise implementation of this procedure.\nSo, if you implement it using these equations that we have on the right, these would give you an efficient way or both of the efficient way of computing h of x. This forward propagation view also helps us to understand what Neural Networks might be doing and why they might help us to learn interesting nonlinear hypotheses. Consider the following neural network and let's say I cover up the left path of this picture for now. If you look at what's left in this picture. This looks a lot like logistic regression where what we're doing is we're using that note, that's just the logistic regression unit and we're using that to make a prediction h of x. \nAnd concretely, what the hypotheses is outputting is h of x is going to be equal to g which is my sigmoid activation function times theta 0 times a0 is equal to 1 plus theta 1 plus theta 2 times a2 plus theta 3 times a3 whether values a1, a2, a3 are those given by these three given units. Now, to be actually consistent to my early notation. Actually, we need to, you know, fill in these superscript 2's here everywhere and I also have these indices 1 there because I have only one output unit, but if you focus on the blue parts of the notation. This is, you know, this looks awfully like the standard logistic regression model, except that I now have a capital theta instead of lower case theta. And what this is doing is just logistic regression. But where the features fed into logistic regression are these values computed by the hidden layer. Just to say that again, what this neural network is doing is just like logistic regression, except that rather than using the original features x1, x2, x3, is using these new features a1, a2, a3. Again, we'll put the superscripts there, you know, to be consistent with the notation. And the cool thing about this, is that the features a1, a2, a3, they themselves are learned as functions of the input. Concretely, the function mapping from layer 1 to layer 2, that is determined by some other set of parameters, theta 1. So it's as if the neural network, instead of being constrained to feed the features x1, x2, x3 to logistic regression. It gets to learn its own features, a1, a2, a3, to feed into the logistic regression and as you can imagine depending on what parameters it chooses for theta 1. \nYou can learn some pretty interesting and complex features and therefore you can end up with a better hypotheses than if you were constrained to use the raw features x1, x2 or x3 or if you will constrain to say choose the polynomial terms, you know, x1, x2, x3, and so on. But instead, this algorithm has the flexibility to try to learn whatever features at once, using these a1, a2, a3 in order to feed into this last unit that's essentially a logistic regression here. \nI realized this example is described as a somewhat high level and so I'm not sure if this intuition of the neural network, you know, having more complex features will quite make sense yet, but if it doesn't yet in the next two videos I'm going to go through a specific example of how a neural network can use this hidden there to compute more complex features to feed into this final output layer and how that can learn more complex hypotheses. So, in case what I'm saying here doesn't quite make sense, stick with me for the next two videos and hopefully out there working through those examples this explanation will make a little bit more sense. But just the point O. You can have neural networks with other types of diagrams as well, and the way that neural networks are connected, that's called the architecture. So the term architecture refers to how the different neurons are connected to each other. This is an example of a different neural network architecture and once again you may be able to get this intuition of how the second layer, here we have three heading units that are computing some complex function maybe of the input layer, and then the third layer can take the second layer's features and compute even more complex features in layer three so that by the time you get to the output layer, layer four, you can have even more complex features of what you are able to compute in layer three and so get very interesting nonlinear hypotheses. By the way, in a network like this, layer one, this is called an input layer. Layer four is still our output layer, and this network has two hidden layers. So anything that's not an input layer or an output layer is called a hidden layer. So, hopefully from this video you've gotten a sense of how the feed forward propagation step in a neural network works where you start from the activations of the input layer and forward propagate that to the first hidden layer, then the second hidden layer, and then finally the output layer. And you also saw how we can vectorize that computation. In the next, I realized that some of the intuitions in this video of how, you know, other certain layers are computing complex features of the early layers. I realized some of that intuition may be still slightly abstract and kind of a high level. And so what I would like to do in the two videos is work through a detailed example of how a neural network can be used to compute nonlinear functions of the input and hope that will give you a good sense of the sorts of complex nonlinear hypotheses we can get out of Neural Networks.""",47,0,1
coursera,stanford_university,machine-learning,examples-and-intuitions-i,"b""In this and the next video I want to\nwork through a detailed example showing how a neural network can compute a complex\nnon linear function of the input. And hopefully this will give you a good\nsense of why neural networks can be used to learn complex\nnon linear hypotheses. Consider the following problem\nwhere we have features X1 and X2 that are binary values. So, either 0 or 1. So, X1 and X2 can each take on\nonly one of two possible values. In this example,\nI've drawn only two positive examples and two negative examples. That you can think of this as a simplified\nversion of a more complex learning problem where we may have a bunch of positive\nexamples in the upper right and lower left and a bunch of negative\nexamples denoted by the circles. And what we'd like to do is learn\na non-linear division of boundary that may need to separate the positive and\nnegative examples. So, how can a neural network do this and\nrather than using the example and the variable to use this maybe easier\nto examine example on the left. Concretely what this is, is really computing the type of\nlabel y equals x 1 x or x 2. Or actually this is actually the x 1 x nor\nx 2 function where x nor is the alternative notation for\nnot x 1 or x 2. So, x 1 x or x 2 that's true only if\nexactly 1 of x 1 or x 2 is equal to 1. It turns out that these specific\nexamples in the works out a little bit better if we use the XNOR example instead. These two are the same of course. This means not x1 or x2 and so, we're\ngoing to have positive examples of either both are true or both are false and\nwhat have as y equals 1, y equals 1. And we're going to have y equals 0 if only\none of them is true and we're going to figure out if we can get a neural network\nto fit to this sort of training set. In order to build up to a network that\nfits the XNOR example we're going to start with a slightly simpler one and\nshow a network that fits the AND function. Concretely, let's say we have input x1 and x2 that are again binaries so,\nit's either 0 or 1 and\nlet's say our target labels y = x1 AND x2. This is a logical AND. So, can we get a one-unit network to\ncompute this logical AND function? In order to do so, I'm going to actually draw in the bias\nunit as well the plus one unit. Now let me just assign some values to\nthe weights or parameters of this network. I'm gonna write down the parameters\non this diagram here, -30 here. +20 and + 20. And what this mean is just that\nI'm assigning a value of -30 to the value associated with X0\nthis +1 going into this unit and a parameter value of +20 that\nmultiplies to X1 a value of +20 for the parameter that multiplies into x 2. So, concretely it's the same that the hypothesis h(x)=g(-30+20\nX1 plus 20 X2. So, sometimes it's just\nconvenient to draw these weights. Draw these parameters up here in\nthe diagram within and of course this- 30. This is actually theta 1 of 1 0. This is theta 1 of 1 1 and\nthat's theta 1 of 1 2 but it's just easier to think\nabout it as associating these parameters with\nthe edges of the network. Let's look at what this little\nsingle neuron network will compute. Just to remind you the sigmoid activation\nfunction g(z) looks like this. It starts from 0 rises\nsmoothly crosses 0.5 and then it asymptotic as 1 and\nto give you some landmarks, if the horizontal axis value\nz is equal to 4.6 then the sigmoid function is equal to 0.99. This is very close to 1 and\nkind of symmetrically, if it's -4.6 then the sigmoid function\nthere is 0.01 which is very close to 0. Let's look at the four possible\ninput values for x1 and x2 and look at what the hypotheses\nwill output in that case. If x1 and x2 are both equal to 0. If you look at this, if x1 x2 are both equal to 0\nthen the hypothesis of g of -30. So, this is a very far to the left of this\ndiagram so it will be very close to 0. If x 1 equals 0 and x equals 1,\nthen this formula here evaluates the g that is the sigma function applied to -10,\nand again that's you know to the far left of this plot and\nso, that's again very close to 0. This is also g of minus 10 that\nis f x 1 is equal to 1 and x 2 0, this minus 30 plus\n20 which is minus 10 and finally if x 1 equals 1 x 2 equals 1 then\nyou have g of minus 30 plus 20 plus 20. So, that's g of positive 10 which\nis there for very close to 1. And if you look in this column this\nis exactly the logical and function. So, this is computing h of x is approximately x 1 and x 2. In other words it outputs one If and\nonly if x2, x1 and x2, are both equal to 1. So, by writing out our\nlittle truth table like this we manage to figure what's\nthe logical function that our neural network computes. This network showed here computes the OR\nfunction. Just to show you how I worked that out. If you are write out the hypothesis\nthat this confusing g of -10 + 20 x 1 + 20 x 2 and so\nyou fill in these values. You find that's g of minus\n10 which is approximately 0. g of 10 which is approximately 1 and so\non and these are approximately 1 and approximately 1 and these numbers\nare essentially the logical OR function. So, hopefully with this you now understand\nhow single neurons in a neural network can be used to compute logical\nfunctions like AND and OR and so on. In the next video we'll continue\nbuilding on these examples and work through a more complex example. We'll get to show you how a neural network\nnow with multiple layers of units can be used to compute more complex\nfunctions like the XOR function or the XNOR function.""",48,0,1
coursera,stanford_university,machine-learning,examples-and-intuitions-ii,"b""In this video I'd like to keep working\nthrough our example to show how a Neural Network can compute\ncomplex non linear hypothesis. In the last video we saw how\na Neural Network can be used to compute the functions x1 AND\nx2, and the function x1 OR x2 when x1 and x2 are binary,\nthat is when they take on values 0,1. We can also have a network\nto compute negation, that is to compute the function not x1. Let me just write down the ways\nassociated with this network. We have only one input feature x1\nin this case and the bias unit +1. And if I associate this with\nthe weights plus 10 and -20, then my hypothesis is computing this\nh(x) equals sigmoid (10- 20 x1). So when x1 is equal to 0,\nmy hypothesis would be computing g(10- 20 x 0) is just 10. And so that's approximately 1,\nand when x is equal to 1, this will be g(-10) which is\napproximately equal to 0. And if you look at what these values are,\nthat's essentially the not x1 function. Cells include negations,\nthe general idea is to put that large negative weight in front\nof the variable you want to negate. Minus 20 multiplied by x1 and that's the general idea of\nhow you end up negating x1. And so in an example that I hope\nthat you can figure out yourself. If you want to compute a function\nlike this NOT x1 AND NOT x2, part of that will probably be putting\nlarge negative weights in front of x1 and x2, but it should be feasible. So you get a neural network with just\none output unit to compute this as well. All right, so\nthis logical function, NOT x1 AND NOT x2, is going to be equal to 1 if and\nonly if x1 equals x2 equals 0. All right since this is a logical\nfunction, this says NOT x1 means x1 must be 0 and NOT x2,\nthat means x2 must be equal to 0 as well. So this logical function is equal\nto 1 if and only if both x1 and x2 are equal to 0 and hopefully you\nshould be able to figure out how to make a small neural network to\ncompute this logical function as well. Now, taking the three pieces that we\nhave put together as the network for computing x1 AND x2, and the network\ncomputing for computing NOT x1 AND NOT x2. And one last network computing for\ncomputing x1 OR x2, we should be able to put these three pieces together\nto compute this x1 XNOR x2 function. And just to remind you if this is x1, x2, this function that we want to compute\nwould have negative examples here and here, and we'd have positive\nexamples there and there. And so clearly this will need\na non linear decision boundary in order to separate the positive and\nnegative examples. Let's draw the network. I'm going to take my input +1, x1,\nx2 and create my first hidden unit here. I'm gonna call this a 21 cuz\nthat's my first hidden unit. And I'm gonna copy the weight over\nfrom the red network, the x1 and x2. As well so then -30, 20, 20. Next let me create a second hidden\nunit which I'm going to call a 2 2. That is the second hidden\nunit of layer two. I'm going to copy over the cyan\nthat's work in the middle, so I'm gonna have the weights 10 -20 -20. And so,\nlet's pull some of the truth table values. For the red network,\nwe know that was computing the x1 and x2, and so\nthis will be approximately 0 0 0 1, depending on the values of x1 and\nx2, and for a 2 2, the cyan network. What do we know? The function NOT x1 AND NOT x2,\nthat outputs 1 0 0 0, for the 4 values of x1 and x2. Finally, I'm going to create my output\nnode, my output unit that is a 3 1. This is one more output h(x) and I'm going\nto copy over the old network for that. And I'm going to need a +1 bias unit here,\nso you draw that in, And I'm going to copy over the weights\nfrom the green networks. So that's -10, 20, 20 and we know earlier\nthat this computes the OR function. So let's fill in the truth table entries. So the first entry is 0 OR\n1 which can be 1 that makes 0 OR 0 which is 0, 0 OR 0 which is 0,\n1 OR 0 and that falls to 1. And thus h(x) is equal to 1 when\neither both x1 and x2 are zero or when x1 and x2 are both 1 and\nconcretely h(x) outputs 1 exactly at these two locations and\nthen outputs 0 otherwise. And thus will this neural network,\nwhich has a input layer, one hidden layer, and one output layer, we end up with a nonlinear decision\nboundary that computes this XNOR function. And the more general intuition\nis that in the input layer, we just have our four inputs. Then we have a hidden layer, which computed some slightly more complex\nfunctions of the inputs that its shown here this is slightly\nmore complex functions. And then by adding yet another layer we end up with an even\nmore complex non linear function. And this is a sort of intuition about\nwhy neural networks can compute pretty complicated functions. That when you have multiple layers you\nhave relatively simple function of the inputs of the second layer. But the third layer I can build on that to\ncomplete even more complex functions, and then the layer after that can\ncompute even more complex functions. To wrap up this video, I want to show you a fun example of\nan application of a the Neural Network that captures this intuition of the deeper\nlayers computing more complex features. I want to show you a video of that\ncustomer a good friend of mine Yann LeCunj. Yann is a professor at\nNew York University, NYU and he was one of the early pioneers\nof Neural Network reasearch and is sort of a legend in the field now and\nhis ideas are used in all sorts of products and\napplications throughout the world now. So I wanna show you a video from some\nof his early work in which he was using a neural network to recognize handwriting,\nto do handwritten digit recognition. You might remember early in this class,\nat the start of this class I said that one of the earliest successes of neural\nnetworks was trying to use it to read zip codes to help USPS Laws and\nread postal codes. So this is one of the attempts, this is one of the algorithms used\nto try to address that problem. In the video that I'll\nshow you this area here is the input area that shows a canvasing\ncharacter shown to the network. This column here shows a visualization of\nthe features computed by sort of the first hidden layer of the network. So that the first hidden layer of\nthe network and so the first hidden layer, this visualization shows\ndifferent features. Different edges and\nlines and so on detected. This is a visualization\nof the next hidden layer. It's kinda harder to see, harder to\nunderstand the deeper, hidden layers, and that's a visualization of why\nthe next hidden layer is confusing. You probably have a hard\ntime seeing what's going on much beyond the first hidden layer, but then finally, all of these learned\nfeatures get fed to the upper layer. And shown over here is the final answer,\nit's the final predictive value for what handwritten digit the neural\nnetwork thinks it is being shown. So let's take a look at the video. [MUSIC] So I hope you enjoyed the video and that\nthis hopefully gave you some intuition about the source of pretty complicated\nfunctions neural networks can learn. In which it takes its input this image,\njust takes this input, the raw pixels and the first hidden layer\ncomputes some set of features. The next hidden layer computes\neven more complex features and even more complex features. And these features can then be\nused by essentially the final layer of the logistic classifiers\nto make accurate predictions without the numbers that the network sees.""",49,0,1
coursera,stanford_university,machine-learning,multiclass-classification,"b'In this video, I want to tell you about how to use neural networks to do multiclass classification where we may have more than one category that we\'re trying to distinguish amongst. In the last part of the last video, where we had the handwritten digit recognition problem, that was actually a multiclass classification problem because there were ten possible categories for recognizing the digits from 0 through 9 and so, if you want us to fill you in on the details of how to do that. The way we do multiclass classification in a neural network is essentially an extension of the one versus all method. So, let\'s say that we have a computer vision example, where instead of just trying to recognize cars as in the original example that I started off with, but let\'s say that we\'re trying to recognize, you know, four categories of objects and given an image we want to decide if it is a pedestrian, a car, a motorcycle or a truck. If that\'s the case, what we would do is we would build a neural network with four output units so that our neural network now outputs a vector of four numbers. So, the output now is actually needing to be a vector of four numbers and what we\'re going to try to do is get the first output unit to classify: is the image a pedestrian, yes or no. The second unit to classify: is the image a car, yes or no. This unit to classify: is the image a motorcycle, yes or no, and this would classify: is the image a truck, yes or no. And thus, when the image is of a pedestrian, we would ideally want the network to output 1, 0, 0, 0, when it is a car we want it to output 0, 1, 0, 0, when this is a motorcycle, we get it to or rather, we want it to output 0, 0, 1, 0 and so on. So this is just like the ""one versus all"" method that we talked about when we were describing logistic regression, and here we have essentially four logistic regression classifiers, each of which is trying to recognize one of the four classes that we want to distinguish amongst. So, rearranging the slide of it, here\'s our neural network with four output units and those are what we want h of x to be when we have the different images, and the way we\'re going to represent the training set in these settings is as follows. So, when we have a training set with different images of pedestrians, cars, motorcycles and trucks, what we\'re going to do in this example is that whereas previously we had written out the labels as y being an integer from 1, 2, 3 or 4. Instead of representing y this way, we\'re going to instead represent y as follows: namely Yi will be either 1, 0, 0, 0 or 0, 1, 0, 0 or 0, 0, 1, 0 or 0, 0, 0, 1 depending on what the corresponding image Xi is. And so one training example will be one pair Xi colon Yi where Xi is an image with, you know one of the four objects and Yi will be one of these vectors. And hopefully, we can find a way to get our Neural Networks to output some value. So, the h of x is approximately y and both h of x and Yi, both of these are going to be in our example, four dimensional vectors when we have four classes. So, that\'s how you get neural network to do multiclass classification. This wraps up our discussion on how to represent Neural Networks that is on our hypotheses representation. In the next set of videos, let\'s start to talk about how take a training set and how to automatically learn the parameters of the neural network.'",50,0,1
coursera,stanford_university,machine-learning,cost-function,"b""Neural networks are one of the most\npowerful learning algorithms that we have today. In this and in the next few videos,\nI'd like to start talking about a learning algorithm for fitting the parameters of\na neural network given a training set. As with the discussion of most of our\nlearning algorithms, we're going to begin by talking about the cost function\nfor fitting the parameters of the network. I'm going to focus on the application\nof neural networks to classification problems. So suppose we have a network\nlike that shown on the left. And suppose we have a training\nset like this is x I, y I pairs of M training example. I'm going to use upper case L\nto denote the total number of layers in this network. So for the network shown on the left\nwe would have capital L equals 4. I'm going to use S subscript L\nto denote the number of units, that is the number of neurons. Not counting the bias unit\nin their L of the network. So for example, we would have a S one,\nwhich is equal there, equals S three unit,\nS two in my example is five units. And the output layer S four, which is also equal to S L because\ncapital L is equal to four. The output layer in my example\nunder that has four units. We're going to consider two types\nof classification problems. The first is Binary classification,\nwhere the labels y are either 0 or 1. In this case, we will have 1 output unit,\nso this Neural Network unit on top has 4 output units, but if we had\nbinary classification we would have only one output unit that computes h(x). And the output of the neural network would\nbe h(x) is going to be a real number. And in this case the number\nof output units, S L, where L is again\nthe index of the final layer. Cuz that's the number of layers\nwe have in the network so the number of units we have in the output\nlayer is going to be equal to 1. In this case to simplify notation later,\nI'm also going to set K=1 so you can think of K as also denoting\nthe number of units in the output layer. The second type of classification\nproblem we'll consider will be multi-class classification problem\nwhere we may have K distinct classes. So our early example had this\nrepresentation for y if we have 4 classes, and in this case we will have\ncapital K output units and our hypothesis or\noutput vectors that are K dimensional. And the number of output\nunits will be equal to K. And usually we would have K greater\nthan or equal to 3 in this case, because if we had two causes, then we don't\nneed to use the one verses all method. We use the one verses all method only\nif we have K greater than or equals V classes, so having only two classes we\nwill need to use only one upper unit. Now let's define the cost function for\nour neural network. The cost function we use for the neural\nnetwork is going to be a generalization of the one that we use for\nlogistic regression. For logistic regression we used to\nminimize the cost function J(theta) that was minus 1/m of this cost function and\nthen plus this extra regularization term here,\nwhere this was a sum from J=1 through n, because we did not regularize\nthe bias term theta0. For a neural network, our cost function\nis going to be a generalization of this. Where instead of having basically just\none, which is the compression output unit, we may instead have K of them. So here's our cost function. Our new network now outputs vectors in R K\nwhere R might be equal to 1 if we have a binary classification problem. I'm going to use this notation h(x)\nsubscript i to denote the ith output. That is, h(x) is a k-dimensional\nvector and so this subscript i just selects out the ith element of the vector\nthat is output by my neural network. My cost function J(theta) is\nnow going to be the following. Is - 1 over M of a sum of\na similar term to what we have for logistic regression, except that we\nhave the sum from K equals 1 through K. This summation is basically\na sum over my K output. A unit.\nSo if I have four output units, that is if the final layer of my\nneural network has four output units, then this is a sum from k\nequals one through four of basically the logistic regression\nalgorithm's cost function but summing that cost function over each\nof my four output units in turn. And so you notice in particular\nthat this applies to Yk Hk, because we're basically taking the K upper\nunits, and comparing that to the value of Yk which is that one of those\nvectors saying what cost it should be. And finally, the second term\nhere is the regularization term, similar to what we had for\nthe logistic regression. This summation term looks really\ncomplicated, but all it's doing is it's summing over these terms theta j i l for\nall values of i j and l. Except that we don't sum over the terms\ncorresponding to these bias values like we have for logistic progression. Completely, we don't sum over the terms\nresponding to where i is equal to 0. So that is because when we're\ncomputing the activation of a neuron, we have terms like these. Theta i 0. Plus theta i1, x1 plus and so on. Where I guess put in a two there,\nthis is the first hit in there. And so the values with a zero there, that corresponds to something that\nmultiplies into an x0 or an a0. And so this is kinda like a bias unit and\nby analogy to what we were doing for logistic progression, we won't sum over\nthose terms in our regularization term because we don't want to regularize\nthem and string their values as zero. But this is just one possible convention,\nand even if you were to sum over i equals 0 up to Sl, it would work about the\nsame and doesn't make a big difference. But maybe this convention of\nnot regularizing the bias term is just slightly more common. So that's the cost function we're\ngoing to use for our neural network. In the next video we'll start\nto talk about an algorithm for trying to optimize the cost function.""",51,0,1
coursera,stanford_university,machine-learning,backpropagation-algorithm,"b""In the previous video, we talked about a cost function for the neural network. In this video, let's start to talk about an algorithm, for trying to minimize the cost function. In particular, we'll talk about the back propagation algorithm. Here's the cost function that we wrote down in the previous video. What we'd like to do is try to find parameters theta to try to minimize j of theta. In order to use either gradient descent or one of the advance optimization algorithms. What we need to do therefore is to write code that takes this input the parameters theta and computes j of theta and these partial derivative terms. Remember, that the parameters in the the neural network of these things, theta superscript l subscript ij, that's the real number and so, these are the partial derivative terms we need to compute. In order to compute the cost function j of theta, we just use this formula up here and so, what I want to do for the most of this video is focus on talking about how we can compute these partial derivative terms. Let's start by talking about the case of when we have only one training example, so imagine, if you will that our entire training set comprises only one training example which is a pair xy. I'm not going to write x1y1 just write this. Write a one training example as xy and let's tap through the sequence of calculations we would do with this one training example. The first thing we do is we apply forward propagation in order to compute whether a hypotheses actually outputs given the input. Concretely, the called the a(1) is the activation values of this first layer that was the input there. So, I'm going to set that to x and then we're going to compute z(2) equals theta(1) a(1) and a(2) equals g, the sigmoid activation function applied to z(2) and this would give us our activations for the first middle layer. That is for layer two of the network and we also add those bias terms. Next we apply 2 more steps of this four and propagation to compute a(3) and a(4) which is also the upwards of a hypotheses h of x. So this is our vectorized implementation of forward propagation and it allows us to compute the activation values for all of the neurons in our neural network. Next, in order to compute the derivatives, we're going to use an algorithm called back propagation. The intuition of the back propagation algorithm is that for each note we're going to compute the term delta superscript l subscript j that's going to somehow represent the error of note j in the layer l. So, recall that a superscript l subscript j that does the activation of the j of unit in layer l and so, this delta term is in some sense going to capture our error in the activation of that neural duo. So, how we might wish the activation of that note is slightly different. Concretely, taking the example neural network that we have on the right which has four layers. And so capital L is equal to 4. For each output unit, we're going to compute this delta term. So, delta for the j of unit in the fourth layer is equal to just the activation of that unit minus what was the actual value of 0 in our training example. So, this term here can also be written h of x subscript j, right. So this delta term is just the difference between when a hypotheses output and what was the value of y in our training set whereas y subscript j is the j of element of the vector value y in our labeled training set. And by the way, if you think of delta a and y as vectors then you can also take those and come up with a vectorized implementation of it, which is just delta 4 gets set as a4 minus y. Where here, each of these delta 4 a4 and y, each of these is a vector whose dimension is equal to the number of output units in our network. So we've now computed the era term's delta 4 for our network. What we do next is compute the delta terms for the earlier layers in our network. Here's a formula for computing delta 3 is delta 3 is equal to theta 3 transpose times delta 4. And this dot times, this is the element y's multiplication operation that we know from MATLAB. So delta 3 transpose delta 4, that's a vector; g prime z3 that's also a vector and so dot times is in element y's multiplication between these two vectors. This term g prime of z3, that formally is actually the derivative of the activation function g evaluated at the input values given by z3. If you know calculus, you can try to work it out yourself and see that you can simplify it to the same answer that I get. But I'll just tell you pragmatically what that means. What you do to compute this g prime, these derivative terms is just a3 dot times1 minus A3 where A3 is the vector of activations. 1 is the vector of ones and A3 is again the activation the vector of activation values for that layer. Next you apply a similar formula to compute delta 2 where again that can be computed using a similar formula. Only now it is a2 like so and I then prove it here but you can actually, it's possible to prove it if you know calculus that this expression is equal to mathematically, the derivative of the g function of the activation function, which I'm denoting by g prime. And finally, that's it and there is no delta1 term, because the first layer corresponds to the input layer and that's just the feature we observed in our training sets, so that doesn't have any error associated with that. It's not like, you know, we don't really want to try to change those values. And so we have delta terms only for layers 2, 3 and for this example. The name back propagation comes from the fact that we start by computing the delta term for the output layer and then we go back a layer and compute the delta terms for the third hidden layer and then we go back another step to compute delta 2 and so, we're sort of back propagating the errors from the output layer to layer 3 to their to hence the name back complication. Finally, the derivation is surprisingly complicated, surprisingly involved but if you just do this few steps steps of computation it is possible to prove viral frankly some what complicated mathematical proof. It's possible to prove that if you ignore authorization then the partial derivative terms you want are exactly given by the activations and these delta terms. This is ignoring lambda or alternatively the regularization term lambda will equal to 0. We'll fix this detail later about the regularization term, but so by performing back propagation and computing these delta terms, you can, you know, pretty quickly compute these partial derivative terms for all of your parameters. So this is a lot of detail. Let's take everything and put it all together to talk about how to implement back propagation to compute derivatives with respect to your parameters. And for the case of when we have a large training set, not just a training set of one example, here's what we do. Suppose we have a training set of m examples like that shown here. The first thing we're going to do is we're going to set these delta l subscript i j. So this triangular symbol? That's actually the capital Greek alphabet delta . The symbol we had on the previous slide was the lower case delta. So the triangle is capital delta. We're gonna set this equal to zero for all values of l i j. Eventually, this capital delta l i j will be used to compute the partial derivative term, partial derivative respect to theta l i j of J of theta. So as we'll see in a second, these deltas are going to be used as accumulators that will slowly add things in order to compute these partial derivatives. Next, we're going to loop through our training set. So, we'll say for i equals 1 through m and so for the i iteration, we're going to working with the training example xi, yi. So the first thing we're going to do is set a1 which is the activations of the input layer, set that to be equal to xi is the inputs for our i training example, and then we're going to perform forward propagation to compute the activations for layer two, layer three and so on up to the final layer, layer capital L. Next, we're going to use the output label yi from this specific example we're looking at to compute the error term for delta L for the output there. So delta L is what a hypotheses output minus what the target label was? And then we're going to use the back propagation algorithm to compute delta L minus 1, delta L minus 2, and so on down to delta 2 and once again there is now delta 1 because we don't associate an error term with the input layer. And finally, we're going to use these capital delta terms to accumulate these partial derivative terms that we wrote down on the previous line. And by the way, if you look at this expression, it's possible to vectorize this too. Concretely, if you think of delta ij as a matrix, indexed by subscript ij. Then, if delta L is a matrix we can rewrite this as delta L, gets updated as delta L plus lower case delta L plus one times aL transpose. So that's a vectorized implementation of this that automatically does an update for all values of i and j.\nFinally, after executing the body of the four-loop we then go outside the four-loop and we compute the following. We compute capital D as follows and we have two separate cases for j equals zero and j not equals zero. The case of j equals zero corresponds to the bias term so when j equals zero that's why we're missing is an extra regularization term. Finally, while the formal proof is pretty complicated what you can show is that once you've computed these D terms, that is exactly the partial derivative of the cost function with respect to each of your perimeters and so you can use those in either gradient descent or in one of the advanced authorization algorithms. So that's the back propagation algorithm and how you compute derivatives of your cost function for a neural network. I know this looks like this was a lot of details and this was a lot of steps strung together. But both in the programming assignments write out and later in this video, we'll give you a summary of this so we can have all the pieces of the algorithm together so that you know exactly what you need to implement if you want to implement back propagation to compute the derivatives of your neural network's cost function with respect to those parameters.""",52,0,1
coursera,stanford_university,machine-learning,backpropagation-intuition,"b""In the previous video, we talked\nabout the backpropagation algorithm. To a lot of people seeing it for\nthe first time, their first impression is often that wow\nthis is a really complicated algorithm, and there are all these different steps,\nand I'm not sure how they fit together. And it's kinda this black box\nof all these complicated steps. In case that's how you're feeling about\nbackpropagation, that's actually okay. Backpropagation maybe unfortunately\nis a less mathematically clean, or less mathematically simple algorithm, compared to linear regression or\nlogistic regression. And I've actually used backpropagation,\nyou know, pretty successfully for many years. And even today I still don't sometimes\nfeel like I have a very good sense of just what it's doing, or intuition about\nwhat back propagation is doing. If, for those of you that are doing\nthe programming exercises, that will at least\nmechanically step you through the different steps of how\nto implement back prop. So you'll be able to get it to work for\nyourself. And what I want to do in this video is\nlook a little bit more at the mechanical steps of backpropagation, and try to give\nyou a little more intuition about what the mechanical steps the back prop is\ndoing to hopefully convince you that, you know,\nit's at least a reasonable algorithm. In case even after this video in case back\npropagation still seems very black box and kind of like a,\ntoo many complicated steps and a little bit magical to you,\nthat's actually okay. And Even though I've used back prop for\nmany years, sometimes this is a difficult algorithm to understand, but hopefully\nthis video will help a little bit. In order to better\nunderstand backpropagation, let's take another closer look at\nwhat forward propagation is doing. Here's a neural network with two input\nunits that is not counting the bias unit, and two hidden units in this layer, and\ntwo hidden units in the next layer. And then, finally, one output unit. Again, these counts two, two, two,\nare not counting these bias units on top. In order to illustrate\nforward propagation, I'm going to draw this network\na little bit differently. And in particular I'm going to draw\nthis neuro-network with the nodes drawn as these very fat ellipsis, so\nthat I can write text in them. When performing forward propagation,\nwe might have some particular example. Say some example x i comma y i. And it'll be this x i that we\nfeed into the input layer. So this maybe x i 2 and x i 2\nare the values we set the input layer to. And when we forward propagated\nto the first hidden layer here, what we do is compute z (2) 1 and z (2) 2. So these are the weighted sum\nof inputs of the input units. And then we apply the sigmoid\nof the logistic function, and the sigmoid activation\nfunction applied to the z value. Here's are the activation values. So that gives us a (2) 1 and a (2) 2. And then we forward propagate\nagain to get here z (3) 1. Apply the sigmoid of\nthe logistic function, the activation function\nto that to get a (3) 1. And similarly, like so\nuntil we get z (4) 1. Apply the activation function. This gives us a (4)1, which is the final\noutput value of the neural network. Let's erase this arrow to\ngive myself some more space. And if you look at what this\ncomputation really is doing, focusing on this hidden unit, let's say. We have to add this weight. Shown in magenta there is my weight theta\n(2) 1 0, the indexing is not important. And this way here,\nwhich I'm highlighting in red, that is theta (2) 1 1 and\nthis weight here, which I'm drawing in cyan,\nis theta (2) 1 2. So the way we compute this value,\nz(3)1 is, z(3)1 is as equal to this\nmagenta weight times this value. So that's theta (2) 10 x 1. And then plus this red\nweight times this value, so that's theta(2) 11 times a(2)1. And finally this cyan\nweight times this value, which is therefore plus\ntheta(2)12 times a(2)1. And so that's forward propagation. And it turns out that as we'll\nsee later in this video, what backpropagation is doing is\ndoing a process very similar to this. Except that instead of the computations\nflowing from the left to the right of this network, the computations since their flow\nfrom the right to the left of the network. And using a very similar\ncomputation as this. And I'll say in two slides\nexactly what I mean by that. To better understand what backpropagation\nis doing, let's look at the cost function. It's just the cost function that we had\nfor when we have only one output unit. If we have more than one output unit, we just have a summation you know over\nthe output units indexed by k there. If you have only one output unit\nthen this is a cost function. And we do forward propagation and\nbackpropagation on one example at a time. So let's just focus on the single example,\nx (i) y (i) and focus on the case of\nhaving one output unit. So y (i) here is just a real number. And let's ignore regularization,\nso lambda equals 0. And this final term,\nthat regularization term, goes away. Now if you look inside the summation, you find that the cost term\nassociated with the training example, that is the cost associated with\nthe training example x(i), y(i). That's going to be given\nby this expression. So, the cost to live off\nexamplie i is written as follows. And what this cost function does is it\nplays a role similar to the squared arrow. So, rather than looking at\nthis complicated expression, if you want you can think of cost\nof i being approximately the square difference between what the neural network\noutputs, versus what is the actual value. Just as in logistic repression,\nwe actually prefer to use the slightly more complicated cost\nfunction using the log. But for the purpose of intuition,\nfeel free to think of the cost function as being the sort of the squared\nerror cost function. And so this cost(i) measures\nhow well is the network doing on correctly predicting example i. How close is the output to\nthe actual observed label y(i)? Now let's look at what\nbackpropagation is doing. One useful intuition is that\nbackpropagation is computing these delta superscript l subscript j terms. And we can think of these as the quote\nerror of the activation value that we got for unit j in the layer,\nin the lth layer. More formally, for, and\nthis is maybe only for those of you who\nare familiar with calculus. More formally,\nwhat the delta terms actually are is this, they're the partial derivative\nwith respect to z,l,j, that is this weighted sum of inputs\nthat were confusing these z terms. Partial derivatives with respect to\nthese things of the cost function. So concretely, the cost function\nis a function of the label y and of the value,\nthis h of x output value neural network. And if we could go inside\nthe neural network and just change those z l\nj values a little bit, then that will affect these values\nthat the neural network is outputting. And that will end up\nchanging the cost function. And again really, this is only for\nthose of you who are expert in Calculus. If you're comfortable\nwith partial derivatives, what these delta terms are is they turn\nout to be the partial derivative of the cost function, with respect to these\nintermediate terms that were confusing. And so they're a measure of how much would\nwe like to change the neural network's weights, in order to affect these\nintermediate values of the computation. So as to affect the final output\nof the neural network h(x) and therefore affect the overall cost. In case this lost part of this\npartial derivative intuition, in case that doesn't make sense. Don't worry about the rest of this, we can do without really talking\nabout partial derivatives. But let's look in more detail about\nwhat backpropagation is doing. For the output layer,\nthe first set's this delta term, delta (4) 1, as y (i) if we're\ndoing forward propagation and back propagation on this\ntraining example i. That says y(i) minus a(4)1. So this is really the error, right? It's the difference between\nthe actual value of y minus what was the value predicted, and so\nwe're gonna compute delta(4)1 like so. Next we're gonna do,\npropagate these values backwards. I'll explain this in a second, and\nend up computing the delta terms for the previous layer. We're gonna end up with delta(3)1. Delta(3)2. And then we're gonna propagate\nthis further backward, and end up computing delta(2)1 and\ndelta(2)2. Now the backpropagation\ncalculation is a lot like running the forward propagation algorithm,\nbut doing it backwards. So here's what I mean. Let's look at how we end up\nwith this value of delta(2)2. So we have delta(2)2. And similar to forward propagation,\nlet me label a couple of the weights. So this weight,\nwhich I'm going to draw in cyan. Let's say that weight is theta(2)1 2, and this one down here when\nwe highlight this in red. That is going to be let's\nsay theta(2) of 2 2. So if we look at how delta(2)2, is computed,\nhow it's computed with this note. It turns out that what we're going to do,\nis gonna take this value and multiply it by this weight, and add it\nto this value multiplied by that weight. So it's really a weighted\nsum of these delta values, weighted by the corresponding\nedge strength. So completely, let me fill this in,\nthis delta(2)2 is going to be equal to, Theta(2)1 2 is that magenta\nlay times delta(3)1. Plus, and the thing I had in red, that's theta (2)2 times delta (3)2. So it's really literally this\nred wave times this value, plus this magenta weight times this value. And that's how we wind up\nwith that value of delta. And just as another example,\nlet's look at this value. How do we get that value? Well it's a similar process. If this weight,\nwhich I'm gonna highlight in green, if this weight is equal to,\nsay, delta (3) 1 2. Then we have that delta (3) 2 is going\nto be equal to that green weight, theta (3) 12 times delta (4) 1. And by the way, so far I've been\nwriting the delta values only for the hidden units, but\nexcluding the bias units. Depending on how you define\nthe backpropagation algorithm, or depending on how you implement it,\nyou know, you may end up implementing something that computes delta values for\nthese bias units as well. The bias units always output the value of\nplus one, and they are just what they are, and there's no way for\nus to change the value. And so, depending on your\nimplementation of back prop, the way I usually implement it. I do end up computing these delta values,\nbut we just discard them, we don't use them. Because they don't end up being\npart of the calculation needed to compute a derivative. So hopefully that gives you\na little better intuition about what back propegation is doing. In case of all of this still\nseems sort of magical, sort of black box, in a later video, in\nthe putting it together video, I'll try to get a little bit more intuition\nabout what backpropagation is doing. But unfortunately this is a difficult\nalgorithm to try to visualize and understand what it is really doing. But fortunately I've been, I guess many people have been using\nvery successfully for many years. And if you implement the algorithm you can\nhave a very effective learning algorithm. Even though the inner workings of exactly\nhow it works can be harder to visualize.""",53,0,1
coursera,stanford_university,machine-learning,implementation-note-unrolling-parameters,"b""In the previous video, we talked about how to use back propagation to compute the derivatives of your cost function. In this video, I want to quickly tell you about one implementational detail of unrolling your parameters from matrices into vectors, which we need in order to use the advanced optimization routines. Concretely, let's say you've implemented a cost function that takes this input, you know, parameters theta and returns the cost function and returns derivatives. Then you can pass this to an advanced authorization algorithm by fminunc and fminunc isn't the only one by the way. There are also other advanced authorization algorithms. But what all of them do is take those input pointedly the cost function, and some initial value of theta. And both, and these routines assume that theta and the initial value of theta, that these are parameter vectors, maybe Rn or Rn plus 1. But these are vectors and it also assumes that, you know, your cost function will return as a second return value this gradient which is also Rn and Rn plus 1. So also a vector. This worked fine when we were using logistic progression but now that we're using a neural network our parameters are no longer vectors, but instead they are these matrices where for a full neural network we would have parameter matrices theta 1, theta 2, theta 3 that we might represent in Octave as these matrices theta 1, theta 2, theta 3. And similarly these gradient terms that were expected to return. Well, in the previous video we showed how to compute these gradient matrices, which was capital D1, capital D2, capital D3, which we might represent an octave as matrices D1, D2, D3. In this video I want to quickly tell you about the idea of how to take these matrices and unroll them into vectors. So that they end up being in a format suitable for passing into as theta here off for getting out for a gradient there. Concretely, let's say we have a neural network with one input layer with ten units, hidden layer with ten units and one output layer with just one unit, so s1 is the number of units in layer one and s2 is the number of units in layer two, and s3 is a number of units in layer three. In this case, the dimension of your matrices theta and D are going to be given by these expressions. For example, theta one is going to a 10 by 11 matrix and so on. So in if you want to convert between these matrices. vectors. What you can do is take your theta 1, theta 2, theta 3, and write this piece of code and this will take all the elements of your three theta matrices and take all the elements of theta one, all the elements of theta 2, all the elements of theta 3, and unroll them and put all the elements into a big long vector. Which is thetaVec and similarly the second command would take all of your D matrices and unroll them into a big long vector and call them DVec.\nAnd finally if you want to go back from the vector representations to the matrix representations. What you do to get back to theta one say is take thetaVec and pull out the first 110 elements. So theta 1 has 110 elements because it's a 10 by 11 matrix so that pulls out the first 110 elements and then you can use the reshape command to reshape those back into theta 1. And similarly, to get back theta 2 you pull out the next 110 elements and reshape it. And for theta 3, you pull out the final eleven elements and run reshape to get back the theta 3. Here's a quick Octave demo of that process. So for this example let's set theta 1 equal to be ones of 10 by 11, so it's a matrix of all ones. And just to make this easier seen, let's set that to be 2 times ones, 10 by 11 and let's set theta 3 equals 3 times 1's of 1 by 11. So this is 3 separate matrices: theta 1, theta 2, theta 3. We want to put all of these as a vector. ThetaVec equals theta 1; theta 2 theta 3. Right, that's a colon in the middle and like so and now thetavec is going to be a very long vector. That's 231 elements. If I display it, I find that this very long vector with all the elements of the first matrix, all the elements of the second matrix, then all the elements of the third matrix. And if I want to get back my original matrices, I can do reshape thetaVec. Let's pull out the first 110 elements and reshape them to a 10 by 11 matrix. This gives me back theta 1. And if I then pull out the next 110 elements. So that's indices 111 to 220. I get back all of my 2's. And if I go from 221 up to the last element, which is element 231, and reshape to 1 by 11, I get back theta 3. To make this process really concrete, here's how we use the unrolling idea to implement our learning algorithm. Let's say that you have some initial value of the parameters theta 1, theta 2, theta 3. What we're going to do is take these and unroll them into a long vector we're gonna call initial theta to pass in to fminunc as this initial setting of the parameters theta. The other thing we need to do is implement the cost function. Here's my implementation of the cost function. The cost function is going to give us input, thetaVec, which is going to be all of my parameters vectors that in the form that's been unrolled into a vector. So the first thing I'm going to do is I'm going to use thetaVec and I'm going to use the reshape functions. So I'll pull out elements from thetaVec and use reshape to get back my original parameter matrices, theta 1, theta 2, theta 3. So these are going to be matrices that I'm going to get. So that gives me a more convenient form in which to use these matrices so that I can run forward propagation and back propagation to compute my derivatives, and to compute my cost function j of theta. And finally, I can then take my derivatives and unroll them, to keeping the elements in the same ordering as I did when I unroll my thetas. But I'm gonna unroll D1, D2, D3, to get gradientVec which is now what my cost function can return. It can return a vector of these derivatives. So, hopefully, you now have a good sense of how to convert back and forth between the matrix representation of the parameters versus the vector representation of the parameters. The advantage of the matrix representation is that when your parameters are stored as matrices it's more convenient when you're doing forward propagation and back propagation and it's easier when your parameters are stored as matrices to take advantage of the, sort of, vectorized implementations. Whereas in contrast the advantage of the vector representation, when you have like thetaVec or DVec is that when you are using the advanced optimization algorithms. Those algorithms tend to assume that you have all of your parameters unrolled into a big long vector. And so with what we just went through, hopefully you can now quickly convert between the two as needed.""",54,0,1
coursera,stanford_university,machine-learning,gradient-checking,"b""In the last few videos we talked about\nhow to do forward propagation and back propagation in a neural network\nin order to compute derivatives. But back prop as an algorithm\nhas a lot of details and can be a little bit tricky to implement. And one unfortunate property\nis that there are many ways to have subtle bugs in back prop. So that if you run it\nwith gradient descent or some other optimizational algorithm,\nit could actually look like it's working. And your cost function, J of theta may end up decreasing on\nevery iteration of gradient descent. But this could prove true even\nthough there might be some bug in your implementation of back prop. So that it looks J of theta is decreasing,\nbut you might just wind up with a neural\nnetwork that has a higher level of error than you would with\na bug free implementation. And you might just not know that there\nwas this subtle bug that was giving you worse performance. So, what can we do about this? There's an idea called gradient checking that eliminates almost\nall of these problems. So, today every time I\nimplement back propagation or a similar gradient to a [INAUDIBLE]\non a neural network or any other reasonably complex model,\nI always implement gradient checking. And if you do this, it will help you make\nsure and sort of gain high confidence that your implementation of four prop and\nback prop or whatever is 100% correct. And from what I've seen this pretty much\neliminates all the problems associated with a sort of a buggy\nimplementation as a back prop. And in the previous videos I asked you to\ntake on faith that the formulas I gave for computing the deltas and the vs and\nso on, I asked you to take on faith that those actually do compute\nthe gradients of the cost function. But once you implement numerical gradient\nchecking, which is the topic of this video, you'll be able to absolute verify\nfor yourself that the code you're writing does indeed, is indeed computing\nthe derivative of the cross function J. So here's the idea,\nconsider the following example. Suppose that I have the function J\nof theta and I have some value theta and for this example gonna assume\nthat theta is just a real number. And let's say that I want to estimate the\nderivative of this function at this point and so the derivative is equal to\nthe slope of that tangent one. Here's how I'm going to numerically\napproximate the derivative, or rather here's a procedure for\nnumerically approximating the derivative. I'm going to compute theta plus epsilon,\nso now we move it to the right. And I'm gonna compute theta minus epsilon\nand I'm going to look at those two points, And\nconnect them by a straight line And I'm gonna connect these two points\nby a straight line, and I'm gonna use the slope of that little red line as\nmy approximation to the derivative. Which is, the true derivative is\nthe slope of that blue line over there. So, you know it seems like it would\nbe a pretty good approximation. Mathematically, the slope of this\nred line is this vertical height divided by this horizontal width. So this point on top is the J\nof (Theta plus Epsilon). This point here is J\n(Theta minus Epsilon), so this vertical difference is J\n(Theta plus Epsilon) minus J of theta minus epsilon and this\nhorizontal distance is just 2 epsilon. So my approximation is going\nto be that the derivative respect of theta of J of theta at this\nvalue of theta, that that's approximately J of theta plus epsilon minus J of\ntheta minus epsilon over 2 epsilon. Usually, I use a pretty small value for epsilon, expect epsilon to be maybe\non the order of 10 to the minus 4. There's usually a large range of different\nvalues for epsilon that work just fine. And in fact, if you let epsilon become\nreally small, then mathematically this term here, actually mathematically,\nit becomes the derivative. It becomes exactly the slope\nof the function at this point. It's just that we don't want\nto use epsilon that's too, too small, because then you might\nrun into numerical problems. So I usually use epsilon\naround ten to the minus four. And by the way some of you may have\nseen an alternative formula for s meeting the derivative\nwhich is this formula. This one on the right is\ncalled a one-sided difference, whereas the formula on the left,\nthat's called a two-sided difference. The two sided difference gives us\na slightly more accurate estimate, so I usually use that, rather than\nthis one sided difference estimate. So, concretely, when you implement an\noctave, is you implemented the following, you implement call to compute gradApprox, which is going to be our approximation\nderivative as just here this formula, J of theta plus epsilon minus J of theta\nminus epsilon divided by 2 times epsilon. And this will give you a numerical\nestimate of the gradient at that point. And in this example it seems like\nit's a pretty good estimate. Now on the previous slide, we considered the case of when\ntheta was a rolled number. Now let's look at a more general case\nof when theta is a vector parameter, so let's say theta is an R n. And it might be an unrolled version of\nthe parameters of our neural network. So theta is a vector that has n elements,\ntheta 1 up to theta n. We can then use a similar idea to approximate\nall the partial derivative terms. Concretely the partial derivative of\na cost function with respect to the first parameter, theta one, that can be obtained\nby taking J and increasing theta one. So you have J of theta one\nplus epsilon and so on. Minus J of this theta one minus\nepsilon and divide it by two epsilon. The partial derivative respect to the\nsecond parameter theta two, is again this thing except that you would take J of here\nyou're increasing theta two by epsilon, and here you're decreasing theta two by\nepsilon and so on down to the derivative. With respect of theta n would give\nyou increase and decrease theta and by epsilon over there. So, these equations give you a way to\nnumerically approximate the partial derivative of J with respect to any\none of your parameters theta i. Completely, what you implement\nis therefore the following. We implement the following in octave\nto numerically compute the derivatives. We say, for i = 1:n, where n is the dimension\nof our parameter of vector theta. And I usually do this with\nthe unrolled version of the parameter. So theta is just a long list of all of\nmy parameters in my neural network, say. I'm gonna set thetaPlus = theta, then increase thetaPlus of\nthe (i) element by epsilon. And so this is basically thetaPlus\nis equal to theta except for thetaPlus(i) which is now\nincremented by epsilon. Epsilon, so theta plus is equal to,\nwrite theta 1, theta 2 and so on. Then theta I has epsilon added to it and\nthen we go down to theta N. So this is what theta plus is. And similar these two lines set theta\nminus to something similar except that this instead of theta I plus Epsilon,\nthis now becomes theta I minus Epsilon. And then finally you implement\nthis gradApprox (i) and this would give you your approximation\nto the partial derivative respect of theta i of J of theta. And the way we use this in our\nneural network implementation is, we would implement this four loop to\ncompute the top partial derivative of the cost function for respect to\nevery parameter in that network, and we can then take the gradient\nthat we got from backprop. So DVec was the derivative\nwe got from backprop. All right, so backprop, backpropogation, was a relatively efficient way to compute\na derivative or a partial derivative. Of a cost function with\nrespect to all our parameters. And what I usually do is then,\ntake my numerically computed derivative that is this gradApprox that\nwe just had from up here. And make sure that that is equal or\napproximately equal up to small values of numerical round up,\nthat it's pretty close. So the DVec that I got from backprop. And if these two ways of computing\nthe derivative give me the same answer, or give me any similar answers,\nup to a few decimal places, then I'm much more confident that my\nimplementation of backprop is correct. And when I plug these DVec\nvectors into gradient assent or some advanced optimization algorithm,\nI can then be much more confident that I'm computing the\nderivatives correctly, and therefore that hopefully my code will run correctly and\ndo a good job optimizing J of theta. Finally, I wanna put\neverything together and tell you how to implement this\nnumerical gradient checking. Here's what I usually do. First thing I do is implement\nback propagation to compute DVec. So there's a procedure we talked\nabout in the earlier video to compute DVec which may be our\nunrolled version of these matrices. So then what I do, is implement a numerical gradient\nchecking to compute gradApprox. So this is what I described earlier in\nthis video and in the previous slide. Then should make sure that DVec and\ngradApprox give similar values, you know let's say up to\na few decimal places. And finally and this is the important\nstep, before you start to use your code for learning, for seriously training\nyour network, it's important to turn off gradient checking and to no longer\ncompute this gradApprox thing using the numerical derivative formulas that\nwe talked about earlier in this video. And the reason for that is the numeric\ncode gradient checking code, the stuff we talked about in this video,\nthat's a very computationally expensive, that's a very slow way to try\nto approximate the derivative. Whereas In contrast, the back propagation\nalgorithm that we talked about earlier, that is the thing we talked\nabout earlier for computing. You know, D1, D2, D3 for Dvec. Backprop is much more computationally\nefficient way of computing for derivatives. So once you've verified that your\nimplementation of back propagation is correct, you should turn off gradient\nchecking and just stop using that. So just to reiterate, you should be sure\nto disable your gradient checking code before running your algorithm for\nmany iterations of gradient descent or for many iterations of the advanced\noptimization algorithms, in order to train your classifier. Concretely, if you were to run\nthe numerical gradient checking on every single iteration\nof gradient descent. Or if you were in the inner\nloop of your costFunction, then your code would be very slow. Because the numerical gradient\nchecking code is much slower than the backpropagation algorithm,\nthan the backpropagation method where, you remember, we were computing delta(4),\ndelta(3), delta(2), and so on. That was the backpropagation algorithm. That is a much faster way to compute\nderivates than gradient checking. So when you're ready, once you've verified\nthe implementation of back propagation is correct, make sure you turn off or\nyou disable your gradient checking code while you train your algorithm, or\nelse you code could run very slowly. So, that's how you take gradients\nnumericaly, and that's how you can verify tha implementation of back\npropagation is correct. Whenever I implement back propagation or\nsimilar gradient discerning algorithm for a complicated mode,l I always\nuse gradient checking and this really helps me make\nsure that my code is correct.""",55,0,1
coursera,stanford_university,machine-learning,random-initialization,"b""In the previous video, we've put together\nalmost all the pieces you need in order to implement and train in your network. There's just one last idea\nI need to share with you, which is the idea of\nrandom initialization. When you're running an algorithm\nof gradient descent, or also the advanced optimization algorithms,\nwe need to pick some initial value for the parameters theta. So for\nthe advanced optimization algorithm, it assumes you will pass it some\ninitial value for the parameters theta. Now let's consider a gradient descent. For that, we'll also need to\ninitialize theta to something, and then we can slowly take steps to go\ndownhill using gradient descent. To go downhill,\nto minimize the function j of theta. So what can we set the initial\nvalue of theta to? Is it possible to set the initial value\nof theta to the vector of all zeros? Whereas this worked okay when we\nwere using logistic regression, initializing all of your parameters\nto zero actually does not work when you are trading on your own network. Consider trading the follow\nNeural network, and let's say we initialize all\nthe parameters of the network to 0. And if you do that, then what you, what\nthat means is that at the initialization, this blue weight, colored in blue is gonna\nequal to that weight, so they're both 0. And this weight that\nI'm coloring in in red, is equal to that weight,\ncolored in red, and also this weight, which I'm coloring in green is going\nto equal to the value of that weight. And what that means is that both\nof your hidden units, A1 and A2, are going to be computing\nthe same function of your inputs. And thus you end up with for\nevery one of your training examples, you end up with A 2 1 equals A 2 2. And moreover because I'm not going\nto show this in too much detail, but because these outgoing weights\nare the same you can also show that the delta values\nare also gonna be the same. So concretely you end up with delta 1 1,\ndelta 2 1 equals delta 2 2, and if you work through the map further,\nwhat you can show is that the partial derivatives with respect to your\nparameters will satisfy the following, that the partial derivative of\nthe cost function with respected to breaking out the derivatives respect to\nthese two blue waves in your network. You find that these two partial\nderivatives are going to be equal to each other. And so what this means is that even\nafter say one greater descent update, you're going to update, say, this first\nblue rate was learning rate times this, and you're gonna update the second blue\nrate with some learning rate times this. And what this means is that even after one\ncreated the descent update, those two blue rates, those two blue color parameters\nwill end up the same as each other. So there'll be some nonzero value, but\nthis value would equal to that value. And similarly, even after one gradient descent update,\nthis value would equal to that value. There'll still be some non-zero values, just that the two red values\nare equal to each other. And similarly, the two green ways. Well, they'll both change values, but they'll both end up with\nthe same value as each other. So after each update, the parameters\ncorresponding to the inputs going into each of the two hidden\nunits are identical. That's just saying that the two green\nweights are still the same, the two red weights are still the same, the two blue\nweights are still the same, and what that means is that even after one iteration\nof say, gradient descent and descent. You find that your two headed units\nare still computing exactly the same functions of the inputs. You still have the a1(2) = a2(2). And so you're back to this case. And as you keep running greater descent,\nthe blue waves,, the two blue waves, will stay the same as each other. The two red waves will stay\nthe same as each other and the two green waves will\nstay the same as each other. And what this means is that your\nneural network really can compute very interesting functions, right? Imagine that you had not\nonly two hidden units, but imagine that you had many,\nmany hidden units. Then what this is saying is that all\nof your headed units are computing the exact same feature. All of your hidden units are computing\nthe exact same function of the input. And this is a highly\nredundant representation because you find the logistic\nprogression unit. It really has to see only one feature\nbecause all of these are the same. And this prevents you and your network\nfrom doing something interesting. In order to get around this problem,\nthe way we initialize the parameters of a neural network therefore is\nwith random initialization. Concretely, the problem was saw on\nthe previous slide is something called the problem of symmetric ways,\nthat's the ways are being the same. So this random initialization is\nhow we perform symmetry breaking. So what we do is we initialize each value\nof theta to a random number between minus epsilon and epsilon. So this is a notation to b numbers\nbetween minus epsilon and plus epsilon. So my weight for\nmy parameters are all going to be randomly initialized between\nminus epsilon and plus epsilon. The way I write code to do this in octave\nis I've said Theta1 should be equal to this. So this rand 10 by 11, that's how you compute a random\n10 by 11 dimensional matrix. All the values are between 0 and\n1, so these are going to be raw numbers that take on any\ncontinuous values between 0 and 1. And so\nif you take a number between zero and one, multiply it by two times\nINIT_EPSILON then minus INIT_EPSILON, then you end up with a number that's\nbetween minus epsilon and plus epsilon. And the so that leads us,\nthis epsilon here has nothing to do with the epsilon that we were using\nwhen we were doing gradient checking. So when numerical gradient checking, there we were adding some\nvalues of epsilon and theta. This is your unrelated value of epsilon. We just wanted to notate init\nepsilon just to distinguish it from the value of epsilon we\nwere using in gradient checking. And similarly if you want to\ninitialize theta2 to a random 1 by 11 matrix you can do so\nusing this piece of code here. So to summarize,\nto create a neural network what you should do is randomly initialize the waves\nto small values close to zero, between -epsilon and +epsilon say. And then implement back propagation,\ndo great in checking, and use either great in descent or 1b\nadvanced optimization algorithms to try to minimize j(theta) as a function of\nthe parameters theta starting from just randomly chosen initial value for\nthe parameters. And by doing symmetry breaking, which is this process,\nhopefully great gradient descent or the advanced optimization algorithms will\nbe able to find a good value of theta.""",56,0,1
coursera,stanford_university,machine-learning,putting-it-together,"b""So, it's taken us a lot of videos to get through the neural network learning algorithm. In this video, what I'd like to do is try to put all the pieces together, to give a overall summary or a bigger picture view, of how all the pieces fit together and of the overall process of how to implement a neural network learning algorithm. When training a neural network, the first thing you need to do is pick some network architecture and by architecture I just mean connectivity pattern between the neurons. So, you know, we might choose between say, a neural network with three input units and five hidden units and four output units versus one of 3, 5 hidden, 5 hidden, 4 output and here are 3, 5, 5, 5 units in each of three hidden layers and four open units, and so these choices of how many hidden units in each layer and how many hidden layers, those are architecture choices. So, how do you make these choices? Well first, the number of input units well that's pretty well defined. And once you decides on the fix set of features x the number of input units will just be, you know, the dimension of your features x(i) would be determined by that. And if you are doing multiclass classifications the number of output of this will be determined by the number of classes in your classification problem. And just a reminder if you have a multiclass classification where y takes on say values between 1 and 10, so that you have ten possible classes. Then remember to right, your output y as these were the vectors. So instead of clause one, you recode it as a vector like that, or for the second class you recode it as a vector like that. So if one of these apples takes on the fifth class, you know, y equals 5, then what you're showing to your neural network is not actually a value of y equals 5, instead here at the upper layer which would have ten output units, you will instead feed to the vector which you know with one in the fifth position and a bunch of zeros down here. So the choice of number of input units and number of output units is maybe somewhat reasonably straightforward. And as for the number of hidden units and the number of hidden layers, a reasonable default is to use a single hidden layer and so this type of neural network shown on the left with just one hidden layer is probably the most common. Or if you use more than one hidden layer, again the reasonable default will be to have the same number of hidden units in every single layer. So here we have two hidden layers and each of these hidden layers have the same number five of hidden units and here we have, you know, three hidden layers and each of them has the same number, that is five hidden units. Rather than doing this sort of network architecture on the left would be a perfect ably reasonable default. And as for the number of hidden units - usually, the more hidden units the better; it's just that if you have a lot of hidden units, it can become more computationally expensive, but very often, having more hidden units is a good thing. And usually the number of hidden units in each layer will be maybe comparable to the dimension of x, comparable to the number of features, or it could be any where from same number of hidden units of input features to maybe so that three or four times of that. So having the number of hidden units is comparable. You know, several times, or some what bigger than the number of input features is often a useful thing to do So, hopefully this gives you one reasonable set of default choices for neural architecture and and if you follow these guidelines, you will probably get something that works well, but in a later set of videos where I will talk specifically about advice for how to apply algorithms, I will actually say a lot more about how to choose a neural network architecture. Or actually have quite a lot I want to say later to make good choices for the number of hidden units, the number of hidden layers, and so on. Next, here's what we need to implement in order to trade in neural network, there are actually six steps that I have; I have four on this slide and two more steps on the next slide. First step is to set up the neural network and to randomly initialize the values of the weights. And we usually initialize the weights to small values near zero. Then we implement forward propagation so that we can input any excellent neural network and compute h of x which is this output vector of the y values. We then also implement code to compute this cost function j of theta. And next we implement back-prop, or the back-propagation algorithm, to compute these partial derivatives terms, partial derivatives of j of theta with respect to the parameters. Concretely, to implement back prop. Usually we will do that with a fore loop over the training examples. Some of you may have heard of advanced, and frankly very advanced factorization methods where you don't have a four-loop over the m-training examples, that the first time you're implementing back prop there should almost certainly the four loop in your code, where you're iterating over the examples, you know, x1, y1, then so you do forward prop and back prop on the first example, and then in the second iteration of the four-loop, you do forward propagation and back propagation on the second example, and so on. Until you get through the final example. So there should be a four-loop in your implementation of back prop, at least the first time implementing it. And then there are frankly somewhat complicated ways to do this without a four-loop, but I definitely do not recommend trying to do that much more complicated version the first time you try to implement back prop. So concretely, we have a four-loop over my m-training examples and inside the four-loop we're going to perform fore prop and back prop using just this one example. And what that means is that we're going to take x(i), and feed that to my input layer, perform forward-prop, perform back-prop and that will if all of these activations and all of these delta terms for all of the layers of all my units in the neural network then still inside this four-loop, let me draw some curly braces just to show the scope with the four-loop, this is in octave code of course, but it's more a sequence Java code, and a four-loop encompasses all this. We're going to compute those delta terms, which are is the formula that we gave earlier. Plus, you know, delta l plus one times a, l transpose of the code. And then finally, outside the having computed these delta terms, these accumulation terms, we would then have some other code and then that will allow us to compute these partial derivative terms. Right and these partial derivative terms have to take into account the regularization term lambda as well. And so, those formulas were given in the earlier video. So, how do you done that you now hopefully have code to compute these partial derivative terms. Next is step five, what I do is then use gradient checking to compare these partial derivative terms that were computed. So, I've compared the versions computed using back propagation versus the partial derivatives computed using the numerical estimates as using numerical estimates of the derivatives. So, I do gradient checking to make sure that both of these give you very similar values. Having done gradient checking just now reassures us that our implementation of back propagation is correct, and is then very important that we disable gradient checking, because the gradient checking code is computationally very slow. And finally, we then use an optimization algorithm such as gradient descent, or one of the advanced optimization methods such as LB of GS, contract gradient has embodied into fminunc or other  optimization methods. We use these together with back propagation, so back propagation is the thing that computes these partial derivatives for us. And so, we know how to compute the cost function, we know how to compute the partial derivatives using back propagation, so we can use one of these optimization methods to try to minimize j of theta as a function of the parameters theta. And by the way, for neural networks, this cost function j of theta is non-convex, or is not convex and so it can theoretically be susceptible to local minima, and in fact algorithms like gradient descent and the advance optimization methods can, in theory, get stuck in local optima, but it turns out that in practice this is not usually a huge problem and even though we can't guarantee that these algorithms will find a global optimum, usually algorithms like gradient descent will do a very good job minimizing this cost function j of theta and get a very good local minimum, even if it doesn't get to the global optimum. Finally, gradient descents for a neural network might still seem a little bit magical. So, let me just show one more figure to try to get that intuition about what gradient descent for a neural network is doing. This was actually similar to the figure that I was using earlier to explain gradient descent. So, we have some cost function, and we have a number of parameters in our neural network. Right here I've just written down two of the parameter values. In reality, of course, in the neural network, we can have lots of parameters with these. Theta one, theta two--all of these are matrices, right? So we can have very high dimensional parameters but because of the limitations the source of parts we can draw. I'm pretending that we have only two parameters in this neural network. Although obviously we have a lot more in practice. Now, this cost function j of theta measures how well the neural network fits the training data. So, if you take a point like this one, down here, that's a point where j of theta is pretty low, and so this corresponds to a setting of the parameters. There's a setting of the parameters theta, where, you know, for most of the training examples, the output of my hypothesis, that may be pretty close to y(i) and if this is true than that's what causes my cost function to be pretty low. Whereas in contrast, if you were to take a value like that, a point like that corresponds to, where for many training examples, the output of my neural network is far from the actual value y(i) that was observed in the training set. So points like this on the line correspond to where the hypothesis, where the neural network is outputting values on the training set that are far from y(i). So, it's not fitting the training set well, whereas points like this with low values of the cost function corresponds to where j of theta is low, and therefore corresponds to where the neural network happens to be fitting my training set well, because I mean this is what's needed to be true in order for j of theta to be small. So what gradient descent does is we'll start from some random initial point like that one over there, and it will repeatedly go downhill. And so what back propagation is doing is computing the direction of the gradient, and what gradient descent is doing is it's taking little steps downhill until hopefully it gets to, in this case, a pretty good local optimum. So, when you implement back propagation and use gradient descent or one of the advanced optimization methods, this picture sort of explains what the algorithm is doing. It's trying to find a value of the parameters where the output values in the neural network closely matches the values of the y(i)'s observed in your training set. So, hopefully this gives you a better sense of how the many different pieces of neural network learning fit together. In case even after this video, in case you still feel like there are, like, a lot of different pieces and it's not entirely clear what some of them do or how all of these pieces come together, that's actually okay. Neural network learning and back propagation is a complicated algorithm. And even though I've seen the math behind back propagation for many years and I've used back propagation, I think very successfully, for many years, even today I still feel like I don't always have a great grasp of exactly what back propagation is doing sometimes. And what the optimization process looks like of minimizing j if theta. Much this is a much harder algorithm to feel like I have a much less good handle on exactly what this is doing compared to say, linear regression or logistic regression. Which were mathematically and conceptually much simpler and much cleaner algorithms. But so in case if you feel the same way, you know, that's actually perfectly okay, but if you do implement back propagation, hopefully what you find is that this is one of the most powerful learning algorithms and if you implement this algorithm, implement back propagation, implement one of these optimization methods, you find that back propagation will be able to fit very complex, powerful, non-linear functions to your data, and this is one of the most effective learning algorithms we have today.""",57,0,1
coursera,stanford_university,machine-learning,autonomous-driving,"b""In this video, I'd like to show you\na fun and historically important example of neural networks learning of using\na neural network for autonomous driving. That is getting a car to\nlearn to drive itself. The video that I'll showed a minute\nwas something that I'd gotten from Dean Pomerleau, who was a colleague who\nworks out in Carnegie Mellon University out on the east coast\nof the United States. And in part of the video you\nsee visualizations like this. And I want to tell what a visualization\nlooks like before starting the video. Down here on the lower left is the view\nseen by the car of what's in front of it. And so here you kinda see a road that's\nmaybe going a bit to the left, and then going a little bit to the right. And up here on top, this first horizontal bar shows the\ndirection selected by the human driver. And this location of this bright white\nband that shows the steering direction selected by the human driver where you\nknow here far to the left corresponds to steering hard left, here corresponds\nto steering hard to the right. And so this location which is a little bit\nto the left, a little bit left of center means that the human driver at this\npoint was steering slightly to the left. And this second bot here corresponds\nto the steering direction selected by the learning algorithm and again the location of this sort of white\nband means that the neural network was here selecting a steering direction\nthat's slightly to the left. And in fact before the neural\nnetwork starts leaning initially, you see that the network outputs\na grey band, like a grey, like a uniform grey band throughout this\nregion and sort of a uniform gray fuzz corresponds to the neural network\nhaving been randomly initialized. And initially having no\nidea how to drive the car. Or initially having no idea of\nwhat direction to steer in. And is only after it has learned for a while, that will then start to output\nlike a solid white band in just a small part of the region corresponding to\nchoosing a particular steering direction. And that corresponds to when the neural\nnetwork becomes more confident in selecting a band in\none particular location, rather than outputting a sort of light\ngray fuzz, but instead outputting a white band that's more constantly\nselecting one's steering direction. >> ALVINN is a system of\nartificial neural networks that learns to steer by\nwatching a person drive. ALVINN is designed to\ncontrol the NAVLAB 2, a modified Army Humvee who\nhad put sensors, computers, and actuators for\nautonomous navigation experiments. The initial step in configuring ALVINN\nis creating a network just here. During training, a person drives\nthe vehicle while ALVINN watches. Once every two seconds, ALVINN digitizes\na video image of the road ahead, and records the person's steering direction. This training image is reduced in\nresolution to 30 by 32 pixels and provided as input to ALVINN's\nthree layered network. Using the back propagation learning\nalgorithm,ALVINN is training to output the same steering direction as\nthe human driver for that image. Initially the network\nsteering response is random. After about two minutes of training\nthe network learns to accurately imitate the steering reactions\nof the human driver. This same training procedure is\nrepeated for other road types. After the networks have been trained\nthe operator pushes the run switch and ALVINN begins driving. Twelve times per second,\nALVINN digitizes the image and feeds it to its neural networks. Each network, running in parallel,\nproduces a steering direction, and a measure of its'\nconfidence in its' response. The steering direction,\nfrom the most confident network, in this network training for the one lane\nroad, is used to control the vehicle. Suddenly an intersection\nappears ahead of the vehicle. As the vehicle approaches the intersection\nthe confidence of the lone lane network decreases. As it crosses the intersection and\nthe two lane road ahead comes into view, the confidence of the two\nlane network rises. When its' confidence rises the two\nlane network is selected to steer. Safely guiding the vehicle into\nits lane onto the two lane road. >> So that was autonomous driving\nusing the neural network. Of course there are more recently more\nmodern attempts to do autonomous driving. There are few projects in the US and\nEurope and so on, that are giving more robust\ndriving controllers than this, but I think it's still pretty remarkable and\npretty amazing how instant neural network trained with backpropagation can actually\nlearn to drive a car somewhat well.""",58,0,1
coursera,stanford_university,machine-learning,deciding-what-to-try-next,"b'By now you have seen a lot of different learning algorithms. And if you\'ve been following along these videos you should consider yourself an expert on many state-of-the-art machine learning techniques. But even among people that know a certain learning algorithm. There\'s often a huge difference between someone that really knows how to powerfully and effectively apply that algorithm, versus someone that\'s less familiar with some of the material that I\'m about to teach and who doesn\'t really understand how to apply these algorithms and can end up wasting a lot of their time trying things out that don\'t really make sense. What I would like to do is make sure that if you are developing machine learning systems, that you know how to choose one of the most promising avenues to spend your time pursuing. And on this and the next few videos I\'m going to give a number of practical suggestions, advice, guidelines on how to do that. And concretely what we\'d focus on is the problem of, suppose you are developing a machine learning system or trying to improve the performance of a machine learning system, how do you go about deciding what are the proxy avenues to try next? To explain this, let\'s continue using our example of learning to predict housing prices. And let\'s say you\'ve implement and regularize linear regression. Thus minimizing that cost function j.  Now suppose that after you take your learn parameters, if you test your hypothesis on the new set of houses, suppose you find that this is making huge errors in this prediction of the housing prices. The question is what should you then try mixing in order to improve the learning algorithm? There are many things that one can think of that could improve the performance of the learning algorithm. One thing they could try, is to get more training examples. And concretely, you can imagine, maybe, you know, setting up phone surveys, going door to door, to try to get more data on how much different houses sell for. And the sad thing is I\'ve seen a lot of people spend a lot of time collecting more training examples, thinking oh, if we have twice as much or ten times as much training data, that is certainly going to help, right? But sometimes getting more training data doesn\'t actually help and in the next few videos we will see why, and we will see how you can avoid spending a lot of time collecting more training data in settings where it is just not going to help. Other things you might try are to well maybe try a smaller set of features. So if you have some set of features such as x1, x2, x3 and so on, maybe a large number of features. Maybe you want to spend time carefully selecting some small subset of them to prevent overfitting. Or maybe you need to get additional features. Maybe the current set of features aren\'t informative enough and you want to collect more data in the sense of getting more features. And once again this is the sort of project that can scale up the huge projects can you imagine getting phone surveys to find out more houses, or extra land surveys to find out more about the pieces of land and so on, so a huge project. And once again it would be nice to know in advance if this is going to help before we spend a lot of time doing something like this. We can also try adding polynomial features things like x2 square x2 square and product features x1, x2. We can still spend quite a lot of time thinking about that and we can also try other things like decreasing lambda, the regularization parameter or increasing lambda. Given a menu of options like these, some of which can easily scale up to six month or longer projects. Unfortunately, the most common method that people use to pick one of these is to go by gut feeling. In which what many people will do is sort of randomly pick one of these options and maybe say, ""Oh, lets go and get more training data."" And easily spend six months collecting more training data or maybe someone else would rather be saying, ""Well, let\'s go collect a lot more features on these houses in our data set."" And I have a lot of times, sadly seen people spend, you know, literally 6 months doing one of these avenues that they have sort of at random only to discover six months later that that really wasn\'t a promising avenue to pursue. Fortunately, there is a pretty simple technique that can let you very quickly rule out half of the things on this list as being potentially promising things to pursue. And there is a very simple technique, that if you run, can easily rule out many of these options, and potentially save you a lot of time pursuing something that\'s just is not going to work. In the next two videos after this, I\'m going to first talk about how to evaluate learning algorithms. And in the next few videos after that, I\'m going to talk about these techniques, which are called the machine learning diagnostics. And what a diagnostic is, is a test you can run, to get insight into what is or isn\'t working with an algorithm, and which will often give you insight as to what are promising things to try to improve a learning algorithm\'s performance. We\'ll talk about specific diagnostics later in this video sequence. But I should mention in advance that diagnostics can take time to implement and can sometimes, you know, take quite a lot of time to implement and understand but doing so can be a very good use of your time when you are developing learning algorithms because they can often save you from spending many months pursuing an avenue that you could have found out much earlier just was not going to be fruitful. So in the next few videos, I\'m going to first talk about how evaluate your learning algorithms and after that I\'m going to talk about some of these diagnostics which will hopefully let you much more effectively select more of the useful things to try mixing if your goal to improve the machine learning system.'",59,0,1
coursera,stanford_university,machine-learning,evaluating-a-hypothesis,"b'In this video, I would like\nto talk about how to evaluate a hypothesis that has\nbeen learned by your algorithm. In later videos,\nwe will build on this to talk about how to\nprevent in the problems of overfitting and underfitting as well. When we fit the parameters\nof our learning algorithm we think about choosing the parameters\nto minimize the training error. One might think that\ngetting a really low value of training error\nmight be a good thing, but we have already seen that just because a hypothesis\nhas low training error, that doesn\'t mean it is\nnecessarily a good hypothesis. And we\'ve already seen the\nexample of how a hypothesis can overfit. And therefore fail to generalize the\nnew examples not in the training set. So how do you tell if\nthe hypothesis might be overfitting. In this simple example \nwe could plot the hypothesis h of x and just see what was going on. But in general for problems with\nmore features than just one feature, for problems with a large\nnumber of features like these it becomes hard or\nmay be impossible to plot what the hypothesis looks like and so we need some other way\nto evaluate our hypothesis. The standard way to evaluate\na learned hypothesis is as follows. Suppose we have\na data set like this. Here I have just shown \n10 training examples, but of course usually we may have dozens or hundreds or maybe\nthousands of training examples. In order to make sure we\ncan evaluate our hypothesis, what we are going to do is split the data we have into two portions. The first portion is going \nto be our usual training set and the second portion \nis going to be our test set, and a pretty typical split of this all the data we have into a\ntraining set and test set might be around\nsay a 70%, 30% split. Worth more today to\ngrade the training set and relatively less to the test set. And so now, if\nwe have some data set, we run a sine of say 70% of the data to be our\ntraining set where here ""m"" is as usual our\nnumber of training examples and the remainder of our data might then be assigned\nto become our test set. And here, I\'m going to use\nthe notation m subscript test to denote the number of test examples. And so in general, this\nsubscript test is going to denote examples that come\nfrom a test set so that x1 subscript test, \ny1 subscript test is my first test example which \nI guess in this example might be this example over here. Finally, one last detail whereas here I\'ve drawn this\nas though the first 70% goes to the training set and\nthe last 30% to the test set. If there is any sort of\nordinary to the data. That should be better\nto send a random 70% of your data to\nthe training set and a random 30% of\nyour data to the test set. So if your data were\nalready randomly sorted, you could just take\nthe first 70% and last 30% that if your data\nwere not randomly ordered, it would be better to randomly shuffle or to randomly reorder\nthe examples in your training set. Before you know sending\nthe first 70% in the training set and the last 30% of the test set. Here then is a\nfairly typical procedure for how you\nwould train and test the learning algorithm\nand the learning regression. First, you learn the parameters\ntheta from the training set so you minimize the usual \ntraining error objective j of theta, where j of theta \nhere was defined using that 70% of all the data you have. There is only the training data. And then you would\ncompute the test error. And I am going to denote\nthe test error as j subscript test. And so what you do is\ntake your parameter theta that you have learned from \nthe training set, and plug it in here and compute your test set error. Which I am going to write as follows. So this is basically the average squared error as measured on your test set. It\'s pretty much what you\'d expect. So if we run every test\nexample through your hypothesis with parameter theta and \njust measure the squared error that your hypothesis has on\nyour m subscript test, test examples. And of course, this is\nthe definition of the test set error if \nwe are using linear regression and using\nthe squared error metric. How about if we were doing\na classification problem and say using\nlogistic regression instead. In that case,\nthe procedure for training and testing say\nlogistic regression is pretty similar first we will do the parameters\nfrom the training data, that first 70% of the data. And it will compute\nthe test error as follows. It\'s the same objective function as we always use but\nwe just logistic regression, except that now is define using our m subscript test,\ntest examples. While this definition of\nthe test set error j subscript test is\nperfectly reasonable. Sometimes there is an alternative test sets metric that\nmight be easier to interpret, and that\'s the misclassification error. It\'s also called the zero one\nmisclassification error, with zero one denoting that you either get an example right\nor you get an example wrong. Here\'s what I mean. Let me define\nthe error of a prediction. That is h of x. And given the label y as equal to one\nif my hypothesis outputs the value\ngreater than equal to five and Y is equal to zero or if my hypothesis outputs \na value of less than 0.5 and y is equal to one, right, so both of these\ncases basic respond to if your hypothesis\nmislabeled the example assuming your threshold at an 0.5. So either thought it was more\nlikely to be 1, but it was actually 0, or your hypothesis\nstored was more likely to be 0, but the\nlabel was actually 1. And otherwise, we define\nthis error function to be zero. If your hypothesis basically\nclassified the example y correctly. We could then\ndefine the test error, using the misclassification\nerror metric to be one of the m tests\nof sum from i equals one to m subscript test of the error of h of x(i) test comma y(i). And so that\'s just my way of\nwriting out that this is exactly the fraction of\nthe examples in my test set that my hypothesis has mislabeled. And so that\'s the definition of the test set error using the\nmisclassification error of the 0 1 \nmisclassification metric. So that\'s the standard\ntechnique for evaluating how good a learned hypothesis is. In the next video, \nwe will adapt these ideas to helping us do things\nlike choose what features like the degree polynomial\nto use with the learning algorithm or choose the regularization\nparameter for learning algorithm.'",60,0,1
coursera,stanford_university,machine-learning,model-selection-and-train-validation-test-sets,"b""Suppose you're left to decide what degree\nof polynomial to fit to a data set. So that what features to include\nthat gives you a learning algorithm. Or suppose you'd like to choose\nthe regularization parameter longer for learning algorithm. How do you do that? This account model selection process. Browsers, and in our discussion of how to\ndo this, we'll talk about not just how to split your data into the train and test\nsets, but how to switch data into what we discover is called the train,\nvalidation, and test sets. We'll see in this video just\nwhat these things are, and how to use them to do model selection. We've already seen a lot of times\nthe problem of overfitting, in which just because a learning\nalgorithm fits a training set well, that doesn't mean it's a good hypothesis. More generally, this is why the training\nset's error is not a good predictor for how well the hypothesis\nwill do on new example. Concretely, if you fit\nsome set of parameters. Theta0, theta1, theta2, and\nso on, to your training set. Then the fact that your hypothesis\ndoes well on the training set. Well, this doesn't mean much in terms of\npredicting how well your hypothesis will generalize to new examples\nnot seen in the training set. And a more general\nprinciple is that once your parameter is what fit to some set of data. Maybe the training set,\nmaybe something else. Then the error of your hypothesis\nas measured on that same data set, such as the training error, that's unlikely to be a good estimate\nof your actual generalization error. That is how well the hypothesis\nwill generalize to new examples. Now let's consider the model\nselection problem. Let's say you're trying to choose what\ndegree polynomial to fit to data. So, should you choose a linear function,\na quadratic function, a cubic function? All the way up to a 10th-order polynomial. So it's as if there's one extra\nparameter in this algorithm, which I'm going to denote d,\nwhich is, what degree of polynomial. Do you want to pick. So it's as if, in addition to\nthe theta parameters, it's as if there's one more parameter, d, that you're\ntrying to determine using your data set. So, the first option is d equals one,\nif you fit a linear function. We can choose d equals two, d equals\nthree, all the way up to d equals 10. So, we'd like to fit this extra sort\nof parameter which I'm denoting by d. And concretely let's say that\nyou want to choose a model, that is choose a degree of polynomial,\nchoose one of these 10 models. And fit that model and\nalso get some estimate of how well your fitted hypothesis was\ngeneralize to new examples. Here's one thing you could do. What you could, first take your first\nmodel and minimize the training error. And this would give you some\nparameter vector theta. And you could then take your second model,\nthe quadratic function, and fit that to your training set and\nthis will give you some other. Parameter vector theta. In order to distinguish between these\ndifferent parameter vectors, I'm going to use a superscript one superscript\ntwo there where theta superscript one just means the parameters I get by\nfitting this model to my training data. And theta superscript two\njust means the parameters I get by fitting this quadratic function\nto my training data and so on. By fitting a cubic model I get parenthesis\nthree up to, well, say theta 10. And one thing we ccould do is that take\nthese parameters and look at test error. So I can compute on my\ntest set J test of one, J test of theta two, and so on. J test of theta three, and so on. So I'm going to take each of my hypotheses\nwith the corresponding parameters and just measure the performance\nof on the test set. Now, one thing I could do then is,\nin order to select one of these models, I could then see which model\nhas the lowest test set error. And let's just say for this example that I ended up\nchoosing the fifth order polynomial. So, this seems reasonable so far. But now let's say I want to take\nmy fifth hypothesis, this, this, fifth order model, and let's say I want to\nask, how well does this model generalize? One thing I could do is look\nat how well my fifth order polynomial hypothesis\nhad done on my test set. But the problem is this will not\nbe a fair estimate of how well my hypothesis generalizes. And the reason is what we've done is\nwe've fit this extra parameter d, that is this degree of polynomial. And what fits that parameter d,\nusing the test set, namely, we chose the value of d that gave us the\nbest possible performance on the test set. And so, the performance of my parameter\nvector theta5, on the test set, that's likely to be an overly optimistic\nestimate of generalization error. Right, so, that because I had fit this\nparameter d to my test set is no longer fair to evaluate my hypothesis on this\ntest set, because I fit my parameters to this test set, I've chose the degree\nd of polynomial using the test set. And so\nmy hypothesis is likely to do better on this test set than it would on new\nexamples that it hasn't seen before, and that's which is,\nwhich is what I really care about. So just to reiterate, on the previous\nslide, we saw that if we fit some set of parameters, you know, say theta0,\ntheta1, and so on, to some training set, then the performance of the fitted model\non the training set is not predictive of how well the hypothesis will\ngeneralize to new examples. Is because these parameters\nwere fit to the training set, so they're likely to do\nwell on the training set, even if the parameters don't\ndo well on other examples. And, in the procedure I just described on\nthis line, we just did the same thing. And specifically, what we did was,\nwe fit this parameter d to the test set. And by having fit the parameter\nto the test set, this means that the performance of the hypothesis on that\ntest set may not be a fair estimate of how well the hypothesis is, is likely to\ndo on examples we haven't seen before. To address this problem,\nin a model selection setting, if we want to evaluate a hypothesis,\nthis is what we usually do instead. Given the data set, instead of just\nsplitting into a training test set, what we're going to do is then\nsplit it into three pieces. And the first piece is going to be\ncalled the training set as usual. So let me call this first\npart the training set. And the second piece of this data, I'm\ngoing to call the cross validation set. [SOUND] Cross validation. And the cross validation, as V-D. Sometimes it's also called the validation\nset instead of cross validation set. And then the loss can be\nto call the usual test set. And the pretty, pretty typical ratio\nat which to split these things will be to send 60% of your data's,\nyour training set, maybe 20% to your cross validation set,\nand 20% to your test set. And these numbers can vary a little bit\nbut this integration be pretty typical. And so our training sets will now be\nonly maybe 60% of the data, and our cross-validation set, or our validation\nset, will have some number of examples. I'm going to denote that m subscript cv. So that's the number of\ncross-validation examples. Following our early notational convention\nI'm going to use xi cv comma y i cv, to denote the i cross validation example. And finally we also have\na test set over here with our m subscript test being\nthe number of test examples. So, now that we've defined\nthe training validation or cross validation and test sets. We can also define the training error,\ncross validation error, and test error. So here's my training error, and I'm just writing this as J\nsubscript train of theta. This is pretty much the same things. These are the same thing as the J\nof theta that I've been writing so far, this is just a training set error you\nknow, as measuring a training set and then J subscript cv my cross validation error,\nthis is pretty much what you'd expect, just like the training error you've set\nmeasure it on a cross validation data set, and here's my test set\nerror same as before. So when faced with a model selection\nproblem like this, what we're going to do is, instead of using the test set\nto select the model, we're instead going to use the validation set, or the\ncross validation set, to select the model. Concretely, we're going to first take our\nfirst hypothesis, take this first model, and say, minimize the cross function, and this would give me some parameter\nvector theta for the new model. And, as before,\nI'm going to put a superscript 1, just to denote that this is\nthe parameter for the new model. We do the same thing for\nthe quadratic model. Get some parameter vector theta two. Get some para,\nparameter vector theta three, and so on, down to theta ten for the polynomial. And what I'm going to do is, instead of\ntesting these hypotheses on the test set, I'm instead going to test them\non the cross validation set. And measure J subscript cv, to see how well each of these hypotheses\ndo on my cross validation set. And then I'm going to pick the hypothesis\nwith the lowest cross validation error. So for this example, let's say for\nthe sake of argument, that it was my 4th order polynomial, that\nhad the lowest cross validation error. So in that case I'm going to pick\nthis fourth order polynomial model. And finally,\nwhat this means is that that parameter d, remember d was the degree of polynomial,\nright? So d equals two, d equals three,\nall the way up to d equals 10. What we've done is we'll fit that\nparameter d and we'll say d equals four. And we did so\nusing the cross-validation set. And so this degree of polynomial, so the\nparameter, is no longer fit to the test set, and so we've not saved away the test\nset, and we can use the test set to measure, or to estimate the generalization\nerror of the model that was selected. By the of them. So, that was model selection and\nhow you can take your data, split it into a training,\nvalidation, and test set. And use your cross validation\ndata to select the model and evaluate it on the test set. One final note, I should say that in. The machine learning, as of this\npractice today, there aren't many people that will do that early thing that\nI talked about, and said that, you know, it isn't such a good idea, of selecting\nyour model using this test set. And then using the same test set\nto report the error as though selecting your degree of polynomial on the\ntest set, and then reporting the error on the test set as though that were a good\nestimate of generalization error. That sort of practice is unfortunately\nmany, many people do do it. If you have a massive, massive test that\nis maybe not a terrible thing to do, but many practitioners, most practitioners that machine\nlearnimg tend to advise against that. And it's considered better practice\nto have separate train validation and test sets. I just warned you to sometimes people\nto do, you know, use the same data for the purpose of the validation set,\nand for the purpose of the test set. You need a training set and\na test set, and that's good, that's practice,\nthough you will see some people do it. But, if possible, I would recommend\nagainst doing that yourself.""",61,0,1
coursera,stanford_university,machine-learning,diagnosing-bias-vs-variance,"b""If you run a learning algorithm and it doesn't do as long as you are hoping, almost all the time, it will be because you have either a high bias problem or a high variance problem, in other words, either an underfitting problem or an overfitting problem. In this case, it's very important to figure out which of these two problems is bias or variance or a bit of both that you actually have. Because knowing which of these two things is happening would give a very strong indicator for whether the useful and promising ways to try to improve your algorithm. In this video, I'd like to delve more deeply into this bias and variance issue and understand them better as was figure out how to look in a learning algorithm and evaluate or diagnose whether we might have a bias problem or a variance problem since this will be critical to figuring out how to improve the performance of a learning algorithm that you will implement. So, you've already seen this figure a few times where if you fit two simple hypothesis like a straight line that underfits the data, if you fit a two complex hypothesis, then that might fit the training set perfectly but overfit the data and this may be hypothesis of some intermediate level of complexities of some maybe degree two polynomials or not too low and not too high degree that's like just right and gives you the best generalization error over these options. Now that we're armed with the notion of chain training and validation in test sets, we can understand the concepts of bias and variance a little bit better. Concretely, let's let our training error and cross validation error be defined as in the previous videos. Just say the squared error, the average squared error, as measured on the training sets or as measured on the cross validation set. Now, let's plot the following figure. On the horizontal axis I'm going to plot the degree of polynomial. So, as I go to the right I'm going to be fitting higher and higher order polynomials. So where the left of this figure where maybe d equals one, we're going to be fitting very simple functions whereas we're here on the right of the horizontal axis, I have much larger values of ds, of a much higher degree polynomial. So here, that's going to correspond to fitting much more complex functions to your training set. Let's look at the training error and the cross validation error and plot them on this figure. Let's start with the training error. As we increase the degree of the polynomial, we're going to be able to fit our training set better and better and so if d equals one, then there is high training error, if we have a very high degree of polynomial our training error is going to be really low, maybe even 0 because will fit the training set really well. So, as we increase the degree of polynomial, we find typically that the training error decreases. So I'm going to write J subscript train of theta there, because our training error tends to decrease with the degree of the polynomial that we fit to the data. Next, let's look at the cross-validation error or for that matter, if we look at the test set error, we'll get a pretty similar result as if we were to plot the cross validation error. So, we know that if d equals one, we're fitting a very simple function and so we may be underfitting the training set and so it's going to be very high cross-validation error. If we fit an intermediate degree polynomial, we had d equals two in our example in the previous slide, we're going to have a much lower cross-validation error because we're finding a much better fit to the data. Conversely, if d were too high. So if d took on say a value of four, then we're again overfitting, and so we end up with a high value for cross-validation error. So, if you were to vary this smoothly and plot a curve, you might end up with a curve like that where that's JCV of theta. Again, if you plot J test of theta you get something very similar. So, this sort of plot also helps us to better understand the notions of bias and variance. Concretely, suppose you have applied a learning algorithm and it's not performing as well as you are hoping, so if your cross-validation set error or your test set error is high, how can we figure out if the learning algorithm is suffering from high bias or suffering from high variance? So, the setting of a cross-validation error being high corresponds to either this regime or this regime. So, this regime on the left corresponds to a high bias problem. That is, if you are fitting a overly low order polynomial such as a d equals one when we really needed a higher order polynomial to fit to data, whereas in contrast this regime corresponds to a high variance problem. That is, if d the degree of polynomial was too large for the data set that we have, and this figure gives us a clue for how to distinguish between these two cases. Concretely, for the high bias case, that is the case of underfitting, what we find is that both the cross validation error and the training error are going to be high. So, if your algorithm is suffering from a bias problem, the training set error will be high and you might find that the cross validation error will also be high. It might be close, maybe just slightly higher, than the training error. So, if you see this combination, that's a sign that your algorithm may be suffering from high bias. In contrast, if your algorithm is suffering from high variance, then if you look here, we'll notice that J train, that is the training error, is going to be low. That is, you're fitting the training set very well, whereas your cross validation error assuming that this is, say, the squared error which we're trying to minimize say, whereas in contrast your error on a cross validation set or your cross function or cross validation set will be much bigger than your training set error. So, this is a double greater than sign. That's the map symbol for much greater thans, denoted by two greater than signs. So if you see this combination of values, then that's a clue that your learning algorithm may be suffering from high variance and might be overfitting. The key that distinguishes these two cases is, if you have a high bias problem, your training set error will also be high is your hypothesis just not fitting the training set well. If you have a high variance problem, your training set error will usually be low, that is much lower than your cross-validation error. So hopefully that gives you a somewhat better understanding of the two problems of bias and variance. I still have a lot more to say about bias and variance in the next few videos, but what we'll see later is that by diagnosing whether a learning algorithm may be suffering from high bias or high variance, I'll show you even more details on how to do that in later videos. But we'll see that by figuring out whether a learning algorithm may be suffering from high bias or high variance or combination of both, that that would give us much better guidance for what might be promising things to try in order to improve the performance of a learning algorithm.""",62,0,1
coursera,stanford_university,machine-learning,regularization-and-bias-variance,"b""You've seen how regularization\ncan help prevent over-fitting. But how does it affect the bias and\nvariances of a learning algorithm? In this video I'd like to go\ndeeper into the issue of bias and variances and\ntalk about how it interacts with and is affected by the regularization\nof your learning algorithm. Suppose we're fitting a high auto\npolynomial, like that showed here, but to prevent over fitting we need to use\nregularization, like that shown here. So we have this regularization term to try\nto keep the values of the prem to small. And as usual, the regularizations comes\nfrom J = 1 to m, rather than j = 0 to m. Let's consider three cases. The first is the case of the very large\nvalue of the regularization parameter lambda, such as if lambda\nwere equal to 10,000. Some huge value. In this case, all of these parameters,\ntheta 1, theta 2, theta 3, and so on would be heavily penalized and so we end up with most of these\nparameter values being closer to zero. And the hypothesis will be roughly h of x, just equal or\napproximately equal to theta zero. So we end up with a hypothesis that\nmore or less looks like that, more or less a flat, constant straight line. And so this hypothesis has high bias and\nit badly under fits this data set, so the horizontal straight line is just\nnot a very good model for this data set. At the other extreme is if we have\na very small value of lambda, such as if lambda were equal to zero. In that case, given that we're\nfitting a high order polynomial, this is a usual over-fitting setting. In that case, given that we're fitting\na high-order polynomial, basically, without regularization or\nwith very minimal regularization, we end up with our usual high-variance,\nover fitting setting. This is basically if lambda is equal\nto zero, we're just fitting with our regularization, so\nthat over fits the hypothesis. And it's only if we have some\nintermediate value of longer that is neither too large nor\ntoo small that we end up with parameters data that give us\na reasonable fit to this data. So, how can we automatically choose a good\nvalue for the regularization parameter? Just to reiterate, here's our model, and\nhere's our learning algorithm's objective. For the setting where we're\nusing regularization, let me define J train(theta)\nto be something different, to be the optimization objective,\nbut without the regularization term. Previously, in an earlier video, when we\nwere not using regularization I define J train of data to be the same as J of theta\nas the cause function but when we're using regularization when the six well under\nterm we're going to define J train my training set to be just my sum of squared\nerrors on the training set or my average squared error on the training set without\ntaking into account that regularization. And similarly I'm then also going to\ndefine the cross validation sets error and to test that error as before to be the\naverage sum of squared errors on the cross validation in the test sets so just to\nsummarize my definitions of J train J CU and J test are just the average square\nthere one half of the other square record on the training validation of the test set\nwithout the extra regularization term. So, this is how we can automatically\nchoose the regularization parameter lambda. So what I usually do is maybe have some\nrange of values of lambda I want to try out. So I might be considering not using\nregularization or here are a few values I might try lambda considering lambda\n= 0.01, 0.02, 0.04, and so on. And I usually set these up in multiples\nof two, until some maybe larger value if I were to do these in multiples\nof 2 I'd end up with a 10.24. It's 10 exactly, but this is close enough. And the three to four decimal places\nwon't effect your result that much. So, this gives me maybe\n12 different models. And I'm trying to select a month\ncorresponding to 12 different values of the regularization of\nthe parameter lambda. And of course you can also go\nto values less than 0.01 or values larger than 10 but I've just\ntruncated it here for convenience. Given the issue of these 12 models,\nwhat we can do is then the following, we can take this first model with\nlambda equals zero and minimize my cost function J of data and this will\ngive me some parameter of active data. And similar to the earlier video, let me just denote this as\ntheta super script one. And then I can take my second\nmodel with lambda set to 0.01 and minimize my cost function now using\nlambda equals 0.01 of course. To get some different\nparameter vector theta. Let me denote that theta(2). And for that I end up with theta(3). So if part for my third model. And so on until for\nmy final model with lambda set to 10 or 10.24, I end up with this theta(12). Next, I can talk all of these hypotheses,\nall of these parameters and use my cross validation\nset to validate them so I can look at my first model,\nmy second model, fit to these different values of\nthe regularization parameter, and evaluate them with my cross validation set\nbased in measure the average square error of each of these square vector parameters\ntheta on my cross validation sets. And I would then pick whichever one of\nthese 12 models gives me the lowest error on the trans validation set. And let's say, for the sake of this\nexample, that I end up picking theta 5, the 5th order polynomial, because that\nhas the lowest cause validation error. Having done that, finally what I would do\nif I wanted to report each test set error, is to take the parameter theta\n5 that I've selected, and look at how well it does on my test set. So once again, here is as if\nwe've fit this parameter, theta, to my cross-validation set,\nwhich is why I'm setting aside a separate test set that I'm going to\nuse to get a better estimate of how well my parameter vector, theta, will\ngeneralize to previously unseen examples. So that's model selection\napplied to selecting the regularization parameter lambda. The last thing I'd like to do in this\nvideo is get a better understanding of how cross validation and training error vary as we vary\nthe regularization parameter lambda. And so just a reminder right,\nthat was our original cost on j of theta. But for this purpose we're going to define training error without using\na regularization parameter, and cross validation error without\nusing the regularization parameter. And what I'd like to do is plot\nthis Jtrain and plot this Jcv, meaning just how well does my\nhypothesis do on the training set and how does my hypothesis do when\nit cross validation sets. As I vary my regularization\nparameter lambda. So as we saw earlier if lambda is small\nthen we're not using much regularization and we run a larger risk of\nover fitting whereas if lambda is large that is if we were on the right\npart of this horizontal axis then, with a large value of lambda, we run the\nhigher risk of having a biased problem, so if you plot J train and J cv, what you\nfind is that, for small values of lambda, you can fit the trading set relatively\nway cuz you're not regularizing. So, for small values of lambda, the\nregularization term basically goes away, and you're just minimizing\npretty much just gray arrows. So when lambda is small,\nyou end up with a small value for Jtrain, whereas if lambda is large,\nthen you have a high bias problem, and you might not feel your training that\nwell, so you end up the value up there. So Jtrain of theta will tend to\nincrease when lambda increases, because a large value of lambda\ncorresponds to high bias where you might not even fit your\ntrainings that well, whereas a small value of\nlambda corresponds to, if you can really fit a very high degree\npolynomial to your data, let's say. After the cost validation error we\nend up with a figure like this, where over here on the right,\nif we have a large value of lambda, we may end up under fitting,\nand so this is the bias regime. And so\nthe cross validation error will be high. Let me just leave all of that to this\nJcv (theta) because so, with high bias, we won't be fitting, we won't be\ndoing well in cross validation sets, whereas here on the left,\nthis is the high variance regime, where we have two smaller value with longer,\nthen we may be over fitting the data. And so by over fitting the data, then the\ncross validation error will also be high. And so, this is what the cross validation\nerror and what the trading error may look like on a trading stance as we\nvary the regularization parameter lambda. And so once again, it will often be some\nintermediate value of lambda that is just right or that works best In terms of\nhaving a small cross validation error or a small test theta. And whereas the curves I've drawn\nhere are somewhat cartoonish and somewhat idealized so on the real data\nset the curves you get may end up looking a little bit more messy and\njust a little bit more noisy then this. For some data sets you will really\nsee these for sorts of trends and by looking at a plot of the hold-out cross\nvalidation error you can either manual, automatically try to select a point that\nminimizes the cross validation error and select the value of lambda corresponding\nto low cross validation error. When I'm trying to pick the regularization\nparameter lambda for learning algorithm, often I find that plotting a figure like\nthis one shown here helps me understand better what's going on and helps me verify\nthat I am indeed picking a good value for the regularization parameter monitor. So hopefully that gives you more\ninsight into regularization and it's effects on the bias and\nvariance of a learning algorithm. By now you've seen bias and variance\nfrom a lot of different perspectives. And what we like to do in the next video\nis take all the insights we've gone through and build on them to put together\na diagnostic that's called learning curves, which is a tool that I often use\nto diagnose if the learning algorithm may be suffering from a bias problem or a\nvariance problem, or a little bit of both.""",63,0,1
coursera,stanford_university,machine-learning,learning-curves,"b""In this video, I'd like to tell you about learning curves. Learning curves is often a very useful thing to plot. If either you wanted to sanity check that your algorithm is working correctly, or if you want to improve the performance of the algorithm. And learning curves is a tool that I actually use very often to try to diagnose if a physical learning algorithm may be suffering from bias, sort of variance problem or a bit of both. Here's what a learning curve is. To plot a learning curve, what I usually do is plot j train which is, say, average squared error on my training set or Jcv which is the average squared error on my cross validation set. And I'm going to plot that as a function of m, that is as a function of the number of training examples I have. And so m is usually a constant like maybe I just have, you know, a 100 training examples but what I'm going to do is artificially with use my training set exercise. So, I deliberately limit myself to using only, say, 10 or 20 or 30 or 40 training examples and plot what the training error is and what the cross validation is for this smallest training set exercises.\nSo let's see what these plots may look like. Suppose I have only one training example like that shown in this this first example here and let's say I'm fitting a quadratic function. Well, I have only one training example. I'm going to be able to fit it perfectly right? You know, just fit the quadratic function. I'm going to have 0 error on the one training example. If I have two training examples. Well the quadratic function can also fit that very well. So, even if I am using regularization, I can probably fit this quite well. And if I am using no neural regularization, I'm going to fit this perfectly and if I have three training examples again. Yeah, I can fit a quadratic function perfectly so if m equals 1 or m equals 2 or m equals 3, my training error on my training set is going to be 0 assuming I'm not using regularization or it may slightly large in 0 if I'm using regularization and by the way if I have a large training set and I'm artificially restricting the size of my training set in order to J train. Here if I set M equals 3, say, and I train on only three examples, then, for this figure I am going to measure my training error only on the three examples that actually fit my data too and so even I have to say a 100 training examples but if I want to plot what my training error is the m equals 3. What I'm going to do is to measure the training error on the three examples that I've actually fit to my hypothesis 2. And not all the other examples that I have deliberately omitted from the training process. So just to summarize what we've seen is that if the training set size is small then the training error is going to be small as well. Because you know, we have a small training set is going to be very easy to fit your training set very well may be even perfectly now say we have m equals 4 for example. Well then a quadratic function can be a longer fit this data set perfectly and if I have m equals 5 then you know, maybe quadratic function will fit to stay there so so, then as my training set gets larger. It becomes harder and harder to ensure that I can find the quadratic function that process through all my examples perfectly. So in fact as the training set size grows what you find is that my average training error actually increases and so if you plot this figure what you find is that the training set error that is the average error on your hypothesis grows as m grows and just to repeat when the intuition is that when m is small when you have very few training examples. It's pretty easy to fit every single one of your training examples perfectly and so your error is going to be small whereas when m is larger then gets harder all the training examples perfectly and so your training set error becomes more larger now, how about the cross validation error. Well, the cross validation is my error on this cross validation set that I haven't seen and so, you know, when I have a very small training set, I'm not going to generalize well, just not going to do well on that. So, right, this hypothesis here doesn't look like a good one, and it's only when I get a larger training set that, you know, I'm starting to get hypotheses that maybe fit the data somewhat better. So your cross validation error and your test set error will tend to decrease as your training set size increases because the more data you have, the better you do at generalizing to new examples. So, just the more data you have, the better the hypothesis you fit. So if you plot j train, and Jcv this is the sort of thing that you get. Now let's look at what the learning curves may look like if we have either high bias or high variance problems. Suppose your hypothesis has high bias and to explain this I'm going to use a, set an example, of fitting a straight line to data that, you know, can't really be fit well by a straight line. So we end up with a hypotheses that maybe looks like that. Now let's think what would happen if we were to increase the training set size. So if instead of five examples like what I've drawn there, imagine that we have a lot more training examples. Well what happens, if you fit a straight line to this. What you find is that, you end up with you know, pretty much the same straight line. I mean a straight line that just cannot fit this data and getting a ton more data, well the straight line isn't going to change that much. This is the best possible straight-line fit to this data, but the straight line just can't fit this data set that well. So, if you plot across validation error, this is what it will look like. Option on the left, if you have already a miniscule training set size like you know, maybe just one training example and is not going to do well. But by the time you have reached a certain number of training examples, you have almost fit the best possible straight line, and even if you end up with a much larger training set size, a much larger value of m, you know, you're basically getting the same straight line, and so, the cross-validation error - let me label that - or test set error or plateau out, or flatten out pretty soon, once you reached beyond a certain the number of training examples, unless you pretty much fit the best possible straight line. And how about training error? Well, the training error will again be small. And what you find in the high bias case is that the training error will end up close to the cross validation error, because you have so few parameters and so much data, at least when m is large. The performance on the training set and the cross validation set will be very similar. And so, this is what your learning curves will look like, if you have an algorithm that has high bias. And finally, the problem with high bias is reflected in the fact that both the cross validation error and the training error are high, and so you end up with a relatively high value of both Jcv and the j train. This also implies something very interesting, which is that, if a learning algorithm has high bias, as we get more and more training examples, that is, as we move to the right of this figure, we'll notice that the cross validation error isn't going down much, it's basically fattened up, and so if learning algorithms are really suffering from high bias. Getting more training data by itself will actually not help that much,and as our figure example in the figure on the right, here we had only five training. examples, and we fill certain straight line. And when we had a ton more training data, we still end up with roughly the same straight line. And so if the learning algorithm has high bias give me a lot more training data. That doesn't actually help you get a much lower cross validation error or test set error. So knowing if your learning algorithm is suffering from high bias seems like a useful thing to know because this can prevent you from wasting a lot of time collecting more training data where it might just not end up being helpful. Next let us look at the setting of a learning algorithm that may have high variance. Let us just look at the training error in a around if you have very smart training set like five training examples shown on the figure on the right and if we're fitting say a very high order polynomial, and I've written a hundredth degree polynomial which really no one uses, but just an illustration. And if we're using a fairly small value of lambda, maybe not zero, but a fairly small value of lambda, then we'll end up, you know, fitting this data very well that with a function that overfits this. So, if the training set size is small, our training error, that is, j train of theta will be small. And as this training set size increases a bit, you know, we may still be overfitting this data a little bit but it also becomes slightly harder to fit this data set perfectly, and so, as the training set size increases, we'll find that j train increases, because it is just a little harder to fit the training set perfectly when we have more examples, but the training set error will still be pretty low. Now, how about the cross validation error? Well, in high variance setting, a hypothesis is overfitting and so the cross validation error will remain high, even as we get you know, a moderate number of training examples and, so maybe, the cross validation error may look like that. And the indicative diagnostic that we have a high variance problem, is the fact that there's this large gap between the training error and the cross validation error. And looking at this figure. If we think about adding more training data, that is, taking this figure and extrapolating to the right, we can kind of tell that, you know the two curves, the blue curve and the magenta curve, are converging to each other. And so, if we were to extrapolate this figure to the right, then it seems it likely that the training error will keep on going up and the cross-validation error would keep on going down. And the thing we really care about is the cross-validation error or the test set error, right? So in this sort of figure, we can tell that if we keep on adding training examples and extrapolate to the right, well our cross validation error will keep on coming down. And, so, in the high variance setting, getting more training data is, indeed, likely to help. And so again, this seems like a useful thing to know if your learning algorithm is suffering from a high variance problem, because that tells you, for example that it may be be worth your while to see if you can go and get some more training data. Now, on the previous slide and this slide, I've drawn fairly clean fairly idealized curves. If you plot these curves for an actual learning algorithm, sometimes you will actually see, you know, pretty much curves, like what I've drawn here. Although, sometimes you see curves that are a little bit noisier and a little bit messier than this. But plotting learning curves like these can often tell you, can often help you figure out if your learning algorithm is suffering from bias, or variance or even a little bit of both. So when I'm trying to improve the performance of a learning algorithm, one thing that I'll almost always do is plot these learning curves, and usually this will give you a better sense of whether there is a bias or variance problem. And in the next video we'll see how this can help suggest specific actions is to take, or to not take, in order to try to improve the performance of your learning algorithm.""",64,0,1
coursera,stanford_university,machine-learning,deciding-what-to-do-next-revisited,"b""We've talked about how to evaluate learning algorithms, talked about model selection, talked a lot about bias and variance. So how does this help us figure out what are potentially fruitful, potentially not fruitful things to try to do to improve the performance of a learning algorithm. Let's go back to our original motivating example and go for the result. So here is our earlier example of maybe having fit regularized linear regression and finding that it doesn't work as well as we're hoping. We said that we had this menu of options. So is there some way to figure out which of these might be fruitful options? The first thing all of this was getting more training examples. What this is good for, is this helps to fix high variance. And concretely, if you instead have a high bias problem and don't have any variance problem, then we saw in the previous video that getting more training examples, while maybe just isn't going to help much at all. So the first option is useful only if you, say, plot the learning curves and figure out that you have at least a bit of a variance, meaning that the cross-validation error is, you know, quite a bit bigger than your training set error. How about trying a smaller set of features? Well, trying a smaller set of features, that's again something that fixes high variance. And in other words, if you figure out, by looking at learning curves or something else that you used, that have a high bias problem; then for goodness sakes, don't waste your time trying to carefully select out a smaller set of features to use. Because if you have a high bias problem, using fewer features is not going to help. Whereas in contrast, if you look at the learning curves or something else you figure out that you have a high variance problem, then, indeed trying to select out a smaller set of features, that might indeed be a very good use of your time. How about trying to get additional features, adding features, usually, not always, but usually we think of this as a solution for fixing high bias problems. So if you are adding extra features it's usually because your current hypothesis is too simple, and so we want to try to get additional features to make our hypothesis better able to fit the training set. And similarly, adding polynomial features; this is another way of adding features and so there is another way to try to fix the high bias problem. And, if concretely if your learning curves show you that you still have a high variance problem, then, you know, again this is maybe a less good use of your time. And finally, decreasing and increasing lambda. This are quick and easy to try, I guess these are less likely to be a waste of, you know, many months of your life. But decreasing lambda, you already know fixes high bias. In case this isn't clear to you, you know, I do encourage you to pause the video and think through this that convince yourself that decreasing lambda helps fix high bias, whereas increasing lambda fixes high variance. And if you aren't sure why this is the case, do pause the video and make sure you can convince yourself that this is the case. Or take a look at the curves that we were plotting at the end of the previous video and try to make sure you understand why these are the case. Finally, let us take everything we have learned and relate it back to neural networks and so, here is some practical advice for how I usually choose the architecture or the connectivity pattern of the neural networks I use. So, if you are fitting a neural network, one option would be to fit, say, a pretty small neural network with you know, relatively few hidden units, maybe just one hidden unit. If you're fitting a neural network, one option would be to fit a relatively small neural network with, say, relatively few, maybe only one hidden layer and maybe only a relatively few number of hidden units. So, a network like this might have relatively few parameters and be more prone to underfitting. The main advantage of these small neural networks is that the computation will be cheaper. An alternative would be to fit a, maybe relatively large neural network with either more hidden units--there's a lot of hidden in one there--or with more hidden layers. And so these neural networks tend to have more parameters and therefore be more prone to overfitting. One disadvantage, often not a major one but something to think about, is that if you have a large number of neurons in your network, then it can be more computationally expensive. Although within reason, this is often hopefully not a huge problem. The main potential problem of these much larger neural networks is that it could be more prone to overfitting and it turns out if you're applying neural network very often using a large neural network often it's actually the larger, the better but if it's overfitting, you can then use regularization to address overfitting, usually using a larger neural network by using regularization to address is overfitting that's often more effective than using a smaller neural network. And the main possible disadvantage is that it can be more computationally expensive. And finally, one of the other decisions is, say, the number of hidden layers you want to have, right? So, do you want one hidden layer or do you want three hidden layers, as we've shown here, or do you want two hidden layers? And usually, as I think I said in the previous video, using a single hidden layer is a reasonable default, but if you want to choose the number of hidden layers, one other thing you can try is find yourself a training cross-validation, and test set split and try training neural networks with one hidden layer or two hidden layers or three hidden layers and see which of those neural networks performs best on the cross-validation sets. You take your three neural networks with one, two and three hidden layers, and compute the cross validation error at Jcv and all of them and use that to select which of these is you think the best neural network. So, that's it for bias and variance and ways like learning curves, who tried to diagnose these problems. As far as what you think is implied, for one might be truthful or not truthful things to try to improve the performance of a learning algorithm. If you understood the contents of the last few videos and if you apply them you actually be much more effective already and getting learning algorithms to work on problems and even a large fraction, maybe the majority of practitioners of machine learning here in Silicon Valley today doing these things as their full-time jobs. So I hope that these pieces of advice on by experience in diagnostics will help you to much effectively and powerfully apply learning and get them to work very well.""",65,0,1
coursera,stanford_university,machine-learning,prioritizing-what-to-work-on,"b'In the next few videos I\'d like to talk about machine learning system design. These videos will touch on the main issues that you may face when designing a complex machine learning system, and will actually try to give advice on how to strategize putting together a complex machine learning system. In case this next set of videos seems a little disjointed that\'s because these videos will touch on a range of the different issues that you may come across when designing complex learning systems. And even though the next set of videos may seem somewhat less mathematical, I think that this material may turn out to be very useful, and potentially huge time savers when you\'re building big machine learning systems. Concretely, I\'d like to begin with the issue of prioritizing how to spend your time on what to work on, and I\'ll begin with an example on spam classification. Let\'s say you want to build a spam classifier. Here are a couple of examples of obvious spam and non-spam emails. if the one on the left tried to sell things. And notice how spammers will deliberately misspell words, like Vincent with a 1 there, and mortgages. And on the right as maybe an obvious example of non-stamp email, actually email from my younger brother. Let\'s say we have a labeled training set of some number of spam emails and some non-spam emails denoted with labels y equals 1 or 0, how do we build a classifier using supervised learning to distinguish between spam and non-spam? In order to apply supervised learning, the first decision we must make is how do we want to represent x, that is the features of the email. Given the features x and the labels y in our training set, we can then train a classifier, for example using logistic regression. Here\'s one way to choose a set of features for our emails. We could come up with, say, a list of maybe a hundred words that we think are indicative of whether e-mail is spam or non-spam, for example, if a piece of e-mail contains the word \'deal\' maybe it\'s more likely to be spam if it contains the word  \'buy\' maybe more likely to be spam, a word like \'discount\' is more likely to be spam, whereas if a piece of email contains my name, Andrew, maybe that means the person actually knows who I am and that might mean it\'s less likely to be spam. And maybe for some reason I think the word ""now"" may be indicative of non-spam because I get a lot of urgent emails, and so on, and maybe we choose a hundred words or so. Given a piece of email, we can then take this piece of email and encode it into a feature vector as follows. I\'m going to take my list of a hundred words and sort them in alphabetical order say. It doesn\'t have to be sorted. But, you know, here\'s a, here\'s my list of words, just count and so on, until eventually I\'ll get down to now, and so on and given a piece of e-mail like that shown on the right, I\'m going to check and see whether or not each of these words appears in the e-mail and then I\'m going to define a feature vector x where in this piece of an email on the right, my name doesn\'t appear so I\'m gonna put a zero there. The word ""by"" does appear, so I\'m gonna put a one there and I\'m just gonna put one\'s or zeroes. I\'m gonna put a one even though the word ""by"" occurs twice. I\'m not gonna recount how many times the word occurs. The word ""due"" appears, I put a one there. The word ""discount"" doesn\'t appear, at least not in this this little short email, and so on. The word ""now"" does appear and so on. So I put ones and zeroes in this feature vector depending on whether or not a particular word appears. And in this example my feature vector would have to mention one hundred, if I have a hundred, if if I chose a hundred words to use for this representation and each of my features Xj will basically be 1 if you have a particular word that, we\'ll call this word j, appears in the email and Xj would be zero otherwise. Okay. So that gives me a feature representation of a piece of email. By the way, even though I\'ve described this process as manually picking a hundred words, in practice what\'s most commonly done is to look through a training set, and in the training set depict the most frequently occurring n words where n is usually between ten thousand and fifty thousand, and use those as your features. So rather than manually picking a hundred words, here you look through the training examples and pick the most frequently occurring words like ten thousand to fifty thousand words, and those form the features that you are going to use to represent your email for spam classification. Now, if you\'re building a spam classifier one question that you may face is, what\'s the best use of your time in order to make your spam classifier have higher accuracy, you have lower error. One natural inclination is going to collect lots of data. Right? And in fact there\'s this tendency to think that, well the more data we have the better the algorithm will do. And in fact, in the email spam domain, there are actually pretty serious projects called Honey Pot Projects, which create fake email addresses and try to get these fake email addresses into the hands of spammers and use that to try to collect tons of spam email, and therefore you know, get a lot of spam data to train learning algorithms. But we\'ve already seen in the previous sets of videos that getting lots of data will often help, but not all the time. But for most machine learning problems, there are a lot of other things you could usually imagine doing to improve performance. For spam, one thing you might think of is to develop more sophisticated features on the email, maybe based on the email routing information. And this would be information contained in the email header. So, when spammers send email, very often they will try to obscure the origins of the email, and maybe use fake email headers. Or send email through very unusual sets of computer service. Through very unusual routes, in order to get the spam to you. And some of this information will be reflected in the email header. And so one can imagine, looking at the email headers and trying to develop more sophisticated features to capture this sort of email routing information to identify if something is spam. Something else you might consider doing is to look at the email message body, that is the email text, and try to develop more sophisticated features. For example, should the word \'discount\' and the word \'discounts\' be treated as the same words or should we have treat the words \'deal\' and \'dealer\' as the same word? Maybe even though one is lower case and one in capitalized in this example. Or do we want more complex features about punctuation because maybe spam is using exclamation marks a lot more. I don\'t know. And along the same lines, maybe we also want to develop more sophisticated algorithms to detect and maybe to correct to deliberate misspellings, like mortgage, medicine, watches. Because spammers actually do this, because if you have watches with a 4 in there then well, with the simple technique that we talked about just now, the spam classifier might not equate this as the same thing as the word ""watches,"" and so it may have a harder time realizing that something is spam with these deliberate misspellings. And this is why spammers do it. While working on a machine learning problem, very often you can brainstorm lists of different things to try, like these. By the way, I\'ve actually worked on the spam problem myself for a while. And I actually spent quite some time on it. And even though I kind of understand the spam problem, I actually know a bit about it, I would actually have a very hard time telling you of these four options which is the best use of your time so what happens, frankly what happens far too often is that a research group or product group will randomly fixate on one of these options. And sometimes that turns out not to be the most fruitful way to spend your time depending, you know, on which of these options someone ends up randomly fixating on. By the way, in fact, if you even get to the stage where you brainstorm a list of different options to try, you\'re probably already ahead of the curve. Sadly, what most people do is instead of trying to list out the options of things you might try, what far too many people do is wake up one morning and, for some reason, just, you know, have a weird gut feeling that, ""Oh let\'s have a huge honeypot project to go and collect tons more data"" and for whatever strange reason just sort of wake up one morning and randomly fixate on one thing and just work on that for six months. But I think we can do better. And in particular what I\'d like to do in the next video is tell you about the concept of error analysis and talk about the way where you can try to have a more systematic way to choose amongst the options of the many different things you might work, and therefore be more likely to select what is actually a good way to spend your time, you know for the next few weeks, or next few days or the next few months.'",66,0,1
coursera,stanford_university,machine-learning,error-analysis,"b""In the last video I talked about how, when\nfaced with a machine learning problem, there are often lots of different ideas\nfor how to improve the algorithm. In this video, let's talk about\nthe concept of error analysis. Which will hopefully give you a way\nto more systematically make some of these decisions. If you're starting work on\na machine learning problem, or building a machine learning application. It's often considered very\ngood practice to start, not by building a very complicated system\nwith lots of complex features and so on. But to instead start by building\na very simple algorithm that you can implement quickly. And when I start with a learning\nproblem what I usually do is spend at most one day, like literally at most 24 hours, To try\nto get something really quick and dirty. Frankly not at all sophisticated system\nbut get something really quick and dirty running, and implement it and\nthen test it on my cross-validation data. Once you've done that you can\nthen plot learning curves, this is what we talked about\nin the previous set of videos. But plot learning curves\nof the training and test errors to try to figure out if you're\nlearning algorithm maybe suffering from high bias or high variance,\nor something else. And use that to try to\ndecide if having more data, more features, and so\non are likely to help. And the reason that this is a good\napproach is often, when you're just starting out on a learning problem,\nthere's really no way to tell in advance. Whether you need more complex features,\nor whether you need more data, or something else. And it's just very hard to\ntell in advance, that is, in the absence of evidence,\nin the absence of seeing a learning curve. It's just incredibly\ndifficult to figure out where you should be spending your time. And it's often by implementing even a\nvery, very quick and dirty implementation. And by plotting learning curves,\nthat helps you make these decisions. So if you like you can to think of this as\na way of avoiding whats sometimes called premature optimization\nin computer programming. And this idea that says we should let\nevidence guide our decisions on where to spend our time rather than use\ngut feeling, which is often wrong. In addition to plotting learning curves, one other thing that's often very useful\nto do is what's called error analysis. And what I mean by that is that when\nbuilding say a spam classifier. I will often look at my\ncross validation set and manually look at the emails that\nmy algorithm is making errors on. So look at the spam e-mails and non-spam e-mails that the algorithm\nis misclassifying and see if you can spot any systematic patterns in what\ntype of examples it is misclassifying. And often, by doing that, this is the process that will\ninspire you to design new features. Or they'll tell you what\nare the current things or current shortcomings of the system. And give you the inspiration you need\nto come up with improvements to it. Concretely, here's a specific example. Let's say you've built\na spam classifier and you have 500 examples in\nyour cross validation set. And let's say in this example that\nthe algorithm has a very high error rate. And this classifies 100 of these\ncross validation examples. So what I do is manually examine these\n100 errors and manually categorize them. Based on things like what type\nof email it is, what cues or what features you think might have helped\nthe algorithm classify them correctly. So, specifically, by what type of email it\nis, if I look through these 100 errors, I might find that maybe the most\ncommon types of spam emails in these classifies are maybe emails on pharma or\npharmacies, trying to sell drugs. Maybe emails that are trying to\nsell replicas such as fake watches, fake random things, maybe some\nemails trying to steal passwords,. These are also called phishing emails,\nthat's another big category of emails, and maybe other categories. So in terms of classify\nwhat type of email it is, I would actually go through and\ncount up my hundred emails. Maybe I find that 12 of them is label\nemails, or pharma emails, and maybe 4 of them are emails trying to sell replicas,\nthat sell fake watches or something. And maybe I find that 53 of them\nare these what's called phishing emails, basically emails trying to persuade\nyou to give them your password. And 31 emails are other types of emails. And it's by counting up the number of emails in these different categories\nthat you might discover, for example. That the algorithm is doing really, particularly poorly on emails\ntrying to steal passwords. And that may suggest that it\nmight be worth your effort to look more carefully at\nthat type of email and see if you can come up with better\nfeatures to categorize them correctly. And, also what I might do\nis look at what cues or what additional features might have\nhelped the algorithm classify the emails. So let's say that some of our\nhypotheses about things or features that might help us\nclassify emails better are. Trying to detect deliberate\nmisspellings versus unusual email routing versus unusual\nspamming punctuation. Such as if people use a lot\nof exclamation marks. And once again I would manually go\nthrough and let's say I find five cases of this and 16 of this and 32 of this and\na bunch of other types of emails as well. And if this is what you get on your cross\nvalidation set, then it really tells you that maybe deliberate spellings is\na sufficiently rare phenomenon that maybe it's not worth all the time trying to\nwrite algorithms that detect that. But if you find that a lot of spammers are\nusing, you know, unusual punctuation, then maybe that's a strong sign that it might\nactually be worth your while to spend the time to develop more sophisticated\nfeatures based on the punctuation. So this sort of error analysis,\nwhich is really the process of manually examining the mistakes\nthat the algorithm makes, can often help guide you to the most\nfruitful avenues to pursue. And this also explains why I often\nrecommend implementing a quick and dirty implementation of an algorithm. What we really want to do is figure out\nwhat are the most difficult examples for an algorithm to classify. And very often for different algorithms,\nfor different learning algorithms they'll often find similar\ncategories of examples difficult. And by having a quick and dirty implementation, that's often a quick\nway to let you identify some errors and quickly identify what\nare the hard examples. So that you can focus\nyour effort on those. Lastly, when developing learning\nalgorithms, one other useful tip is to make sure that you have a numerical\nevaluation of your learning algorithm. And what I mean by that is you if\nyou're developing a learning algorithm, it's often incredibly helpful. If you have a way of evaluating\nyour learning algorithm that just gives you back a single real\nnumber, maybe accuracy, maybe error. But the single real number that tells you\nhow well your learning algorithm is doing. I'll talk more about this specific\nconcept in later videos, but here's a specific example. Let's say we're trying to decide whether\nor not we should treat words like discount, discounts, discounted,\ndiscounting as the same word. So you know maybe one way to do\nthat is to just look at the first few characters in the word like, you know. If you just look at the first few\ncharacters of a word, then you figure out that maybe all of these\nwords roughly have similar meanings. In natural language processing, the way\nthat this is done is actually using a type of software called stemming software. And if you ever want to do this yourself,\nsearch on a web-search engine for the porter stemmer, and that would be\none reasonable piece of software for doing this sort of stemming,\nwhich will let you treat all these words, discount, discounts, and\nso on, as the same word. But using a stemming software that\nbasically looks at the first few alphabets of a word, more of less,\nit can help, but it can hurt. And it can hurt because for\nexample, the software may mistake the words universe and\nuniversity as being the same thing. Because, you know, these two words\nstart off with the same alphabets. So if you're trying to decide whether or\nnot to use stemming software for a spam cross classifier,\nit's not always easy to tell. And in particular, error analysis\nmay not actually be helpful for deciding if this sort of\nstemming idea is a good idea. Instead, the best way to figure out\nif using stemming software is good to help your classifier is if you have\na way to very quickly just try it and see if it works. And in order to do this, having a way to numerically evaluate your\nalgorithm is going to be very helpful. Concretely, maybe the most natural thing\nto do is to look at the cross validation error of the algorithm's performance\nwith and without stemming. So, if you run your algorithm\nwithout stemming and end up with 5 percent\nclassification error. And you rerun it and you end up with 3 percent classification\nerror, then this decrease in error very quickly allows you to decide that it\nlooks like using stemming is a good idea. For this particular problem,\nthere's a very natural, single, real number evaluation metric,\nnamely the cross validation error. We'll see later examples where\ncoming up with this sort of single, real number evaluation metric\nwill need a little bit more work. But as we'll see in a later video,\ndoing so would also then let you make these decisions much more quickly of say,\nwhether or not to use stemming. And, just as one more quick example, let's\nsay that you're also trying to decide whether or not to distinguish\nbetween upper versus lower case. So, you know, as the word, mom,\nwere upper case, and versus lower case m, should that be treated as the same word or\nas different words? Should this be treated as the same\nfeature, or as different features? And so, once again, because we have\na way to evaluate our algorithm. If you try this down here,\nif I stopped distinguishing upper and lower case,\nmaybe I end up with 3.2 percent error. And I find that therefore, this does\nworse than if I use only stemming. So, this let's me very quickly\ndecide to go ahead and to distinguish or to not distinguish\nbetween upper and lowercase. So when you're developing a learning\nalgorithm, very often you'll be trying out lots of new ideas and lots of new\nversions of your learning algorithm. If every time you try out a new idea,\nif you end up manually examining a bunch of examples\nagain to see if it got better or worse, that's gonna make it\nreally hard to make decisions on. Do you use stemming or not? Do you distinguish upper and\nlower case or not? But by having a single real\nnumber evaluation metric, you can then just look and see, oh,\ndid the arrow go up or did it go down? And you can use that to much more\nrapidly try out new ideas and almost right away tell if\nyour new idea has improved or worsened the performance\nof the learning algorithm. And this will let you often\nmake much faster progress. So the recommended, strongly recommended\nthe way to do error analysis is on the cross validations there\nrather than the test set. But, you know, there are people that will\ndo this on the test set, even though that's definitely a less mathematic\nappropriate, certainly a less recommended way to, thing to do than to do error\nanalysis on your cross validation set. Set to wrap up this video, when starting\non a new machine learning problem, what I almost always recommend\nis to implement a quick and dirty implementation of\nyour learning out of them. And I've almost never seen anyone spend\ntoo little time on this quick and dirty implementation. I've pretty much only ever seen\npeople spend much too much time building their first, supposedly,\nquick and dirty implementation. So really,\ndon't worry about it being too quick, or don't worry about it being too dirty. But really,\nimplement something as quickly as you can. And once you have\nthe initial implementation, this is then a powerful tool for\ndeciding where to spend your time next. Because first you can look\nat the errors it makes, and do this sort of error analysis to see\nwhat other mistakes it makes, and use that to inspire further development. And second, assuming your quick and dirty implementation incorporated\na single real number evaluation metric. This can then be a vehicle for\nyou to try out different ideas and quickly see if the different ideas you're\ntrying out are improving the performance of your algorithm. And therefore let you, maybe much more\nquickly make decisions about what things to fold in and what things to\nincorporate into your learning algorithm.""",67,0,1
coursera,stanford_university,machine-learning,error-metrics-for-skewed-classes,"b'In the previous video, I talked about error analysis and the importance of having error metrics, that is of having a single real number evaluation metric for your learning algorithm to tell how well it\'s doing. In the context of evaluation and of error metrics, there is one important case, where it\'s particularly tricky to come up with an appropriate error metric, or evaluation metric, for your learning algorithm. That case is the case of what\'s called skewed classes. Let me tell you what that means. Consider the problem of cancer classification, where we have features of medical patients and we want to decide whether or not they have cancer. So this is like the malignant versus benign tumor classification example that we had earlier. So let\'s say y equals 1 if the patient has cancer and y equals 0 if they do not. We have trained the progression classifier and let\'s say we test our classifier on a test set and find that we get 1 percent error. So, we\'re making 99% correct diagnosis. Seems like a really impressive result, right. We\'re correct 99% percent of the time. But now, let\'s say we find out that only 0.5 percent of patients in our training test sets actually have cancer. So only half a percent of the patients that come through our screening process have cancer. In this case, the 1% error no longer looks so impressive. And in particular, here\'s a piece of code, here\'s actually a piece of non learning code that takes this input of features x and it ignores it. It just sets y equals 0 and always predicts, you know, nobody has cancer and this algorithm would actually get 0.5 percent error. So this is even better than the 1% error that we were getting just now and this is a non learning algorithm that you know, it is just predicting y equals 0 all the time. So this setting of when the ratio of positive to negative examples is very close to one of two extremes, where, in this case, the number of positive examples is much, much smaller than the number of negative examples because y equals one so rarely, this is what we call the case of skewed classes. We just have a lot more of examples from one class than from the other class. And by just predicting y equals 0 all the time, or maybe our predicting y equals 1 all the time, an algorithm can do pretty well. So the problem with using classification error or classification accuracy as our evaluation metric is the following. Let\'s say you have one joining algorithm that\'s getting 99.2% accuracy. So, that\'s a 0.8% error. Let\'s say you make a change to your algorithm and you now are getting 99.5% accuracy. That is 0.5% error. So, is this an improvement to the algorithm or not? One of the nice things about having a single real number evaluation metric is this helps us to quickly decide if we just need a good change to the algorithm or not. By going from 99.2% accuracy to 99.5% accuracy. You know, did we just do something useful or did we just replace our code with something that just predicts y equals zero more often? So, if you have very skewed classes it becomes much harder to use just classification accuracy, because you can get very high classification accuracies or very low errors, and it\'s not always clear if doing so is really improving the quality of your classifier because predicting y equals 0 all the time doesn\'t seem like a particularly good classifier. But just predicting y equals 0 more often can bring your error down to, you know, maybe as low as 0.5%. When we\'re faced with such a skewed classes therefore we would want to come up with a different error metric or a different evaluation metric. One such evaluation metric are what\'s called precision recall. Let me explain what that is. Let\'s say we are evaluating a classifier on the test set. For the examples in the test set the actual class of that example in the test set is going to be either one or zero, right, if there is a binary classification problem. And what our learning algorithm will do is it will, you know, predict some value for the class and our learning algorithm will predict the value for each example in my test set and the predicted value will also be either one or zero. So let me draw a two by two table as follows, depending on a full of these entries depending on what was the actual class and what was the predicted class. If we have an example where the actual class is one and the predicted class is one then that\'s called an example that\'s a true positive, meaning our algorithm predicted that it\'s positive and in reality the example is positive. If our learning algorithm predicted that something is negative, class zero, and the actual class is also class zero then that\'s what\'s called a true negative. We predicted zero and it actually is zero. To find the other two boxes, if our learning algorithm predicts that the class is one but the actual class is zero, then that\'s called a false positive. So that means our algorithm for the patient is cancelled out in reality if the patient does not. And finally, the last box is a zero, one. That\'s called a false negative because our algorithm predicted zero, but the actual class was one. And so, we have this little sort of two by two table based on what was the actual class and what was the predicted class. So here\'s a different way of evaluating the performance of our algorithm. We\'re going to compute two numbers. The first is called precision - and what that says is, of all the patients where we\'ve predicted that they have cancer, what fraction of them actually have cancer? So let me write this down, the precision of a classifier is the number of true positives divided by the number that we predicted as positive, right? So of all the patients that we went to those patients and we told them, ""We think you have cancer."" Of all those patients, what fraction of them actually have cancer? So that\'s called precision. And another way to write this would be true positives and then in the denominator is the number of predicted positives, and so that would be the sum of the, you know, entries in this first row of the table. So it would be true positives divided by true positives. I\'m going to abbreviate positive as POS and then plus false positives, again abbreviating positive using POS. So that\'s called precision, and as you can tell high precision would be good. That means that all the patients that we went to and we said, ""You know, we\'re very sorry. We think you have cancer,"" high precision means that of that group of patients most of them we had actually made accurate predictions on them and they do have cancer. The second number we\'re going to compute is called recall, and what recall say is, if all the patients in, let\'s say, in the test set or the cross-validation set, but if all the patients in the data set that actually have cancer, what fraction of them that we correctly detect as having cancer. So if all the patients have cancer, how many of them did we actually go to them and you know, correctly told them that we think they need treatment. So, writing this down, recall is defined as the number of positives, the number of true positives, meaning the number of people that have cancer and that we correctly predicted have cancer and we take that and divide that by, divide that by the number of actual positives, so this is the right number of actual positives of all the people that do have cancer. What fraction do we directly flag and you know, send the treatment. So, to rewrite this in a different form, the denominator would be the number of actual positives as you know, is the sum of the entries in this first column over here. And so writing things out differently, this is therefore, the number of true positives, divided by the number of true positives plus the number of false negatives. And so once again, having a high recall would be a good thing. So by computing precision and recall this will usually give us a better sense of how well our classifier is doing. And in particular if we have a learning algorithm that predicts y equals zero all the time, if it predicts no one has cancer, then this classifier will have a recall equal to zero, because there won\'t be any true positives and so that\'s a quick way for us to recognize that, you know, a classifier that predicts y equals 0 all the time, just isn\'t a very good classifier. And more generally, even for settings where we have very skewed classes, it\'s not possible for an algorithm to sort of ""cheat"" and somehow get a very high precision and a very high recall by doing some simple thing like predicting y equals 0 all the time or predicting y equals 1 all the time. And so we\'re much more sure that a classifier of a high precision or high recall actually is a good classifier, and this gives us a more useful evaluation metric that is a more direct way to actually understand whether, you know, our algorithm may be doing well. So one final note in the definition of precision and recall, that we would define precision and recall, usually we use the convention that y is equal to 1, in the presence of the more rare class. So if we are trying to detect. rare conditions such as cancer, hopefully that\'s a rare condition, precision and recall are defined setting y equals 1, rather than y equals 0, to be sort of that the presence of that rare class that we\'re trying to detect. And by using precision and recall, we find, what happens is that even if we have very skewed classes, it\'s not possible for an algorithm to you know, ""cheat"" and predict y equals 1 all the time, or predict y equals 0 all the time, and get high precision and recall. And in particular, if a classifier is getting high precision and high recall, then we are actually confident that the algorithm has to be doing well, even if we have very skewed classes. So for the problem of skewed classes precision recall gives us more direct insight into how the learning algorithm is doing and this is often a much better way to evaluate our learning algorithms, than looking at classification error or classification accuracy, when the classes are very skewed.'",68,0,1
coursera,stanford_university,machine-learning,trading-off-precision-and-recall,"b""In the last video,\nwe talked about precision and recall as an evaluation metric for classification\nproblems with skewed constants. For many applications, we'll want to somehow control the\ntrade-off between precision and recall. Let me tell you how to do that and also\nshow you some even more effective ways to use precision and recall as an evaluation\nmetric for learning algorithms. As a reminder,\nhere are the definitions of precision and recall from the previous video. Let's continue our cancer\nclassification example, where y equals 1 if the patient has\ncancer and y equals 0 otherwise. And let's say we're trained in logistic\nregression classifier which outputs probability between 0 and 1. So, as usual,\nwe're going to predict 1, y equals 1, if h(x) is greater or equal to 0.5. And predict 0 if the hypothesis\noutputs a value less than 0.5. And this classifier may give us some value\nfor precision and some value for recall. But now, suppose we want to predict\nthat the patient has cancer only if we're very confident\nthat they really do. Because if you go to a patient and\nyou tell them that they have cancer, it's going to give them a huge shock. What we give is a seriously bad news, and they may end up going through a pretty\npainful treatment process and so on. And so maybe we want to tell someone that\nwe think they have cancer only if they are very confident. One way to do this would be\nto modify the algorithm, so that instead of setting this threshold\nat 0.5, we might instead say that we will predict that y is equal to 1\nonly if h(x) is greater or equal to 0.7. So this is like saying, we'll tell\nsomeone they have cancer only if we think there's a greater than or equal to,\n70% chance that they have cancer. And, if you do this, then you're predicting someone has cancer\nonly when you're more confident and so you end up with a classifier\nthat has higher precision. Because all of the patients that\nyou're going to and saying, we think you have cancer, although\nthose patients are now ones that you're pretty confident actually have cancer. And so a higher fraction of\nthe patients that you predict have cancer will actually turn out to have\ncancer because making those predictions only if we're pretty confident. But in contrast this classifier will have\nlower recall because now we're going to make predictions, we're going to predict\ny = 1 on a smaller number of patients. Now, can even take this further. Instead of setting the threshold at 0.7,\nwe can set this at 0.9. Now we'll predict y=1 only if we\nare more than 90% certain that the patient has cancer. And so, a large fraction of those\npatients will turn out to have cancer. And so this would be a higher precision\nclassifier will have lower recall because we want to correctly detect that\nthose patients have cancer. Now consider a different example. Suppose we want to avoid missing\ntoo many actual cases of cancer, so we want to avoid false negatives. In particular,\nif a patient actually has cancer, but we fail to tell them that they have\ncancer then that can be really bad. Because if we tell a patient\nthat they don't have cancer, then they're not going to go for\ntreatment. And if it turns out that they have cancer,\nbut we fail to tell them they have cancer, well, they may not get treated at all. And so\nthat would be a really bad outcome because they die because we told them\nthat they don't have cancer. They fail to get treated, but\nit turns out they actually have cancer. So, suppose that, when in doubt,\nwe want to predict that y=1. So, when in doubt, we want to\npredict that they have cancer so that at least they look further into it,\nand these can get treated in case\nthey do turn out to have cancer. In this case, rather than setting\nhigher probability threshold, we might instead take this value and\ninstead set it to a lower value. So maybe 0.3 like so, right? And by doing so, we're saying that,\nyou know what, if we think there's more than a 30% chance that they have cancer\nwe better be more conservative and tell them that they may have cancer so\nthat they can seek treatment if necessary. And in this case what we would have\nis going to be a higher recall classifier, because we're\ngoing to be correctly flagging a higher fraction of all of\nthe patients that actually do have cancer. But we're going to end\nup with lower precision because a higher fraction of\nthe patients that we said have cancer, a high fraction of them will turn\nout not to have cancer after all. And by the way, just as a sider,\nwhen I talk about this to other students, I've been told before,\nit's pretty amazing, some of my students say,\nis how I can tell the story both ways. Why we might want to\nhave higher precision or higher recall and the story\nactually seems to work both ways. But I hope the details of\nthe algorithm is true and the more general principle is\ndepending on where you want, whether you want higher precision- lower\nrecall, or higher recall- lower precision. You can end up predicting y=1 when\nh(x) is greater than some threshold. And so in general, for\nmost classifiers there is going to be a trade off between precision and\nrecall, and as you vary the value of this\nthreshold that we join here, you can actually plot out some curve\nthat trades off precision and recall. Where a value up here, this would\ncorrespond to a very high value of the threshold,\nmaybe threshold equals 0.99. So that's saying, predict y=1 only\nif we're more than 99% confident, at least 99% probability this one. So that would be a high precision,\nrelatively low recall. Where as the point down here, will correspond to a value of\nthe threshold that's much lower, maybe equal 0.01, meaning, when in doubt\nat all, predict y=1, and if you do that, you end up with a much lower precision,\nhigher recall classifier. And as you vary the threshold, if you want\nyou can actually trace of a curve for your classifier to see the range of different\nvalues you can get for precision recall. And by the way, the precision-recall curve\ncan look like many different shapes. Sometimes it will look like this,\nsometimes it will look like that. Now there are many different\npossible shapes for the precision-recall curve, depending\non the details of the classifier. So, this raises another\ninteresting question which is, is there a way to choose this\nthreshold automatically? Or more generally, if we have a few\ndifferent algorithms or a few different ideas for algorithms, how do we compare\ndifferent precision recall numbers? Concretely, suppose we have three\ndifferent learning algorithms. So actually, maybe these are three\ndifferent learning algorithms, maybe these are the same algorithm but just\nwith different values for the threshold. How do we decide which of\nthese algorithms is best? One of the things we talked about earlier\nis the importance of a single real number evaluation metric. And that is the idea of having a number\nthat just tells you how well is your classifier doing. But by switching to the precision\nrecall metric we've actually lost that. We now have two real numbers. And so we often, we end up face the situations like if\nwe trying to compare Algorithm 1 and Algorithm 2, we end up asking ourselves,\nis the precision of 0.5 and a recall of 0.4, was that better or worse\nthan a precision of 0.7 and recall of 0.1? And, if every time you try out a new\nalgorithm you end up having to sit around and think, well, maybe 0.5/0.4 is better\nthan 0.7/0.1, or maybe not, I don't know. If you end up having to sit around and\nthink and make these decisions, that really slows\ndown your decision making process for what changes are useful to\nincorporate into your algorithm. Whereas in contrast, if we have\na single real number evaluation metric like a number that just tells us is\nalgorithm 1 or is algorithm 2 better, then that helps us to much more quickly\ndecide which algorithm to go with. It helps us as well to much\nmore quickly evaluate different changes that we may be contemplating for\nan algorithm. So how can we get a single\nreal number evaluation metric? One natural thing that you might try is to\nlook at the average precision and recall. So, using P and R to denote precision and\nrecall, what you could do is just compute the average and look at what classifier\nhas the highest average value. But this turns out not to be such a good\nsolution, because similar to the example we had earlier it turns out that\nif we have a classifier that predicts y=1 all the time, then if you\ndo that you can get a very high recall, but you end up with a very\nlow value of precision. Conversely, if you have a classifier\nthat predicts y equals zero, almost all the time, that is that it predicts\ny=1 very sparingly, this corresponds to setting a very high threshold using\nthe notation of the previous y. Then you can actually end up with a very\nhigh precision with a very low recall. So, the two extremes of either\na very high threshold or a very low threshold, neither of that\nwill give a particularly good classifier. And the way we recognize that is by\nseeing that we end up with a very low precision or a very low recall. And if you just take the average\nof (P+R)/2 from this example, the average is actually highest for\nAlgorithm 3, even though you can get that sort of performance by\npredicting y=1 all the time and that's just not a very good classifier,\nright? You predict y=1 all the time,\njust normal useful classifier, but all it does is prints out y=1. And so Algorithm 1 or Algorithm 2\nwould be more useful than Algorithm 3. But in this example,\nAlgorithm 3 has a higher average value of precision recall than Algorithms 1 and 2. So we usually think of this\naverage of precision and recall as not a particularly good way\nto evaluate our learning algorithm. In contrast, there's a different way for\ncombining precision and recall. This is called the F Score and\nit uses that formula. And so in this example,\nhere are the F Scores. And so we would tell from these F Scores,\nit looks like Algorithm 1 has the highest F Score, Algorithm 2 has the second\nhighest, and Algorithm 3 has the lowest. And so, if we go by the F Score we would\npick probably Algorithm 1 over the others. The F Score, which is also called the F1\nScore, is usually written F1 Score that I have here, but often people will just\nsay F Score, either term is used. Is a little bit like taking\nthe average of precision and recall, but\nit gives the lower value of precision and recall, whichever it is,\nit gives it a higher weight. And so, you see in the numerator here that the F Score takes a product\nof precision and recall. And so if either precision is 0 or\nrecall is equal to 0, the F Score will be equal to 0. So in that sense, it kind of combines\nprecision and recall, but for the F Score to be large, both precision\nand recall have to be pretty large. I should say that there are many\ndifferent possible formulas for combing precision and recall. This F Score formula is really maybe a, just one out of a much larger number\nof possibilities, but historically or traditionally this is what people\nin Machine Learning seem to use. And the term F Score,\nit doesn't really mean anything, so don't worry about why it's\ncalled F Score or F1 Score. But this usually gives you the effect\nthat you want because if either a precision is zero or recall is zero,\nthis gives you a very low F Score, and so to have a high F Score, you kind\nof need a precision or recall to be one. And concretely, if P=0 or R=0, then this gives you that the F Score = 0. Whereas a perfect F Score,\nso if precision equals one and recall equals 1,\nthat will give you an F Score, that's equal to 1 times 1 over 2 times 2,\nso the F Score will be equal to 1, if you\nhave perfect precision and perfect recall. And intermediate values between 0 and 1, this usually gives a reasonable rank\nordering of different classifiers. So in this video, we talked about the notion of trading\noff between precision and recall, and how we can vary the threshold that we use\nto decide whether to predict y=1 or y=0. So it's the threshold that says,\ndo we need to be at least 70% confident or 90% confident, or\nwhatever before we predict y=1. And by varying the threshold, you can\ncontrol a trade off between precision and recall. We also talked about the F Score, which\ntakes precision and recall, and again, gives you a single real\nnumber evaluation metric. And of course, if your goal is to\nautomatically set that threshold to decide what's really y=1 and y=0,\none pretty reasonable way to do that would also be to try a range of\ndifferent values of thresholds. So you try a range of values of thresholds\nand evaluate these different thresholds on, say, your cross-validation set and\nthen to pick whatever value of threshold gives you the highest F Score\non your crossvalidation [INAUDIBLE]. And that be a pretty reasonable way to\nautomatically choose the threshold for your classifier as well.""",69,0,1
coursera,stanford_university,machine-learning,data-for-machine-learning,"b'In the previous video, we talked about evaluation metrics. In this video, I\'d like to switch tracks a bit and touch on another important aspect of machine learning system design, which will often come up, which is the issue of how much data to train on. Now, in some earlier videos, I had cautioned against blindly going out and just spending lots of time collecting lots of data, because it\'s only sometimes that that would actually help. But it turns out that under certain conditions, and I will say in this video what those conditions are, getting a lot of data and training on a certain type of learning algorithm, can be a very effective way to get a learning algorithm to do very good performance. And this arises often enough that if those conditions hold true for your problem and if you\'re able to get a lot of data, this could be a very good way to get a very high performance learning algorithm. So in this video, let\'s talk more about that. Let me start with a story. Many, many years ago, two researchers that I know, Michelle Banko and Eric Broule ran the following fascinating study. They were interested in studying the effect of using different learning algorithms versus trying them out on different training set sciences, they were considering the problem of classifying between confusable words, so for example, in the sentence: for breakfast I ate, should it be to, two or too? Well, for this example, for breakfast I ate two, 2 eggs. So, this is one example of a set of confusable words and that\'s a different set. So they took machine learning problems like these, sort of supervised learning problems to try to categorize what is the appropriate word to go into a certain position in an English sentence. They took a few different learning algorithms which were, you know, sort of considered state of the art back in the day, when they ran the study in 2001, so they took a variance, roughly a variance on logistic regression called the Perceptron. They also took some of their algorithms that were fairly out back then but somewhat less used now so when the algorithm also very similar to which is a regression but different in some ways, much used somewhat less, used not too much right now took what\'s called a memory based learning algorithm again used somewhat less now. But I\'ll talk a little bit about that later. And they used a naive based algorithm, which is something they\'ll actually talk about in this course. The exact algorithms of these details aren\'t important. Think of this as, you know, just picking four different classification algorithms and really the exact algorithms aren\'t important. But what they did was they varied the training set size and tried out these learning algorithms on the range of training set sizes and that\'s the result they got. And the trends are very clear right first most of these outer rooms give remarkably similar performance. And second, as the training set size increases, on the horizontal axis is the training set size in millions go from you know a hundred thousand up to a thousand million that is a billion training examples. The performance of the algorithms all pretty much monotonically increase and the fact that if you pick any algorithm may be pick a ""inferior algorithm"" but if you give that ""inferior algorithm"" more data, then from these examples, it looks like it will most likely beat even a ""superior algorithm"". So since this original study which is very influential, there\'s been a range of many different studies showing similar results that show that many different learning algorithms you know tend to, can sometimes, depending on details, can give pretty similar ranges of performance, but what can really drive performance is you can give the algorithm a ton of training data. And this is, results like these has led to a saying in machine learning that often in machine learning it\'s not who has the best algorithm that wins, it\'s who has the most data So when is this true and when is this not true? Because we have a learning algorithm for which this is true then getting a lot of data is often maybe the best way to ensure that we have an algorithm with very high performance rather than you know, debating worrying about exactly which of these items to use. Let\'s try to lay out a set of assumptions under which having a massive training set we think will be able to help. Let\'s assume that in our machine learning problem, the features x have sufficient information with which we can use to predict y accurately. For example, if we take the confusable words all of them that we had on the previous slide. Let\'s say that it features x capture what are the surrounding words around the blank that we\'re trying to fill in. So the features capture then we want to have, sometimes for breakfast I have black eggs. Then yeah that is pretty much information to tell me that the word I want in the middle is TWO and that is not word TO and its not the word TOO. So the features capture, you know, one of these surrounding words then that gives me enough information to pretty unambiguously decide what is the label y or in other words what is the word that I should be using to fill in that blank out of this set of three confusable words. So that\'s an example what the future ex has sufficient information for specific y. For a counter example. Consider a problem of predicting the price of a house from only the size of the house and from no other features. So if you imagine I tell you that a house is, you know, 500 square feet but I don\'t give you any other features. I don\'t tell you that the house is in an expensive part of the city. Or if I don\'t tell you that the house, the number of rooms in the house, or how nicely furnished the house is, or whether the house is new or old. If I don\'t tell you anything other than that this is a 500 square foot house, well there\'s so many other factors that would affect the price of a house other than just the size of a house that if all you know is the size, it\'s actually very difficult to predict the price accurately. So that would be a counter example to this assumption that the features have sufficient information to predict the price to the desired level of accuracy. The way I think about testing this assumption, one way I often think about it is, how often I ask myself. Given the input features x, given the features, given the same information available as well as learning algorithm. If we were to go to human expert in this domain. Can a human experts actually or can human expert confidently predict the value of y. For this first example if we go to, you know an expert human English speaker. You go to someone that speaks English well, right, then a human expert in English just read most people like you and me will probably we would probably be able to predict what word should go in here, to a good English speaker can predict this well, and so this gives me confidence that x allows us to predict y accurately, but in contrast if we go to an expert in human prices. Like maybe an expert realtor, right, someone who sells houses for a living. If I just tell them the size of a house and I tell them what the price is well even an expert in pricing or selling houses wouldn\'t be able to tell me and so this is fine that for the housing price example knowing only the size doesn\'t give me enough information to predict the price of the house. So, let\'s say, this assumption holds. Let\'s see then, when having a lot of data could help. Suppose the features have enough information to predict the value of y. And let\'s suppose we use a learning algorithm with a large number of parameters so maybe logistic regression or linear regression with a large number of features. Or one thing that I sometimes do, one thing that I often do actually is using neural network with many hidden units. That would be another learning algorithm with a lot of parameters. So these are all powerful learning algorithms with a lot of parameters that can fit very complex functions. So, I\'m going to call these, I\'m going to think of these as low-bias algorithms because you know we can fit very complex functions and because we have a very powerful learning algorithm, they can fit very complex functions. Chances are, if we run these algorithms on the data sets, it will be able to fit the training set well, and so hopefully the training error will be slow. Now let\'s say, we use a massive, massive training set, in that case, if we have a huge training set, then hopefully even though we have a lot of parameters but if the training set is sort of even much larger than the number of parameters then hopefully these albums will be unlikely to overfit. Right because we have such a massive training set and by unlikely to overfit what that means is that the training error will hopefully be close to the test error. Finally putting these two together that the train set error is small and the test set error is close to the training error what this two together imply is that hopefully the test set error will also be small. Another way to think about this is that in order to have a high performance learning algorithm we want it not to have high bias and not to have high variance. So the bias problem we\'re going to address by making sure we have a learning algorithm with many parameters and so that gives us a low bias alorithm and by using a very large training set, this ensures that we don\'t have a variance problem here. So hopefully our algorithm will have no variance and so is by pulling these two together, that we end up with a low bias and a low variance learning algorithm and this allows us to do well on the test set. And fundamentally it\'s a key ingredients of assuming that the features have enough information and we have a rich class of functions that\'s why it guarantees low bias, and then it having a massive training set that that\'s what guarantees more variance. So this gives us a set of conditions rather hopefully some understanding of what\'s the sort of problem where if you have a lot of data and you train a learning algorithm with lot of parameters, that might be a good way to give a high performance learning algorithm and really, I think the key test that I often ask myself are first, can a human experts look at the features x and confidently predict the value of y. Because that\'s sort of a certification that y can be predicted accurately from the features x and second, can we actually get a large training set, and train the learning algorithm with a lot of parameters in the training set and if you can\'t do both then that\'s more often give you a very kind performance learning algorithm.'",70,0,1
coursera,stanford_university,machine-learning,optimization-objective,"b""By now, you've seen a range of\ndifference learning algorithms. With supervised learning, the performance\nof many supervised learning algorithms will be pretty similar, and what matters\nless often will be whether you use learning algorithm a or\nlearning algorithm b, but what matters more will often be things\nlike the amount of data you create these algorithms on, as well as your\nskill in applying these algorithms. Things like your choice of the features\nyou design to give to the learning algorithms, and how you choose\nthe colorization parameter, and things like that. But, there's one more algorithm that is\nvery powerful and is very widely used both within industry and academia, and\nthat's called the support vector machine. And compared to both logistic\nregression and neural networks, the Support Vector Machine, or\nSVM sometimes gives a cleaner, and sometimes more powerful way of\nlearning complex non-linear functions. And so let's take the next\nvideos to talk about that. Later in this course,\nI will do a quick survey of a range of different supervisory algorithms just\nas a very briefly describe them. But the support vector machine, given\nits popularity and how powerful it is, this will be the last of the supervisory\nalgorithms that I'll spend a significant amount of time on in this course as with\nour development other learning algorithms, we're gonna start by talking\nabout the optimization objective. So, let's get started on this algorithm. In order to describe the support vector\nmachine, I'm actually going to start with logistic regression, and\nshow how we can modify it a bit, and get what is essentially\nthe support vector machine. So in logistic regression, we have our\nfamiliar form of the hypothesis there and the sigmoid activation\nfunction shown on the right. And in order to explain some of the math, I'm going to use z to denote\ntheta transpose axiom. Now let's think about what we would\nlike logistic regression to do. If we have an example with y equals\none and by this I mean an example in either the training set or the test\nset or the cross-validation set, but when y is equal to one then we're sort of\nhoping that h of x will be close to one. Right, we're hoping to correctly\nclassify that example. And what having x subscript 1, what that means is that theta transpose\nx must be must larger than 0. So there's greater than, greater than sign\nthat means much, much greater than 0. And that's because it is z, the theta of transpose x is when z is much bigger than\n0 is far to the right of the sphere. That the outputs of logistic\nprogression becomes close to one. Conversely, if we have an example where y\nis equal to zero, then what we're hoping for is that the hypothesis will\noutput a value close to zero. And that corresponds to theta transpose\nx of z being much less than zero because that corresponds to a hypothesis\nof putting a value close to zero. If you look at the cost function\nof logistic regression, what you'll find is that each\nexample (x,y) contributes a term like this to the overall\ncost function, right? So for the overall cost function,\nwe will also have a sum over all the chain examples and the 1 over m term,\nthat this expression here, that's the term that a single\ntraining example contributes to the overall objective function so\nwe can just rush them. Now if I take the definition for\nthe fall of my hypothesis and plug it in over here,\nthen what I get is that each training example contributes this term,\nignoring the one over M but it contributes that term to my overall\ncost function for logistic regression. Now let's consider two cases of when y is\nequal to one and when y is equal to zero. In the first case,\nlet's suppose that y is equal to 1. In that case, only this first\nterm in the objective matters, because this one minus y term would be\nequal to zero if y is equal to one. So when y is equal to one,\nwhen in our example x comma y, when y is equal to 1 what\nwe get is this term.. Minus log one over one, plus E to the\nnegative Z where as similar to the last line I'm using Z to denote data\ntransposed X and of course in a cost I should have this minus line that we just\nhad if Y is equal to one so that's equal to one I just simplify in a way in the\nexpression that I have written down here. And if we plot this function as a function\nof z, what you find is that you get this curve shown on the lower\nleft of the slide. And thus, we also see that when\nz is equal to large, that is, when theta transpose x is large, that\ncorresponds to a value of z that gives us a fairly small value, a very, very\nsmall contribution to the consumption. And this kinda explains why, when logistic\nregression sees a positive example, with y=1, it tries to set theta\ntransport x to be very large because that corresponds to this term,\nin the cross function, being small. Now, to fill the support vec machine,\nhere's what we're going to do. We're gonna take this cross function, this\nminus log 1 over 1 plus e to negative z, and modify it a little bit. Let me take this point 1 over here, and let me draw the cross\nfunctions you're going to use. The new pass functions can be\nflat from here on out, and then we draw something that\ngrows as a straight line, similar to logistic regression. But this is going to be\na straight line at this portion. So the curve that I just drew in magenta,\nand the curve I just drew purple and magenta, so\nif it's pretty close approximation to the cross function used\nby logistic regression. Except it is now made up of two line\nsegments, there's this flat portion on the right, and then there's this\nstraight line portion on the left. And don't worry too much about\nthe slope of the straight line portion. It doesn't matter that much. But that's the new cost function we're\ngoing to use for when y is equal to one, and you can imagine it should do something\npretty similar to logistic regression. But turns out, that this will\ngive the support vector machine computational advantages and give us,\nlater on, an easier optimization problem that would be easier for\nsoftware to solve. We just talked about\nthe case of y equals one. The other case is if y is equal to zero. In that case, if you look at the cost, then only the second term will apply\nbecause the first term goes away, right? If y is equal to zero,\nthen you have a zero here, so you're left only with the second\nterm of the expression above. And so the cost of an example, or the contribution of the cost function, is\ngoing to be given by this term over here. And if you plot that as a function of z, to have pure z on the horizontal axis,\nyou end up with this one. And for the support vector machine,\nonce again, we're going to replace this blue\nline with something similar and at the same time we replace it with a new\ncost, this flat out here, this 0 out here. And that then grows as a straight line,\nlike so. So let me give these two functions names. This function on the left I'm going\nto call cost subscript 1 of z, and this function of the right I'm\ngonna call cost subscript 0 of z. And the subscript just refers to the cost\ncorresponding to when y is equal to 1, versus when y Is equal to zero. Armed with these definitions, we're now\nready to build a support vector machine. Here's the cost function, j of theta,\nthat we have for logistic regression. In case this equation looks a bit\nunfamiliar, it's because previously we had a minus sign outside, but here what\nI did was I instead moved the minus signs inside these expressions, so\nit just makes it look a little different. For the support vector machine what we're\ngoing to do is essentially take this and replace this with cost1 of z,\nthat is cost1 of theta transpose x. And we're going to take this and\nreplace it with cost0 of z, that is cost0 of theta transpose x. Where the cost one function is what we had on the previous\nslide that looks like this. And the cost zero function, again what\nwe had on the previous slide, and it looks like this. So what we have for\nthe support vector machine is a minimization problem of one over M, the sum of Y I times cost one, theta transpose X I, plus one minus Y I times cause zero of theta transpose X I, and then plus my usual\nregularization parameter. Like so. Now, by convention, for\nthe support of vector machine, we're actually write\nthings slightly different. We re-parameterize this just\nvery slightly differently. First, we're going to get rid\nof the 1 over m terms, and this just this happens to be a slightly\ndifferent convention that people use for support vector machines compared to or\njust a progression. But here's what I mean. You're one way to do this, we're just\ngonna get rid of these one over m terms and this should give you me the same\noptimal value of beta right? Because one over m is just as constant so whether I solved this minimization\nproblem with one over n in front or not. I should end up with the same\noptimal value for theta. Here's what I mean, to give you an example,\nsuppose I had a minimization problem. Minimize over a long number U of\nU minus five squared plus one. Well, the minimum of this\nhappens to be U equals five. Now if I were to take this objective\nfunction and multiply it by 10. So here my minimization problem is min\nover U, 10 U minus five squared plus 10. Well the value of U that minimizes\nthis is still U equals five right? So multiply something that you're\nminimizing over, by some constant, 10 in this case, it does not change\nthe value of U that gives us, that minimizes this function. So the same way,\nwhat I've done is by crossing out the M is all I'm doing is multiplying my\nobjective function by some constant M and it doesn't change the value of theta. That achieves the minimum. The second bit of notational change,\nwhich is just, again, the more standard convention when using SVMs instead of\nlogistic regression, is the following. So for logistic regression, we add\ntwo terms to the objective function. The first is this term, which is the cost\nthat comes from the training set and the second is this row,\nwhich is the regularization term. And what we had was we had a, we control\nthe trade-off between these by saying, what we want is A plus, and\nthen my regularization parameter lambda. And then times some other term B,\nwhere I guess I'm using your A to denote this first term, and I'm using B to denote\nthe second term, maybe without the lambda. And instead of prioritizing\nthis as A plus lambda B, and so what we did was by\nsetting different values for this regularization parameter lambda,\nwe could trade off the relative weight between how much we wanted the training\nset well, that is, minimizing A, versus how much we care about keeping\nthe values of the parameter small, so that will be, the parameter is B for\nthe support vector machine, just by convention,\nwe're going to use a different parameter. So instead of using lambda here to control\nthe relative waiting between the first and second terms. We're instead going to use a different\nparameter which by convention is called C and is set to minimize C times a + B. So for logistic regression,\nif we set a very large value of lambda, that means you will give\nB a very high weight. Here is that if we set C\nto be a very small value, then that responds to giving B\na much larger rate than C, than A. So this is just a different way of\ncontrolling the trade off, it's just a different way of prioritizing how much\nwe care about optimizing the first term, versus how much we care about\noptimizing the second term. And if you want you can think\nof this as the parameter C playing a role similar to 1 over lambda. And it's not that it's two equations or\nthese two expressions will be equal. This equals 1 over lambda,\nthat's not the case. It's rather that if C is equal to 1\nover lambda, then these two optimization objectives should give you the same value\nthe same optimal value for theta so we just filling that in I'm\ngonna cross out lambda here and write in the constant C there. So that gives us our overall\noptimization objective function for the support vector machine. And if you minimize that function, then what you have is\nthe parameters learned by the SVM. Finally unlike logistic regression,\nthe support vector machine doesn't output the probability is that what we\nhave is we have this cost function, that we minimize to get the parameter's\ndata, and what a support vector machine does is it just makes a prediction of y\nbeing equal to one or zero, directly. So the hypothesis will predict one if theta transpose x is greater or\nequal to zero, and it will predict zero otherwise and so\nhaving learned the parameters theta, this is the form of the hypothesis for\nthe support vector machine. So that was a mathematical definition\nof what a support vector machine does. In the next few videos,\nlet's try to get back to intuition about what this optimization\nobjective leads to and whether the source of the hypotheses SVM\nwill learn and we'll also talk about how to modify this just a little bit to\nthe complex nonlinear functions.""",71,0,1
coursera,stanford_university,machine-learning,large-margin-intuition,"b""Sometimes people talk about support vector machines, as large margin classifiers, in this video I'd like to tell you what that means, and this will also give us a useful picture of what an SVM hypothesis may look like. Here's my cost function for the support vector machine where here on the left I've plotted my cost 1 of z function that I used for positive examples and on the right I've  plotted my zero of 'Z' function, where I have 'Z' here on the horizontal axis. Now, let's think about what it takes to make these cost functions small. If you have a positive example, so if y is equal to 1, then cost 1 of Z is zero only when Z is greater than or equal to 1. So in other words, if you have a positive example, we really want theta transpose x to be greater than or equal to 1 and conversely if y is equal to zero, look this cost zero of z function, then it's only in this region where z is less than equal to 1 we have the cost is zero as z is equals to zero, and this is an interesting property of the support vector machine right, which is that, if you have a positive example so if y is equal to one, then all we really need is that theta transpose x is greater than equal to zero. And that would mean that we classify correctly because if theta transpose x is greater than zero our hypothesis will predict zero. And similarly, if you have a negative example, then really all you want is that theta transpose x is less than zero and that will make sure we got the example right. But the support vector machine wants a bit more than that. It says, you know, don't just barely get the example right. So then don't just have it just a little bit bigger than zero. What i really want is for this to be quite a lot bigger than zero say maybe bit greater or equal to one and I want this to be much less than zero. Maybe I want it less than or equal to -1. And so this builds in an extra safety factor or safety margin factor into the support vector machine. Logistic regression does something similar too of course, but let's see what happens or let's see what the consequences of this are, in the context of the support vector machine. Concretely, what I'd like to do next is consider a case case where we set this constant C to be a very large value, so let's imagine we set C to a very large value, may be a hundred thousand, some huge number. Let's see what the support vector machine will do. If C is very, very large, then when minimizing this optimization objective, we're going to be highly motivated to choose a value, so that this first term is equal to zero. So let's try to understand the optimization problem in the context of, what would it take to make this first term in the objective equal to zero, because you know, maybe we'll set C to some huge constant, and this will hope, this should give us additional intuition about what sort of hypotheses a support vector machine learns. So we saw already that whenever you have a training example with a label of y=1 if you want to make that first term zero, what you need is is to find a value of theta so that theta transpose x i is greater than or equal to 1. And similarly, whenever we have an example, with label zero, in order to make sure that the cost, cost zero of Z,  in order to make sure that cost is zero we need that theta transpose x i is less than or equal to -1. So, if we think of our optimization problem as now, really choosing parameters and show that this first term is equal to zero, what we're left with is the following optimization problem. We're going to minimize that first term zero, so C times zero, because we're going to choose parameters so that's equal to zero, plus one half and then you know that second term and this first term is 'C' times zero, so let's just cross that out because I know that's going to be zero. And this will be subject to the constraint that theta transpose x(i) is greater than or equal to one, if y(i) Is equal to one and theta transpose x(i) is less than or equal to minus one whenever you have a negative example and it turns out that when you solve this optimization problem, when you minimize this as a function of the parameters theta you get a very interesting decision boundary. Concretely, if you look at a data set like this with positive and negative examples,  this data is linearly separable and by that, I mean that there exists, you know, a straight line, altough there is many a different straight lines, they can separate the positive and negative examples perfectly. For example, here is one decision boundary that separates the positive and negative examples, but somehow that doesn't look like a very natural one, right? Or by drawing an even worse one, you know here's another decision boundary that separates the positive and negative examples but just barely. But neither of those seem like particularly good choices. The Support Vector Machines will instead choose this decision boundary, which I'm drawing in black. And that seems like a much better decision boundary than either of the ones that I drew in magenta or in green. The black line seems like a more robust separator, it does a better job of separating the positive and negative examples. And mathematically, what that does is, this black decision boundary has a larger distance. That distance is called the margin, when I draw up this two extra blue lines, we see that the black decision boundary has some larger minimum distance from any of my training examples, whereas the magenta and the green lines they come awfully close to the training examples. and then that seems to do a less a good job separating the positive and negative classes than my black line. And so this distance is called the margin of the support vector machine and this gives the SVM a certain robustness, because it tries to separate the data with as a large a margin as possible. So the support vector machine is sometimes also called a large margin classifier and this is actually a consequence of the optimization problem we wrote down on the previous slide. I know that you might be wondering how is it that the optimization problem I wrote down in the previous slide, how does that lead to this large margin classifier. I know I haven't explained that yet. And in the next video I'm going to sketch a little bit of the intuition about why that optimization problem gives us this large margin classifier. But this is a useful feature to keep in mind if you are trying to understand what are the sorts of hypothesis that an SVM will choose. That is, trying to separate the positive and negative examples with as big a margin as possible. I want to say one last thing about large margin classifiers in this intuition, so we wrote out this large margin classification setting in the case of when C, that regularization concept, was very large, I think I set that to a hundred thousand or something. So given a dataset like this, maybe we'll choose that decision boundary that separate the positive and negative examples on large margin. Now, the SVM is actually sligthly more sophisticated than this large margin view might suggest. And in particular, if all you're doing is use a large margin classifier then your learning algorithms can be sensitive to outliers, so lets just add an extra positive example like that shown on the screen. If he had one example then it seems as if to separate data with a large margin, maybe I'll end up learning a decision boundary like that, right? that is the magenta line and it's really not clear that based on the single outlier based on a single example and it's really not clear that it's actually a good idea to change my decision boundary from the black one over to the magenta one. So, if C, if the regularization parameter C were very large, then this is actually what SVM will do, it will change the decision boundary from the black to the magenta one but if C were reasonably small if you were to use the C, not too large then you still end up with this black decision boundary. And of course if the data were not linearly separable so if you had some positive examples in here, or if you had some negative examples in here then the SVM will also do the right thing. And so this picture of a large margin classifier that's really, that's really the picture that gives better intuition only for the case of when the regulations parameter C is very large, and just to remind you this corresponds C plays a role similar to one over Lambda, where Lambda is the regularization parameter we had previously. And so it's only of one over Lambda is very large or equivalently if Lambda is very small that you end up with things like this Magenta decision boundary, but in practice when applying support vector machines, when C is not very very large like that, it can do a better job ignoring the few outliers like here. And also do fine and  do reasonable things even if your data is not linearly separable. But when we talk about bias and variance in the context of support vector machines which will do a little bit later, hopefully all of of this trade-offs involving the regularization parameter will become clearer at that time. So I hope that gives some intuition about how this support vector machine functions as a large margin classifier that tries to separate the data with a large margin, technically this picture of this view is true only when the parameter C is very large, which is a useful way to think about support vector machines. There was one missing step in this video which is, why is it that the optimization problem we wrote down on these slides, how does that actually lead to the large margin classifier, I didn't do that in this video, in the next video I will sketch a little bit more of the math behind that to explain that separate reasoning of how the optimization problem we wrote out results in a large margin classifier.""",72,0,1
coursera,stanford_university,machine-learning,mathematics-behind-large-margin-classification,"b""In this video, I'd like to tell you a bit about the math behind large margin classification. This video is optional, so please feel free to skip it. It may also give you better intuition about how the optimization problem of the support vex machine, how that leads to large margin classifiers. In order to get started, let me first remind you of a couple of properties of what vector inner products look like. Let's say I have two vectors U and V, that look like this. So both two dimensional vectors. Then let's see what U transpose V looks like. And U transpose V is also called the inner products between the vectors U and V. Use a two dimensional vector, so I can on plot it on this figure. So let's say that's the vector U. And what I mean by that is if on the horizontal axis that value takes whatever value U1 is and on the vertical axis the height of that is whatever U2 is the second component of the vector U. Now, one quantity that will be nice to have is the norm of the vector U. So, these are, you know, double bars on the left and right that denotes the norm or length of U. So this just means; really the euclidean length of the vector U. And this is Pythagoras theorem is just equal to U1 squared plus U2 squared square root, right? And this is the length of the vector U. That's a real number. Just say you know, what is the length of this, what is the length of this vector down here. What is the length of this arrow that I just drew, is the normal view? Now let's go back and look at the vector V because we want to compute the inner product. So V will be some other vector with, you know, some value V1, V2. And so, the vector V will look like that, towards V like so. Now let's go back and look at how to compute the inner product between U and V. Here's how you can do it. Let me take the vector V and project it down onto the vector U. So I'm going to take a orthogonal projection or a 90 degree projection, and project it down onto U like so. And what I'm going to do measure length of this red line that I just drew here. So, I'm going to call the length of that red line P. So, P is the length or is the magnitude of the projection of the vector V onto the vector U. Let me just write that down. So, P is the length of the projection of the vector V onto the vector U. And it is possible to show that unit product U transpose V, that this is going to be equal to P  times the norm or the length of the vector U. So, this is one way to compute the inner product. And if you actually do the geometry figure out what P is and figure out what the norm of U is. This should give you the same way, the same answer as the other way of computing unit product. Right. Which is if you take U transpose V then U transposes this U1 U2, its a one by two matrix, 1 times V. And so this should actually give you U1, V1 plus U2, V2. And so the theorem of linear algebra that these two formulas give you the same answer. And by the way, U transpose V is also equal to V transpose U. So if you were to do the same process in reverse, instead of projecting V onto U, you could project U onto V. Then, you know, do the same process, but with the rows of U and V reversed. And you would actually, you should actually get the same number whatever that number is. And just to clarify what's going on in this equation the norm of U is a real number and P is also a real number. And so U transpose V is the regular multiplication as two real numbers of the length of P times the normal view. Just one last detail, which is if you look at the norm of P, P is actually signed so to the right. And it can either be positive or negative. So let me say what I mean by that, if U is a vector that looks like this and V is a vector that looks like this. So if the angle between U and V is greater than ninety degrees. Then if I project V onto U, what I get is a projection it looks like this and so that length P. And in this case, I will still have that U transpose V is equal to P times the norm of U. Except in this example P will be negative. So, you know, in inner products if the angle between U and V is less than ninety degrees, then P is the positive length for that red line whereas if the angle of this angle of here is greater than 90 degrees then P here will be negative of the length of the super line of that little line segment right over there. So the inner product between two vectors can also be negative if the angle between them is greater than 90 degrees. So that's how vector inner products work. We're going to use these properties of vector inner product to try to understand the support vector machine optimization objective over there. Here is the optimization objective for the support vector machine that we worked out earlier. Just for the purpose of this slide I am going to make one simplification or once just to make the objective easy to analyze and what I'm going to do is ignore the indeceptrums. So, we'll just ignore theta 0 and set that to be equal to 0. To make things easier to plot, I'm also going to set N the number of features to be equal to 2. So, we have only 2 features, X1 and X2. Now, let's look at the objective function. The optimization objective of the SVM. What we have only two features. When N is equal to 2. This can be written, one half of theta one squared plus theta two squared. Because we only have two parameters, theta one and thetaa two. What I'm going to do is rewrite this a bit. I'm going to write this as one half of theta one squared plus theta two squared and the square root squared. And the reason I can do that, is because for any number, you know, W, right, the square roots of W and then squared, that's just equal to W. So square roots and squared should give you the same thing. What you may notice is that this term inside is that's equal to the norm or the length of the vector theta and what I mean by that is that if we write out the vector theta like this, as you know theta one, theta two. Then this term that I've just underlined in red, that's exactly the length, or the norm, of the vector theta. We are calling the definition of the norm of the vector that we have on the previous line. And in fact this is actually equal to the length of the vector theta, whether you write it as theta zero, theta 1, theta 2. That is, if theta zero is equal to zero, as I assume here. Or just the length of theta 1, theta 2; but for this line I am going to ignore theta 0. So let me just, you know, treat theta as this, let me just write theta, the normal theta as this theta 1, theta 2 only, but the math works out either way, whether we include theta zero here or not. So it's not going to matter for the rest of our derivation. And so finally this means that my optimization objective is equal to one half of the norm of theta squared. So all the support vector machine is doing in the optimization objective is it's minimizing the squared norm of the square length of the parameter vector theta. Now what I'd like to do is look at these terms, theta transpose X and understand better what they're doing. So given the parameter vector theta and given and example x, what is this is equal to? And on the previous slide, we figured out what U transpose V looks like, with different vectors U and V. And so we're going to take those definitions, you know, with theta and X(i) playing the roles of U and V. And let's see what that picture looks like. So, let's say I plot. Let's say I look at just a single training example. Let's say I have a positive example the drawing was across there and let's say that is my example X(i), what that really means is plotted on the horizontal axis some value X(i) 1 and on the vertical axis X(i) 2. That's how I plot my training examples. And although we haven't been really thinking of this as a vector, what this really is, this is a vector from the origin from 0, 0 out to the location of this training example. And now let's say we have a parameter vector and I'm going to plot that as vector, as well. What I mean by that is if I plot theta 1 here and theta 2 there so what is the inner product theta transpose X(i). While using our earlier method, the way we compute that is we take my example and project it onto my parameter vector theta. And then I'm going to look at the length of this segment that I'm coloring in, in red. And I'm going to call that P superscript I to denote that this is a projection of the i-th training example onto the parameter vector theta. And so what we have is that theta transpose X(i) is equal to following what we have on the previous slide, this is going to be equal to P times the length of the norm of the vector theta. And this is of course also equal to theta 1 x1 plus theta 2 x2. So each of these is, you know, an equally valid way of computing the inner product between theta and X(i). Okay. So where does this leave us? What this means is that, this constrains that theta transpose X(i) be greater than or equal to one or less than minus one. What this means is that it can replace the use of constraints that P(i) times X be greater than or equal to one. Because theta transpose X(i) is equal to P(i) times the norm of theta. So writing that into our optimization objective. This is what we get where I have, instead of theta transpose X(i), I now have this P(i) times the norm of theta. And just to remind you we worked out earlier too that this optimization objective can be written as one half times the norm of theta squared. So, now let's consider the training example that we have at the bottom and for now, continuing to use the simplification that theta 0 is equal to 0. Let's see what decision boundary the support vector machine will choose. Here's one option, let's say the support vector machine were to choose this decision boundary. This is not a very good choice because it has very small margins. This decision boundary comes very close to the training examples. Let's see why the support vector machine will not do this. For this choice of parameters it's possible to show that the parameter vector theta is actually at 90 degrees to the decision boundary. And so, that green decision boundary corresponds to a parameter vector theta that points in that direction. And by the way, the simplification that theta 0 equals 0 that just means that the decision boundary must pass through the origin, (0,0) over there. So now, let's look at what this implies for the optimization objective. Let's say that this example here. Let's say that's my first example, you know, X1. If we look at the projection of this example onto my parameters theta. That's the projection. And so that little red line segment. That is equal to P1. And that is going to be pretty small, right. And similarly, if this example here, if this happens to be X2, that's my second example. Then, if I look at the projection of this this example onto theta. You know. Then, let me draw this one in magenta. This little magenta line segment, that's going to be P2. That's the projection of the second example onto my, onto the direction of my parameter vector theta which goes like this. And so, this little projection line segment is getting pretty small. P2 will actually be a negative number, right so P2 is in the opposite direction. This vector has greater than 90 degree angle with my parameter vector theta, it's going to be less than 0. And so what we're finding is that these terms P(i) are going to be pretty small numbers. So if we look at the optimization objective and see, well, for positive examples we need P(i) times the norm of theta to be bigger than either one. But if P(i) over here, if P1 over here is pretty small, that means that we need the norm of theta to be pretty large, right? If P1 of theta is small and we want P1 you know times in all of theta to be bigger than either one, well the only way for that to be true for the profit that these two numbers to be large if P1 is small, as we said we want the norm of theta to be large. And similarly for our negative example, we need P2 times the norm of theta to be less than or equal to minus one. And we saw in this example already that P2 is going pretty small negative number, and so the only way for that to happen as well is for the norm of theta to be large, but what we are doing in the optimization objective is we are trying to find a setting of parameters where the norm of theta is small, and so you know, so this doesn't seem like such a good direction for the parameter vector and theta. In contrast, just look at a different decision boundary. Here, let's say, this SVM chooses that decision boundary. Now the is going to be very different. If that is the decision boundary, here is the corresponding direction for theta. So, with the direction boundary you know, that vertical line that corresponds to it is possible to show using linear algebra that the way to get that green decision boundary is have the vector of theta be at 90 degrees to it, and now if you look at the projection of your data onto the vector x, lets say its before this example is my example of x1. So when I project this on to x, or onto theta, what I find is that this is P1. That length there is P1. The other example, that example is and I do the same projection and what I find is that this length here is a P2 really that is going to be less than 0. And you notice that now P1 and P2, these lengths of the projections are going to be much bigger, and so if we still need to enforce these constraints that P1 of the norm of theta is phase number one because P1 is so much bigger now. The normal can be smaller. And so, what this means is that by choosing the decision boundary shown on the right instead of on the left, the SVM can make the norm of the parameters theta much smaller. So, if we can make the norm of theta smaller and therefore make the squared norm of theta smaller, which is why the SVM would choose this hypothesis on the right instead. And this is how the SVM gives rise to this large margin certification effect. Mainly, if you look at this green line, if you look at this green hypothesis we want the projections of my positive and negative examples onto theta to be large, and the only way for that to hold true this is if surrounding the green line. There's this large margin, there's this large gap that separates positive and negative examples is really the magnitude of this gap. The magnitude of this margin is exactly the values of P1, P2, P3 and so on. And so by making the margin large, by these tyros P1, P2, P3 and so on that's the SVM can end up with a smaller value for the norm of theta which is what it is trying to do in the objective. And this is why this machine ends up with enlarge margin classifiers because itss trying to maximize the norm of these P1 which is the distance from the training examples to the decision boundary. Finally, we did this whole derivation using this simplification that the parameter theta 0 must be equal to 0. The effect of that as I mentioned briefly, is that if theta 0 is equal to 0 what that means is that we are entertaining decision boundaries that pass through the origins of decision boundaries pass through the origin like that, if you allow theta zero to be non 0 then what that means is that you entertain the decision boundaries that did not cross through the origin, like that one I just drew. And I'm not going to do the full derivation that. It turns out that this same large margin proof works in pretty much in exactly the same way. And there's a generalization of this argument that we just went through them long ago through that shows that even when theta 0 is non 0, what the SVM is trying to do when you have this optimization objective. Which again corresponds to the case of when C is very large. But it is possible to show that, you know, when theta is not equal to 0 this support vector machine is still finding is really trying to find the large margin separator that between the positive and negative examples. So that explains how this support vector machine is a large margin classifier. In the next video we will start to talk about how to take some of these SVM ideas and start to apply them to build a complex nonlinear classifiers.""",73,0,1
coursera,stanford_university,machine-learning,kernels-i,"b""In this video, I'd like to start adapting support vector machines in order to develop complex nonlinear classifiers. The main technique for doing that is something called kernels. Let's see what this kernels are and how to use them. If you have a training set that looks like this, and you want to find a nonlinear decision boundary to distinguish the positive and negative examples, maybe a decision boundary that looks like that. One way to do so is to come up with a set of complex polynomial features, right? So, set of features that looks like this, so that you end up with a hypothesis X that predicts 1 if you know that theta 0 and plus theta 1 X1 plus dot dot dot all those polynomial features is greater than 0, and predict 0, otherwise. And another way of writing this, to introduce a level of new notation that I'll use later, is that we can think of a hypothesis as computing a decision boundary using this. So, theta 0 plus theta 1 f1 plus theta 2, f2 plus theta 3, f3 plus and so on. Where I'm going to use this new denotation f1, f2, f3 and so on to denote these new sort of features that I'm computing, so f1 is just X1, f2 is equal to X2, f3 is equal to this one here. So, X1X2. So, f4 is equal to X1 squared where f5 is to be x2 squared and so on and we seen previously that coming up with these high order polynomials is one way to come up with lots more features, the question is, is there a different choice of features or is there better sort of features than this high order polynomials because you know it's not clear that this high order polynomial is what we want, and what we talked about computer vision talk about when the input is an image with lots of pixels. We also saw how using high order polynomials becomes very computationally expensive because there are a lot of these higher order polynomial terms. So, is there a different or a better choice of the features that we can use to plug into this sort of hypothesis form. So, here is one idea for how to define new features f1, f2, f3. On this line I am going to define only three new features, but for real problems we can get to define a much larger number. But here's what I'm going to do in this phase of features X1, X2, and I'm going to leave X0 out of this, the interceptor X0, but in this phase X1 X2, I'm going to just, you know, manually pick a few points, and then call these points l1, we are going to pick a different point, let's call that l2 and let's pick the third one and call this one l3, and for now let's just say that I'm going to choose these three points manually. I'm going to call these three points line ups, so line up one, two, three. What I'm going to do is define my new features as follows, given an example X, let me define my first feature f1 to be some measure of the similarity between my training example X and my first landmark and this specific formula that I'm going to use to measure similarity is going to be this is E to the minus the length of X minus l1, squared, divided by two sigma squared. So, depending on whether or not you watched the previous optional video, this notation, you know, this is the length of the vector W. And so, this thing here, this X minus l1, this is actually just the euclidean distance squared, is the euclidean distance between the point x and the landmark l1. We will see more about this later. But that's my first feature, and my second feature f2 is going to be, you know, similarity function that measures how similar X is to l2 and the game is going to be defined as the following function. This is E to the minus of the square of the euclidean distance between X and the second landmark, that is what the enumerator is and then divided by 2 sigma squared and similarly f3 is, you know, similarity between X and l3, which is equal to, again, similar formula. And what this similarity function is, the mathematical term for this, is that this is going to be a kernel function. And the specific kernel I'm using here, this is actually called a Gaussian kernel. And so this formula, this particular choice of similarity function is called a Gaussian kernel. But the way the terminology goes is that, you know, in the abstract these different similarity functions are called kernels and we can have different similarity functions and the specific example I'm giving here is called the Gaussian kernel. We'll see other examples of other kernels. But for now just think of these as similarity functions. And so, instead of writing similarity between X and l, sometimes we also write this a kernel denoted you know, lower case k between x and one of my landmarks all right. So let's see what a criminals actually do and why these sorts of similarity functions, why these expressions might make sense. So let's take my first landmark. My landmark l1, which is one of those points I chose on my figure just now. So the similarity of the kernel between x and l1 is given by this expression. Just to make sure, you know, we are on the same page about what the numerator term is, the numerator can also be written as a sum from J equals 1 through N on sort of the distance. So this is the component wise distance between the vector X and the vector l. And again for the purpose of these slides I'm ignoring X0. So just ignoring the intercept term X0, which is always equal to 1. So, you know, this is how you compute the kernel with similarity between X and a landmark. So let's see what this function does. Suppose X is close to one of the landmarks. Then this euclidean distance formula and the numerator will be close to 0, right. So, that is this term here, the distance was great, the distance using X and 0 will be close to zero, and so f1, this is a simple feature, will be approximately E to the minus 0 and then the numerator squared over 2 is equal to squared so that E to the 0, E to minus 0, E to 0 is going to be close to one. And I'll put the approximation symbol here because the distance may not be exactly 0, but if X is closer to landmark this term will be close to 0 and so f1 would be close 1. Conversely, if X is far from 01 then this first feature f1 will be E to the minus of some large number squared, divided divided by two sigma squared and E to the minus of a large number is going to be close to 0. So what these features do is they measure how similar X is from one of your landmarks and the feature f is going to be close to one when X is close to your landmark and is going to be 0 or close to zero when X is far from your landmark. Each of these landmarks. On the previous line, I drew three landmarks, l1, l2,l3. Each of these landmarks, defines a new feature f1, f2 and f3. That is, given the the training example X, we can now compute three new features: f1, f2, and f3, given, you know, the three landmarks that I wrote just now. But first, let's look at this exponentiation function, let's look at this similarity function and plot in some figures and just, you know, understand better what this really looks like. For this example, let's say I have two features X1 and X2. And let's say my first landmark, l1 is at a location, 3 5. So and let's say I set sigma squared equals one for now. If I plot what this feature looks like, what I get is this figure. So the vertical axis, the height of the surface is the value of f1 and down here on the horizontal axis are, if I have some training example, and there is x1 and there is x2. Given a certain training example, the training example here which shows the value of x1 and x2 at a height above the surface, shows the corresponding value of f1 and down below this is the same figure I had showed, using a quantifiable plot, with x1 on horizontal axis, x2 on horizontal axis and so, this figure on the bottom is just a contour plot of the 3D surface. You notice that when X is equal to 3 5 exactly, then we the f1 takes on the value 1, because that's at the maximum and X moves away as X goes further away then this feature takes on values that are close to 0. And so, this is really a feature, f1 measures, you know, how close X is to the first landmark and if varies between 0 and one depending on how close X is to the first landmark l1. Now the other was due on this slide is show the effects of varying this parameter sigma squared. So, sigma squared is the parameter of the Gaussian kernel and as you vary it, you get slightly different effects. Let's set sigma squared to be equal to 0.5 and see what we get. We set sigma square to 0.5, what you find is that the kernel looks similar, except for the width of the bump becomes narrower. The contours shrink a bit too. So if sigma squared equals to 0.5 then as you start from X equals 3 5 and as you move away, then the feature f1 falls to zero much more rapidly and conversely, if you has increase since where three in that case and as I move away from, you know l. So this point here is really l, right, that's l1 is at location 3 5, right. So it's shown up here. And if sigma squared is large, then as you move away from l1, the value of the feature falls away much more slowly. So, given this definition of the features, let's see what source of hypothesis we can learn. Given the training example X, we are going to compute these features f1, f2, f3 and a hypothesis is going to predict one when theta 0 plus theta 1 f1 plus theta 2 f2, and so on is greater than or equal to 0. For this particular example, let's say that I've already found a learning algorithm and let's say that, you know, somehow I ended up with these values of the parameter. So if theta 0 equals minus 0.5, theta 1 equals 1, theta 2 equals 1, and theta 3 equals 0 And what I want to do is consider what happens if we have a training example that takes has location at this magenta dot, right where I just drew this dot over here. So let's say I have a training example X, what would my hypothesis predict? Well, If I look at this formula. Because my training example X is close to l1, we have that f1 is going to be close to 1 the because my training example X is far from l2 and l3 I have that, you know, f2 would be close to 0 and f3 will be close to 0. So, if I look at that formula, I have theta 0 plus theta 1 times 1 plus theta 2 times some value. Not exactly 0, but let's say close to 0. Then plus theta 3 times something close to 0. And this is going to be equal to plugging in these values now. So, that gives minus 0.5 plus 1 times 1 which is 1, and so on. Which is equal to 0.5 which is greater than or equal to 0. So, at this point, we're going to predict Y equals 1, because that's greater than or equal to zero. Now let's take a different point. Now lets' say I take a different point, I'm going to draw this one in a different color, in cyan say, for a point out there, if that were my training example X, then if you make a similar computation, you find that f1, f2, Ff3 are all going to be close to 0. And so, we have theta 0 plus theta 1, f1, plus so on and this will be about equal to minus 0.5, because theta 0 is minus 0.5 and f1, f2, f3 are all zero. So this will be minus 0.5, this is less than zero. And so, at this point out there, we're going to predict Y equals zero. And if you do this yourself for a range of different points, be sure to convince yourself that if you have a training example that's close to L2, say, then at this point we'll also predict Y equals one. And in fact, what you end up doing is, you know, if you look around this boundary, this space, what we'll find is that for points near l1 and l2 we end up predicting positive. And for points far away from l1 and l2, that's for points far away from these two landmarks, we end up predicting that the class is equal to 0. As so, what we end up doing,is that the decision boundary of this hypothesis would end up looking something like this where inside this red decision boundary would predict Y equals 1 and outside we predict Y equals 0. And so this is how with this definition of the landmarks and of the kernel function. We can learn pretty complex non-linear decision boundary, like what I just drew where we predict positive when we're close to either one of the two landmarks. And we predict negative when we're very far away from any of the landmarks. And so this is part of the idea of kernels of and how we use them with the support vector machine, which is that we define these extra features using landmarks and similarity functions to learn more complex nonlinear classifiers. So hopefully that gives you a sense of the idea of kernels and how we could use it to define new features for the Support Vector Machine. But there are a couple of questions that we haven't answered yet. One is, how do we get these landmarks? How do we choose these landmarks? And another is, what other similarity functions, if any, can we use other than the one we talked about, which is called the Gaussian kernel. In the next video we give answers to these questions and put everything together to show how support vector machines with kernels can be a powerful way to learn complex nonlinear functions.""",74,0,1
coursera,stanford_university,machine-learning,kernels-ii,"b""In the last video, we started to talk about the kernels idea and how it can be used to define new features for the support vector machine. In this video, I'd like to throw in some of the missing details and, also, say a few words about how to use these ideas in practice. Such as, how they pertain to, for example, the bias variance trade-off in support vector machines. In the last video, I talked about the process of picking a few landmarks. You know, l1, l2, l3 and that allowed us to define the similarity function also called the kernel or in this example if you have this similarity function this is a Gaussian kernel. And that allowed us to build this form of a hypothesis function. But where do we get these landmarks from? Where do we get l1, l2, l3 from? And it seems, also, that for complex learning problems, maybe we want a lot more landmarks than just three of them that we might choose by hand. So in practice this is how the landmarks are chosen which is that given the machine learning problem. We have some data set of some some positive and negative examples. So, this is the idea here which is that we're gonna take the examples and for every training example that we have, we are just going to call it. We're just going to put landmarks as exactly the same locations as the training examples. So if I have one training example if that is x1, well then I'm going to choose this is my first landmark to be at xactly the same location as my first training example. And if I have a different training example x2. Well we're going to set the second landmark to be the location of my second training example. On the figure on the right, I used red and blue dots just as illustration, the color of this figure, the color of the dots on the figure on the right is not significant. But what I'm going to end up with using this method is I'm going to end up with m landmarks of l1, l2 down to l(m) if I have m training examples with one landmark per location of my per location of each of my training examples. And this is nice because it is saying that my features are basically going to measure how close an example is to one of the things I saw in my training set. So, just to write this outline a little more concretely, given m training examples, I'm going to choose the the location of my landmarks to be exactly near the locations of my m training examples. When you are given example x, and in this example x can be something in the training set, it can be something in the cross validation set, or it can be something in the test set. Given an example x we are going to compute, you know, these features as so f1, f2, and so on. Where l1 is actually equal to x1 and so on. And these then give me a feature vector. So let me write f as the feature vector. I'm going to take these f1, f2 and so on, and just group them into feature vector. Take those down to fm. And, you know, just by convention. If we want, we can add an extra feature f0, which is always equal to 1. So this plays a role similar to what we had previously. For x0, which was our interceptor. So, for example, if we have a training example x(i), y(i), the features we would compute for this training example will be as follows: given x(i), we will then map it to, you know, f1(i). Which is the similarity. I'm going to abbreviate as SIM instead of writing out the whole word similarity, right? And f2(i) equals the similarity between x(i) and l2, and so on, down to fm(i) equals the similarity between x(i) and l(m). And somewhere in the middle. Somewhere in this list, you know, at the i-th component, I will actually have one feature component which is f subscript i(i), which is going to be the similarity between x and l(i). Where l(i) is equal to x(i), and so you know fi(i) is just going to be the similarity between x and itself. And if you're using the Gaussian kernel this is actually e to the minus 0 over 2 sigma squared and so, this will be equal to 1 and that's okay. So one of my features for this training example is going to be equal to 1. And then similar to what I have above. I can take all of these m features and group them into a feature vector. So instead of representing my example, using, you know, x(i) which is this what R(n) plus R(n) one dimensional vector. Depending on whether you can set terms, is either R(n) or R(n) plus 1. We can now instead represent my training example using this feature vector f. I am going to write this f superscript i.  Which is going to be taking all of these things and stacking them into a vector. So, f1(i) down to fm(i) and if you want and well, usually we'll also add this f0(i), where f0(i) is equal to 1. And so this vector here gives me my new feature vector with which to represent my training example. So given these kernels and similarity functions, here's how we use a simple vector machine. If you already have a learning set of parameters theta, then if you given a value of x and you want to make a prediction. What we do is we compute the features f, which is now an R(m) plus 1 dimensional feature vector. And we have m here because we have m training examples and thus m landmarks and what we do is we predict 1 if theta transpose f is greater than or equal to 0. Right. So, if theta transpose f, of course, that's just equal to theta 0, f0 plus theta 1, f1 plus dot dot dot, plus theta m f(m). And so my parameter vector theta is also now going to be an m plus 1 dimensional vector. And we have m here because where the number of landmarks is equal to the training set size. So m was the training set size and now, the parameter vector theta is going to be m plus one dimensional. So that's how you make a prediction if you already have a setting for the parameter's theta. How do you get the parameter's theta? Well you do that using the SVM learning algorithm, and specifically what you do is you would solve this minimization problem. You've minimized the parameter's theta of C times this cost function which we had before. Only now, instead of looking there instead of making predictions using theta transpose x(i) using our original features, x(i). Instead we've taken the features x(i) and replace them with a new features so we are using theta transpose f(i) to make a prediction on the i'f training examples and we see that, you know, in both places here and it's by solving this minimization problem that you get the parameters for your Support Vector Machine. And one last detail is because this optimization problem we really have n equals m features. That is here. The number of features we have. Really, the effective number of features we have is dimension of f. So that n is actually going to be equal to m. So, if you want to, you can think of this as a sum, this really is a sum from j equals 1 through m. And then one way to think about this, is you can think of it as n being equal to m, because if f isn't a new feature, then we have m plus 1 features, with the plus 1 coming from the interceptor. And here, we still do sum from j equal 1 through n, because similar to our earlier videos on regularization, we still do not regularize the parameter theta zero, which is why this is a sum for j equals 1 through m instead of j equals zero though m.  So that's the support vector machine learning algorithm. That's one sort of, mathematical detail aside that I should mention, which is that in the way the support vector machine is implemented, this last term is actually done a little bit differently. So you don't really need to know about this last detail in order to use support vector machines, and in fact the equations that are written down here should give you all the intuitions that should need. But in the way the support vector machine is implemented, you know, that term, the sum of j of theta j squared right? Another way to write this is this can be written as theta transpose theta if we ignore the parameter theta 0. So theta 1 down to theta m.  Ignoring theta 0. Then this sum of j of theta j squared that this can also be written theta transpose theta. And what most support vector machine implementations do is actually replace this theta transpose theta, will instead, theta transpose times some matrix inside, that depends on the kernel you use, times theta. And so this gives us a slightly different distance metric. We'll use a slightly different measure instead of minimizing exactly the norm of theta squared means that minimize something slightly similar to it. That's like a rescale version of the parameter vector theta that depends on the kernel. But this is kind of a mathematical detail. That allows the support vector machine software to run much more efficiently. And the reason the support vector machine does this is with this modification. It allows it to scale to much bigger training sets. Because for example, if you have a training set with 10,000 training examples. Then, you know, the way we define landmarks, we end up with 10,000 landmarks. And so theta becomes 10,000 dimensional. And maybe that works, but when m becomes really, really big then solving for all of these parameters, you know, if m were 50,000 or a 100,000 then solving for all of these parameters can become expensive for the support vector machine optimization software, thus solving the minimization problem that I drew here. So kind of as mathematical detail, which again you really don't need to know about. It actually modifies that last term a little bit to optimize something slightly different than just minimizing the norm squared of theta squared, of theta. But if you want, you can feel free to think of this as an kind of a n implementational detail that does change the objective a bit, but is done primarily for reasons of computational efficiency, so usually you don't really have to worry about this. And by the way, in case your wondering why we don't apply the kernel's idea to other algorithms as well like logistic regression, it turns out that if you want, you can actually apply the kernel's idea and define the source of features using landmarks and so on for logistic regression. But the computational tricks that apply for support vector machines don't generalize well to other algorithms like logistic regression. And so, using kernels with logistic regression is going too very slow, whereas, because of computational tricks, like that embodied and how it modifies this and the details of how the support vector machine software is implemented, support vector machines and kernels tend go particularly well together. Whereas, logistic regression and kernels, you know, you can do it, but this would run very slowly. And it won't be able to take advantage of advanced optimization techniques that people have figured out for the particular case of running a support vector machine with a kernel. But all this pertains only to how you actually implement software to minimize the cost function. I will say more about that in the next video, but you really don't need to know about how to write software to minimize this  cost function because you can find very good off the shelf software for doing so. And just as, you know, I wouldn't recommend writing code to invert a matrix or to compute a square root, I actually do not recommend writing software to minimize this cost function yourself, but instead to use off the shelf software packages that people have developed and so those software packages already embody these numerical optimization tricks, so you don't really have to worry about them. But one other thing that is worth knowing about is when you're applying a support vector machine, how do you choose the parameters of the support vector machine? And the last thing I want to do in this video is say a little word about the bias and variance trade offs when using a support vector machine. When using an SVM, one of the things you need to choose is the parameter C which was in the optimization objective, and you recall that C played a role similar to 1 over lambda, where lambda was the regularization parameter we had for logistic regression. So, if you have a large value of C, this corresponds to what we have back in logistic regression, of a small value of lambda meaning of not using much regularization. And if you do that, you tend to have a hypothesis with lower bias and higher variance. Whereas if you use a smaller value of C then this corresponds to when we are using logistic regression with a large value of lambda and that corresponds to a hypothesis with higher bias and lower variance. And so, hypothesis with large C has a higher variance, and is more prone to overfitting, whereas hypothesis with small C has higher bias and is thus more prone to underfitting. So this parameter C is one of the parameters we need to choose. The other one is the parameter sigma squared, which appeared in the Gaussian kernel. So if the Gaussian kernel sigma squared is large, then in the similarity function, which was this you know E to the minus x minus landmark varies squared over 2 sigma squared. In this one of the example; If I have only one feature, x1, if I have a landmark there at that location, if sigma squared is large, then, you know, the Gaussian kernel would tend to fall off relatively slowly and so this would be my feature f(i), and so this would be smoother function that varies more smoothly, and so this will give you a hypothesis with higher bias and lower variance, because the Gaussian kernel that falls off smoothly, you tend to get a hypothesis that varies slowly, or varies smoothly as you change the input x. Whereas in contrast, if sigma squared was small and if that's my landmark given my 1 feature x1, you know, my Gaussian kernel, my similarity function, will vary more abruptly. And in both cases I'd pick out 1, and so if sigma squared is small, then my features vary less smoothly. So if it's just higher slopes or higher derivatives here. And using this, you end up fitting hypotheses of lower bias and you can have higher variance. And if you look at this week's points exercise, you actually get to play around with some of these ideas yourself and see these effects yourself. So, that was the support vector machine with kernels algorithm. And hopefully this discussion of bias and variance will give you some sense of how you can expect this algorithm to behave as well.""",75,0,1
coursera,stanford_university,machine-learning,using-an-svm,"b""So far we've been talking about SVMs in a fairly abstract level. In this video I'd like to talk about what you actually need to do in order to run or to use an SVM. The support vector machine algorithm poses a particular optimization problem. But as I briefly mentioned in an earlier video, I really do not recommend writing your own software to solve for the parameter's theta yourself. So just as today, very few of us, or maybe almost essentially none of us would think of writing code ourselves to invert a matrix or take a square root of a number, and so on. We just, you know, call some library function to do that. In the same way, the software for solving the SVM optimization problem is very complex, and there have been researchers that have been doing essentially numerical optimization research for many years. So you come up with good software libraries and good software packages to do this. And then strongly recommend just using one of the highly optimized software libraries rather than trying to implement something yourself. And there are lots of good software libraries out there. The two that I happen to use the most often are the linear SVM but there are really lots of good software libraries for doing this that you know, you can link to many of the major programming languages that you may be using to code up learning algorithm. Even though you shouldn't be writing your own SVM optimization software, there are a few things you need to do, though. First is to come up with with some choice of the parameter's C. We talked a little bit of the bias/variance properties of this in the earlier video. Second, you also need to choose the kernel or the similarity function that you want to use. So one choice might be if we decide not to use any kernel. And the idea of no kernel is also called a linear kernel. So if someone says, I use an SVM with a linear kernel, what that means is you know, they use an SVM without using without using a kernel and it was a version of the SVM that just uses theta transpose X, right, that predicts 1 theta 0 plus theta 1 X1 plus so on plus theta N, X N is greater than equals 0. This term linear kernel, you can think of this as you know this is the version of the SVM that just gives you a standard linear classifier. So that would be one reasonable choice for some problems, and you know, there would be many software libraries, like linear, was one example, out of many, one example of a software library that can train an SVM without using a kernel, also called a linear kernel. So, why would you want to do this? If you have a large number of features, if N is large, and M the number of training examples is small, then you know you have a huge number of features that if X, this is an X is an Rn, Rn +1. So if you have a huge number of features already, with a small training set, you know, maybe you want to just fit a linear decision boundary and not try to fit a very complicated nonlinear function, because might not have enough data. And you might risk overfitting, if you're trying to fit a very complicated function in a very high dimensional feature space, but if your training set sample is small. So this would be one reasonable setting where you might decide to just not use a kernel, or equivalents to use what's called a linear kernel. A second choice for the kernel that you might make, is this Gaussian kernel, and this is what we had previously. And if you do this, then the other choice you need to make is to choose this parameter sigma squared when we also talk a little bit about the bias variance tradeoffs of how, if sigma squared is large, then you tend to have a higher bias, lower variance classifier, but if sigma squared is small, then you have a higher variance, lower bias classifier. So when would you choose a Gaussian kernel? Well, if your omission of features X, I mean Rn, and if N is small, and, ideally, you know, if n is large, right, so that's if, you know, we have say, a two-dimensional training set, like the example I drew earlier. So n is equal to 2, but we have a pretty large training set. So, you know, I've drawn in a fairly large number of training examples, then maybe you want to use a kernel to fit a more complex nonlinear decision boundary, and the Gaussian kernel would be a fine way to do this. I'll say more towards the end of the video, a little bit more about when you might choose a linear kernel, a Gaussian kernel and so on. But if concretely, if you decide to use a Gaussian kernel, then here's what you need to do. Depending on what support vector machine software package you use, it may ask you to implement a kernel function, or to implement the similarity function. So if you're using an octave or MATLAB implementation of an SVM, it may ask you to provide a function to compute a particular feature of the kernel. So this is really computing f subscript i for one particular value of i, where f here is just a single real number, so maybe I should move this better written f(i), but what you need to do is to write a kernel function that takes this input, you know, a training example or a test example whatever it takes in some vector X and takes as input one of the landmarks and but only I've come down X1 and X2 here, because the landmarks are really training examples as well. But what you need to do is write software that takes this input, you know, X1, X2 and computes this sort of similarity function between them and return a real number. And so what some support vector machine packages do is expect you to provide this kernel function that take this input you know, X1, X2 and returns a real number. And then it will take it from there and it will automatically generate all the features, and so automatically take X and map it to f1, f2, down to f(m) using this function that you write, and generate all the features and train the support vector machine from there. But sometimes you do need to provide this function yourself. Other if you are using the Gaussian kernel, some SVM implementations will also include the Gaussian kernel and a few other kernels as well, since the Gaussian kernel is probably the most common kernel. Gaussian and linear kernels are really the two most popular kernels by far. Just one implementational note. If you have features of very different scales, it is important to perform feature scaling before using the Gaussian kernel. And here's why. If you imagine the computing the norm between X and l, right, so this term here, and the numerator term over there. What this is doing, the norm between X and l, that's really saying, you know, let's compute the vector V, which is equal to X minus l. And then let's compute the norm does vector V, which is the difference between X. So the norm of V is really equal to V1 squared plus V2 squared plus dot dot dot, plus Vn squared. Because here X is in Rn, or Rn plus 1, but I'm going to ignore, you know, X0. So, let's pretend X is an Rn, square on the left side is what makes this correct. So this is equal to that, right? And so written differently, this is going to be X1 minus l1 squared, plus x2 minus l2 squared, plus dot dot dot plus Xn minus ln squared. And now if your features take on very different ranges of value. So take a housing prediction, for example, if your data is some data about houses. And if X is in the range of thousands of square feet, for the first feature, X1. But if your second feature, X2 is the number of bedrooms. So if this is in the range of one to five bedrooms, then X1 minus l1 is going to be huge. This could be like a thousand squared, whereas X2 minus l2 is going to be much smaller and if that's the case, then in this term, those distances will be almost essentially dominated by the sizes of the houses and the number of bathrooms would be largely ignored. As so as, to avoid this in order to make a machine work well, do perform future scaling. And that will sure that the SVM gives, you know, comparable amount of attention to all of your different features, and not just to in this example to size of houses were big movement here the features. When you try a support vector machines chances are by far the two most common kernels you use will be the linear kernel, meaning no kernel, or the Gaussian kernel that we talked about. And just one note of warning which is that not all similarity functions you might come up with are valid kernels. And the Gaussian kernel and the linear kernel and other kernels that you sometimes others will use, all of them need to satisfy a technical condition. It's called Mercer's Theorem and the reason you need to this is because support vector machine algorithms or implementations of the SVM have lots of clever numerical optimization tricks. In order to solve for the parameter's theta efficiently and in the original design envisaged, those are decision made to restrict our attention only to kernels that satisfy this technical condition called Mercer's Theorem. And what that does is, that makes sure that all of these SVM packages, all of these SVM software packages can use the large class of optimizations and get the parameter theta very quickly. So, what most people end up doing is using either the linear or Gaussian kernel, but there are a few other kernels that also satisfy Mercer's theorem and that you may run across other people using, although I personally end up using other kernels you know, very, very rarely, if at all. Just to mention some of the other kernels that you may run across. One is the polynomial kernel. And for that the similarity between X and l is defined as, there are a lot of options, you can take X transpose l squared. So, here's one measure of how similar X and l are. If X and l are very close with each other, then the inner product will tend to be large. And so, you know, this is a slightly unusual kernel. That is not used that often, but you may run across some people using it. This is one version of a polynomial kernel. Another is X transpose l cubed. These are all examples of the polynomial kernel. X transpose l plus 1 cubed. X transpose l plus maybe a number different then one 5 and, you know, to the power of 4 and so the polynomial kernel actually has two parameters. One is, what number do you add over here? It could be 0. This is really plus 0 over there, as well as what's the degree of the polynomial over there. So the degree power and these numbers. And the more general form of the polynomial kernel is X transpose l, plus some constant and then to some degree in the X1 and so both of these are parameters for the polynomial kernel. So the polynomial kernel almost always or usually performs worse. And the Gaussian kernel does not use that much, but this is just something that you may run across. Usually it is used only for data where X and l are all strictly non negative, and so that ensures that these inner products are never negative. And this captures the intuition that X and l are very similar to each other, then maybe the inter product between them will be large. They have some other properties as well but people tend not to use it much. And then, depending on what you're doing, there are other, sort of more esoteric kernels as well, that you may come across. You know, there's a string kernel, this is sometimes used if your input data is text strings or other types of strings. There are things like the chi-square kernel, the histogram intersection kernel, and so on. There are sort of more esoteric kernels that you can use to measure similarity between different objects. So for example, if you're trying to do some sort of text classification problem, where the input x is a string then maybe we want to find the similarity between two strings using the string kernel, but I personally you know end up very rarely, if at all, using these more esoteric kernels.\nI think I might have use the chi-square kernel, may be once in my life and the histogram kernel, may be once or twice in my life. I've actually never used the string kernel myself. But in case you've run across this in other applications. You know, if you do a quick web search we do a quick Google search or quick Bing search you should have found definitions that these are the kernels as well.\nSo just two last details I want to talk about in this video. One in multiclass classification. So, you have four classes or more generally 3 classes output some appropriate decision bounday between your multiple classes. Most SVM, many SVM packages already have built-in multiclass classification functionality. So if your using a pattern like that, you just use the both that functionality and that should work fine. Otherwise, one way to do this is to use the one versus all method that we talked about when we are developing logistic regression. So what you do is you trade kSVM's if you have k classes, one to distinguish each of the classes from the rest. And this would give you k parameter vectors, so this will give you, upi lmpw. theta 1, which is trying to distinguish class y equals one from all of the other classes, then you get the second parameter, vector theta 2, which is what you get when you, you know, have y equals 2 as the positive class and all the others as negative class and so on up to a parameter vector theta k, which is the parameter vector for distinguishing the final class key from anything else, and then lastly, this is exactly the same as the one versus all method we have for logistic regression. Where we you just predict the class i with the largest theta transpose X.  So let's multiclass classification designate. For the more common cases that there is a good chance that whatever software package you use, you know, there will be a reasonable chance that are already have built in multiclass classification functionality, and so you don't need to worry about this result. Finally, we developed support vector machines starting off with logistic regression and then modifying the cost function a little bit. The last thing we want to do in this video is, just say a little bit about. when you will use one of these two algorithms, so let's say n is the number of features and m is the number of training examples. So, when should we use one algorithm versus the other? Well, if n is larger relative to your training set size, so for example, if you take a business with a number of features this is much larger than m and this might be, for example, if you have a text classification problem, where you know, the dimension of the feature vector is I don't know, maybe, 10 thousand. And if your training set size is maybe 10 you know, maybe, up to 1000. So, imagine a spam classification problem, where email spam, where you have 10,000 features corresponding to 10,000 words but you have, you know, maybe 10 training examples or maybe up to 1,000 examples. So if n is large relative to m, then what I would usually do is use logistic regression or use it as the m without a kernel or use it with a linear kernel. Because, if you have so many features with smaller training sets, you know, a linear function will probably do fine, and you don't have really enough data to fit a very complicated nonlinear function. Now if is n is small and m is intermediate what I mean by this is n is maybe anywhere from 1 - 1000, 1 would be very small. But maybe up to 1000 features and if the number of training examples is maybe anywhere from 10, you know, 10 to maybe up to 10,000 examples. Maybe up to 50,000 examples. If m is pretty big like maybe 10,000 but not a million. Right? So if m is an intermediate size then often an SVM with a linear kernel will work well. We talked about this early as well, with the one concrete example, this would be if you have a two dimensional training set. So, if n is equal to 2 where you have, you know, drawing in a pretty large number of training examples. So Gaussian kernel will do a pretty good job separating positive and negative classes. One third setting that's of interest is if n is small but m is large. So if n is you know, again maybe 1 to 1000, could be larger. But if m was, maybe 50,000 and greater to millions. So, 50,000, a 100,000, million, trillion. You have very very large training set sizes, right. So if this is the case, then a SVM of the Gaussian Kernel will be somewhat slow to run. Today's SVM packages, if you're using a Gaussian Kernel, tend to struggle a bit. If you have, you know, maybe 50 thousands okay, but if you have a million training examples, maybe or even a 100,000 with a massive value of m. Today's SVM packages are very good, but they can still struggle a little bit when you have a massive, massive trainings that size when using a Gaussian Kernel. So in that case, what I would usually do is try to just manually create have more features and then use logistic regression or an SVM without the Kernel. And in case you look at this slide and you see logistic regression or SVM without a kernel. In both of these places, I kind of paired them together. There's a reason for that, is that logistic regression and SVM without the kernel, those are really pretty similar algorithms and, you know, either logistic regression or SVM without a kernel will usually do pretty similar things and give pretty similar performance, but depending on your implementational details, one may be more efficient than the other. But, where one of these algorithms applies, logistic regression where SVM without a kernel, the other one is to likely to work pretty well as well. But along with the power of the SVM is when you use different kernels to learn complex nonlinear functions. And this regime, you know, when you have maybe up to 10,000 examples, maybe up to 50,000. And your number of features, this is reasonably large. That's a very common regime and maybe that's a regime where a support vector machine with a kernel kernel will shine. You can do things that are much harder to do that will need logistic regression. And finally, where do neural networks fit in? Well for all of these problems, for all of these different regimes, a well designed neural network is likely to work well as well. The one disadvantage, or the one reason that might not sometimes use the neural network is that, for some of these problems, the neural network might be slow to train. But if you have a very good SVM implementation package, that could run faster, quite a bit faster than your neural network. And, although we didn't show this earlier, it turns out that the optimization problem that the SVM has is a convex optimization problem and so the good SVM optimization software packages will always find the global minimum or something close to it. And so for the SVM you don't need to worry about local optima. In practice local optima aren't a huge problem for neural networks but they all solve, so this is one less thing to worry about if you're using an SVM. And depending on your problem, the neural network may be slower, especially in this sort of regime than the SVM. In case the guidelines they gave here, seem a little bit vague and if you're looking at some problems, you know, the guidelines are a bit vague, I'm still not entirely sure, should I use this algorithm or that algorithm, that's actually okay. When I face a machine learning problem, you know, sometimes its actually just not clear whether that's the best algorithm to use, but as you saw in the earlier videos, really, you know, the algorithm does matter, but what often matters even more is things like, how much data do you have. And how skilled are you, how good are you at doing error analysis and debugging learning algorithms, figuring out how to design new features and figuring out what other features to give you learning algorithms and so on. And often those things will matter more than what you are using logistic regression or an SVM. But having said that, the SVM is still widely perceived as one of the most powerful learning algorithms, and there is this regime of when there's a very effective way to learn complex non linear functions. And so I actually, together with logistic regressions, neural networks, SVM's, using those to speed learning algorithms you're I think very well positioned to build state of the art you know, machine learning systems for a wide region for applications and this is another very powerful tool to have in your arsenal. One that is used all over the place in Silicon Valley, or in industry and in the Academia, to build many high performance machine learning system.""",76,0,1
coursera,stanford_university,machine-learning,unsupervised-learning-introduction,"b""In this video, I'd like to\nstart to talk about clustering. This will be exciting, because this is our\nfirst unsupervised learning algorithm, where we learn from unlabeled\ndata instead from labelled data. So, what is unsupervised learning? I briefly talked about unsupervised\nlearning at the beginning of the class but it's useful to contrast it\nwith supervised learning. So, here's a typical supervised\nlearning problem where we're given a labeled training set and the goal is to\nfind the decision boundary that separates the positive label examples and\nthe negative label examples. So, the supervised learning problem in\nthis case is given a set of labels to fit a hypothesis to it. In contrast,\nin the unsupervised learning problem we're given data that does not have\nany labels associated with it. So, we're given data that looks like this. Here's a set of points add in no labels,\nand so, our training set is written just x1, x2, and so on up to\nx m and we don't get any labels y. And that's why the points plotted up on\nthe figure don't have any labels with them. So, in unsupervised learning what\nwe do is we give this sort of unlabeled training set to an algorithm and we just ask the algorithm find\nsome structure in the data for us. Given this data set one type of structure\nwe might have an algorithm find is that it looks like this data set has points\ngrouped into two separate clusters and so an algorithm that finds clusters like the ones I've just circled\nis called a clustering algorithm. And this would be our first\ntype of unsupervised learning, although there will be other types of\nunsupervised learning algorithms that we'll talk about later that finds\nother types of structure or other types of patterns in\nthe data other than clusters. We'll talk about this after\nwe've talked about clustering. So, what is clustering good for? Early in this class I already\nmentioned a few applications. One is market segmentation where you\nmay have a database of customers and want to group them into\ndifferent marker segments so you can sell to them separately or serve\nyour different market segments better. Social network analysis. There are actually groups have\ndone this things like looking at a group of people's social networks. So, things like Facebook,\nGoogle+, or maybe information about who other people that you email\nthe most frequently and who are the people that they email the most frequently and\nto find coherence in groups of people. So, this would be another maybe clustering\nalgorithm where you know want to find who are the coherent groups of\nfriends in the social network? Here's something that one of my friends\nactually worked on which is, use clustering to organize computer clusters\nor to organize data centers better. Because if you know which computers in the\ndata center in the cluster tend to work together, you can use that to\nreorganize your resources and how you layout the network and how you\ndesign your data center communications. And lastly, something that actually\nanother friend worked on using clustering algorithms to\nunderstand galaxy formation and using that to understand\nastronomical data. So, that's clustering which is our\nfirst example of an unsupervised learning algorithm. In the next video we'll start to talk\nabout a specific clustering algorithm.""",77,0,1
coursera,stanford_university,machine-learning,k-means-algorithm,"b""In the clustering problem we are given an unlabeled data set and we would like to have an algorithm automatically group the data into coherent subsets or into coherent clusters for us. The K Means algorithm is by far the most popular, by far the most widely used clustering algorithm, and in this video I would like to tell you what the K Means Algorithm is and how it works. The K means clustering algorithm is best illustrated in pictures. Let's say I want to take an unlabeled data set like the one shown here, and I want to group the data into two clusters. If I run the K Means clustering algorithm, here is what I'm going to do. The first step is to randomly initialize two points, called the cluster centroids. So, these two crosses here, these are called the Cluster Centroids and I have two of them because I want to group my data into two clusters. K Means is an iterative algorithm and it does two things. First is a cluster assignment step, and second is a move centroid step. So, let me tell you what those things mean. The first of the two steps in the loop of K means, is this cluster assignment step. What that means is that, it's going through each of the examples, each of these green dots shown here and depending on whether it's closer to the red cluster centroid or the blue cluster centroid, it is going to assign each of the data points to one of the two cluster centroids. Specifically, what I mean by that, is to go through your data set and color each of the points either red or blue, depending on whether it is closer to the red cluster centroid or the blue cluster centroid, and I've done that in this diagram here. So, that was the cluster assignment step. The other part of K means, in the loop of K means, is the move centroid step, and what we are going to do is, we are going to take the two cluster centroids, that is, the red cross and the blue cross, and we are going to move them to the average of the points colored the same colour. So what we are going to do is look at all the red points and compute the average, really the mean of the location of all the red points, and we are going to move the red cluster centroid there. And the same things for the blue cluster centroid, look at all the blue dots and compute their mean, and then move the blue cluster centroid there. So, let me do that now. We're going to move the cluster centroids as follows and I've now moved them to their new means. The red one moved like that and the blue one moved like that and the red one moved like that. And then we go back to another cluster assignment step, so we're again going to look at all of my unlabeled examples and depending on whether it's closer the red or the blue cluster centroid, I'm going to color them either red or blue. I'm going to assign each point to one of the two cluster centroids, so let me do that now. And so the colors of some of the points just changed. And then I'm going to do another move centroid step. So I'm going to compute the average of all the blue points, compute the average of all the red points and move my cluster centroids like this, and so, let's do that again. Let me do one more cluster assignment step. So colour each point red or blue, based on what it's closer to and then do another move centroid step and we're done. And in fact if you keep running additional iterations of K means from here the cluster centroids will not change any further and the colours of the points will not change any further. And so, this is the, at this point, K means has converged and it's done a pretty good job finding the two clusters in this data. Let's write out the K means algorithm more formally. The K means algorithm takes two inputs. One is a parameter K, which is the number of clusters you want to find in the data. I'll later say how we might go about trying to choose k, but for now let's just say that we've decided we want a certain number of clusters and we're going to tell the algorithm how many clusters we think there are in the data set. And then K means also takes as input this sort of unlabeled training set of just the Xs and because this is unsupervised learning, we don't have the labels Y anymore. And for unsupervised learning of the K means I'm going to use the convention that XI is an RN dimensional vector. And that's why my training examples are now N dimensional rather N plus one dimensional vectors. This is what the K means algorithm does. The first step is that it randomly initializes k cluster centroids which we will call mu 1, mu 2, up to mu k. And so in the earlier diagram, the cluster centroids corresponded to the location of the red cross and the location of the blue cross. So there we had two cluster centroids, so maybe the red cross was mu 1 and the blue cross was mu 2, and more generally we would have k cluster centroids rather than just 2. Then the inner loop of k means does the following, we're going to repeatedly do the following. First for each of my training examples, I'm going to set this variable CI to be the index 1 through K of the cluster centroid closest to XI. So this was my cluster assignment step, where we took each of my examples and coloured it either red or blue, depending on which cluster centroid it was closest to. So CI is going to be a number from 1 to K that tells us, you know, is it closer to the red cross or is it closer to the blue cross, and another way of writing this is I'm going to, to compute Ci, I'm going to take my Ith example Xi and and I'm going to measure it's distance to each of my cluster centroids, this is mu and then lower-case k, right, so capital K is the total number centroids and I'm going to use lower case k here to index into the different centroids. But so, Ci is going to, I'm going to minimize over my values of k and find the value of K that minimizes this distance between Xi and the cluster centroid, and then, you know, the value of k that minimizes this, that's what gets set in Ci. So, here's another way of writing out what Ci is. If I write the norm between Xi minus Mu-k, then this is the distance between my ith training example Xi and the cluster centroid Mu subscript K, this is--this here, that's a lowercase K. So uppercase K is going to be used to denote the total number of cluster centroids, and this lowercase K's a number between one and capital K. I'm just using lower case K to index into my different cluster centroids. Next is lower case k. So that's the distance between the example and the cluster centroid and so what I'm going to do is find the value of K, of lower case k that minimizes this, and so the value of k that minimizes you know, that's what I'm going to set as Ci, and by convention here I've written the distance between Xi and the cluster centroid, by convention people actually tend to write this as the squared distance. So we think of Ci as picking the cluster centroid with the smallest squared distance to my training example Xi. But of course minimizing squared distance, and minimizing distance that should give you the same value of Ci, but we usually put in the square there, just as the convention that people use for K means. So that was the cluster assignment step. The other in the loop of K means does the move centroid step. And what that does is for each of my cluster centroids, so for lower case k equals 1 through K, it sets Mu-k equals to the average of the points assigned to cluster. So as a concrete example, let's say that one of my cluster centroids, let's say cluster centroid two, has training examples, you know, 1, 5, 6, and 10 assigned to it. And what this means is, really this means that C1 equals to C5 equals to C6 equals to and similarly well c10 equals, too, right? If we got that from the cluster assignment step, then that means examples 1,5,6 and 10 were assigned to the cluster centroid two. Then in this move centroid step, what I'm going to do is just compute the average of these four things. So X1 plus X5 plus X6 plus X10. And now I'm going to average them so here I have four points assigned to this cluster centroid, just take one quarter of that. And now Mu2 is going to be an n-dimensional vector. Because each of these example x1, x5, x6, x10 each of them were an n-dimensional vector, and I'm going to add up these things and, you know, divide by four because I have four points assigned to this cluster centroid, I end up with my move centroid step, for my cluster centroid mu-2. This has the effect of moving mu-2 to the average of the four points listed here. One thing that I've asked is, well here we said, let's let mu-k be the average of the points assigned to the cluster. But what if there is a cluster centroid no points with zero points assigned to it. In that case the more common thing to do is to just eliminate that cluster centroid. And if you do that, you end up with K minus one clusters instead of k clusters. Sometimes if you really need k clusters, then the other thing you can do if you have a cluster centroid with no points assigned to it is you can just randomly reinitialize that cluster centroid, but it's more common to just eliminate a cluster if somewhere during K means it with no points assigned to that cluster centroid, and that can happen, altthough in practice it happens not that often. So that's the K means Algorithm. Before wrapping up this video I just want to tell you about one other common application of K Means and that's to the problems with non well separated clusters. Here's what I mean. So far we've been picturing K Means and applying it to data sets like that shown here where we have three pretty well separated clusters, and we'd like an algorithm to find maybe the 3 clusters for us. But it turns out that very often K Means is also applied to data sets that look like this where there may not be several very well separated clusters. Here is an example application, to t-shirt sizing. Let's say you are a t-shirt manufacturer you've done is you've gone to the population that you want to sell t-shirts to, and you've collected a number of examples of the height and weight of these people in your population and so, well I guess height and weight tend to be positively highlighted so maybe you end up with a data set like this, you know, with a sample or set of examples of different peoples heights and weight. Let's say you want to size your t shirts. Let's say I want to design and sell t shirts of three sizes, small, medium and large. So how big should I make my small one? How big should I my medium? And how big should I make my large t-shirts. One way to do that would to be to run my k means clustering logarithm on this data set that I have shown on the right and maybe what K Means will do is group all of these points into one cluster and group all of these points into a second cluster and group all of those points into a third cluster. So, even though the data, you know, before hand it didn't seem like we had 3 well separated clusters, K Means will kind of separate out the data into multiple pluses for you. And what you can do is then look at this first population of people and look at them and, you know, look at the height and weight, and try to design a small t-shirt so that it kind of fits this first population of people well and then design a medium t-shirt and design a large t-shirt. And this is in fact kind of an example of market segmentation where you're using K Means to separate your market into 3 different segments. So you can design a product separately that is a small, medium, and large t-shirts, that tries to suit the needs of each of your 3 separate sub-populations well. So that's the K Means algorithm. And by now you should know how to implement the K Means Algorithm and kind of get it to work for some problems. But in the next few videos what I want to do is really get more deeply into the nuts and bolts of K means and to talk a bit about how to actually get this to work really well.""",78,0,1
coursera,stanford_university,machine-learning,optimization-objective,"b""Most of the supervised learning algorithms\nwe've seen, things like linear regression, logistic regression, and so on, all of\nthose algorithms have an optimization objective or some cost function that\nthe algorithm was trying to minimize. It turns out that k-means also\nhas an optimization objective or a cost function that\nit's trying to minimize. And in this video I'd like to tell you\nwhat that optimization objective is. And the reason I want to do so\nis because this will be useful to us for two purposes. First, knowing what is the optimization\nobjective of k-means will help us to debug the learning algorithm and just make\nsure that k-means is running correctly. And second, and perhaps more importantly,\nin a later video we'll talk about how we can use this to help k-means find better\ncosts for this and avoid the local ultima. But we do that in a later\nvideo that follows this one. Just as a quick reminder while\nk-means is running we're going to be keeping track of two sets of variables. First is the ci's and\nthat keeps track of the index or the number of the cluster, to which\nan example xi is currently assigned. And then the other set of variables\nwe use is mu subscript k, which is the location\nof cluster centroid k. Again, for k-means we use capital K to\ndenote the total number of clusters. And here lower case k is going to be\nan index into the cluster centroids and so, lower case k is going to be\na number between one and capital K. Now here's one more bit of notation, which\nis gonna use mu subscript ci to denote the cluster centroid of the cluster to\nwhich example xi has been assigned, right? And to explain that\nnotation a little bit more, lets say that xi has been\nassigned to cluster number five. What that means is that ci, that is the\nindex of xi, that that is equal to five. Right? Because having ci equals five,\nif that's what it means for the example xi to be assigned\nto cluster number five. And so mu subscript ci is going\nto be equal to mu subscript 5. Because ci is equal to five. And so this mu subscript ci is the cluster\ncentroid of cluster number five, which is the cluster to which\nmy example xi has been assigned. Out with this notation, we're now ready\nto write out what is the optimization objective of the k-means clustering\nalgorithm and here it is. The cost function that k-means is\nminimizing is a function J of all of these parameters, c1 through cm and\nmu 1 through mu K. That k-means is varying\nas the algorithm runs. And the optimization objective\nis shown to the right, is the average of 1 over m of sum from\ni equals 1 through m of this term here. That I've just drawn the red box around,\nright? The square distance between each\nexample xi and the location of the cluster centroid to\nwhich xi has been assigned. So let's draw this and\njust let me explain this. Right, so here's the location of\ntraining example xi and here's the location of the cluster centroid\nto which example xi has been assigned. So to explain this in pictures,\nif here's x1, x2, and if a point here is my example xi, so\nif that is equal to my example xi, and if xi has been assigned to some\ncluster centroid, I'm gonna denote my cluster centroid with a cross, so\nif that's the location of mu 5, let's say. If x i has been assigned cluster\ncentroid five as in my example up there, then this square distance,\nthat's the square of the distance between the point xi and this cluster centroid\nto which xi has been assigned. And what k-means can be shown to be doing is that it is trying to define\nparameters ci and mu i. Trying to find c and mu to try to\nminimize this cost function J. This cost function is sometimes\nalso called the distortion cost function, or\nthe distortion of the k-means algorithm. And just to provide a little bit more\ndetail, here's the k-means algorithm. Here's exactly the algorithm as we have\nwritten it out on the earlier slide. And what this first step of this algorithm\nis, this was the cluster assignment step where we assigned each point\nto the closest centroid. And it's possible to show\nmathematically that what the cluster assignment step is\ndoing is exactly Minimizing J, with respect to the variables c1,\nc2 and so on, up to cm, while holding the cluster\ncentroids mu 1 up to mu K, fixed. So what the cluster assignment step\ndoes is it doesn't change the cluster centroids, but what it's doing is this\nis exactly picking the values of c1, c2, up to cm. That minimizes the cost function,\nor the distortion function J. And it's possible to prove that\nmathematically, but I won't do so here. But it has a pretty intuitive meaning of\njust well, let's assign each point to a cluster centroid that is closest to it,\nbecause that's what minimizes the square of distance between\nthe points in the cluster centroid. And then the second step of k-means,\nthis second step over here. The second step was\nthe move centroid step. And once again I won't prove it, but\nit can be shown mathematically that what the move centroid step does is\nit chooses the values of mu that minimizes J, so it minimizes\nthe cost function J with respect to, wrt is my abbreviation for, with respect\nto, when it minimizes J with respect to the locations of the cluster\ncentroids mu 1 through mu K. So if is really is doing is this\ntaking the two sets of variables and partitioning them into\ntwo halves right here. First the c sets of variables and\nthen you have the mu sets of variables. And what it does is it first minimizes\nJ with respect to the variable c and then it minimizes J with respect to\nthe variables mu and then it keeps on. And, so all that's all that k-means does. And now that we understand k-means as\ntrying to minimize this cost function J, we can also use this to try to debug other\nany algorithm and just kind of make sure that our implementation of\nk-means is running correctly. So, we now understand the k-means\nalgorithm as trying to optimize this cost function J, which\nis also called the distortion function. We can use that to debug k-means and help\nmake sure that k-means is converging and is running properly. And in the next video we'll\nalso see how we can use this to help k-means find better clusters and\nto help k-means to avoid""",79,0,1
coursera,stanford_university,machine-learning,random-initialization,"b""In this video, I'd like to talk about how to initialize K-means and more importantly, this will lead into a discussion of how to make K-means avoid local optima as well. Here's the K-means clustering algorithm that we talked about earlier. One step that we never really talked much about was this step of how you randomly initialize the cluster centroids. There are few different ways that one can imagine using to randomly initialize the cluster centroids. But, it turns out that there is one method that is much more recommended than most of the other options one might think about. So, let me tell you about that option since it's what often seems to work best. Here's how I usually initialize my cluster centroids. When running K-means, you should have the number of cluster centroids, K, set to be less than the number of training examples M. It would be really weird to run K-means with a number of cluster centroids that's, you know, equal or greater than the number of examples you have, right? So the way I usually initialize K-means is, I would randomly pick k training examples. So, and, what I do is then set Mu1 of MuK equal to these k examples. Let me show you a concrete example. Lets say that k is equal to 2 and so on this example on the right let's say I want to find two clusters. So, what I'm going to do in order to initialize my cluster centroids is, I'm going to randomly pick a couple examples. And let's say, I pick this one and I pick that one. And the way I'm going to initialize my cluster centroids is, I'm just going to initialize my cluster centroids to be right on top of those examples. So that's my first cluster centroid and that's my second cluster centroid, and that's one random initialization of K-means. The one I drew looks like a particularly good one. And sometimes I might get less lucky and maybe I'll end up picking that as my first random initial example, and that as my second one. And here I'm picking two examples because k equals 2. Some we have randomly picked two training examples and if I chose those two then I'll end up with, may be this as my first cluster centroid and that as my second initial location of the cluster centroid. So, that's how you can randomly initialize the cluster centroids. And so at initialization, your first cluster centroid Mu1 will be equal to x(i) for some randomly value of i and Mu2 will be equal to x(j) for some different randomly chosen value of j and so on, if you have more clusters and more cluster centroid. And sort of the side common. I should say that in the earlier video where I first illustrated K-means with the animation. In that set of slides. Only for the purpose of illustration. I actually used a different method of initialization for my cluster centroids. But the method described on this slide, this is really the recommended way. And the way that you should probably use, when you implement K-means. So, as they suggested perhaps by these two illustrations on the right. You might really guess that K-means can end up converging to different solutions depending on exactly how the clusters were initialized, and so, depending on the random initialization. K-means can end up at different solutions. And, in particular, K-means can actually end up at local optima. If you're given the data sale like this. Well, it looks like, you know, there are three clusters, and so, if you run K-means and if it ends up at a good local optima this might be really the global optima, you might end up with that cluster ring. But if you had a particularly unlucky, random initialization, K-means can also get stuck at different local optima. So, in this example on the left it looks like this blue cluster has captured a lot of points of the left and then the they were on the green clusters each is captioned on the relatively small number of points. And so, this corresponds to a bad local optima because it has basically taken these two clusters and used them into 1 and furthermore, has split the second cluster into two separate sub-clusters like so, and it has also taken the second cluster and split it into two separate sub-clusters like so, and so, both of these examples on the lower right correspond to different local optima of K-means and in fact, in this example here, the cluster, the red cluster has captured only a single optima example. And the term local optima, by the way, refers to local optima of this distortion function J, and what these solutions on the lower left, what these local optima correspond to is really solutions where K-means has gotten stuck to the local optima and it's not doing a very good job minimizing this distortion function J. So, if you're worried about K-means getting stuck in local optima, if you want to increase the odds of K-means finding the best possible clustering, like that shown on top here, what we can do, is try multiple, random initializations. So, instead of just initializing K-means once and hopping that that works, what we can do is, initialize K-means lots of times and run K-means lots of times, and use that to try to make sure we get as good a solution, as good a local or global optima as possible. Concretely, here's how you could go about doing that. Let's say, I decide to run K-meanss a hundred times so I'll execute this loop a hundred times and it's fairly typical a number of times when came to will be something from 50 up to may be 1000. So, let's say you decide to say K-means one hundred times. So what that means is that we would randomnly initialize K-means. And for each of these one hundred random intializations we would run K-means and that would give us a set of clusteringings, and a set of cluster centroids, and then we would then compute the distortion J, that is compute this cause function on the set of cluster assignments and cluster centroids that we got. Finally, having done this whole procedure a hundred times. You will have a hundred different ways of clustering the data and then finally what you do is all of these hundred ways you have found of clustering the data, just pick one, that gives us the lowest cost. That gives us the lowest distortion. And it turns out that if you are running K-means with a fairly small number of clusters , so you know if the number of clusters is anywhere from two up to maybe 10 - then doing multiple random initializations can often, can sometimes make sure that you find a better local optima. Make sure you find the better clustering data. But if K is very large, so, if K is much greater than 10, certainly if K were, you know, if you were trying to find hundreds of clusters, then, having multiple random initializations is less likely to make a huge difference and there is a much higher chance that your first random initialization will give you a pretty decent solution already and doing, doing multiple random initializations will probably give you a slightly better solution but, but maybe not that much. But it's really in the regime of where you have a relatively small number of clusters, especially if you have, maybe 2 or 3 or 4 clusters that random initialization could make a huge difference in terms of making sure you do a good job minimizing the distortion function and giving you a good clustering. So, that's K-means with random initialization. If you're trying to learn a clustering with a relatively small number of clusters, 2, 3, 4, 5, maybe, 6, 7, using multiple random initializations can sometimes, help you find much better clustering of the data. But, even if you are learning a large number of clusters, the initialization, the random initialization method that I describe here. That should give K-means a reasonable starting point to start from for finding a good set of clusters.""",80,0,1
coursera,stanford_university,machine-learning,choosing-the-number-of-clusters,"b'In this video I\'d like to talk about one last detail of K-means clustering which is how to choose the number of clusters, or how to choose the value of the parameter capsule K. To be honest, there actually isn\'t a great way of answering this or doing this automatically and by far the most common way of choosing the number of clusters, is still choosing it manually by looking at visualizations or by looking at the output of the clustering algorithm or something else. But I do get asked this question quite a lot of how do you choose the number of clusters, and so I just want to tell you know what are peoples\' current thinking on it although, the most common thing is actually to choose the number of clusters by hand. A large part of why it might not always be easy to choose the number of clusters is that it is often generally ambiguous how many clusters there are in the data. Looking at this data set some of you may see four clusters and that would suggest using K equals 4. Or some of you may see two clusters and that will suggest K equals 2 and now this may see three clusters. And so, looking at the data set like this, the true number of clusters, it actually seems genuinely ambiguous to me, and I don\'t think there is one right answer. And this is part of our supervised learning. We are aren\'t given labels, and so there isn\'t always a clear cut answer. And this is one of the things that makes it more difficult to say, have an automatic algorithm for choosing how many clusters to have. When people talk about ways of choosing the number of clusters, one method that people sometimes talk about is something called the Elbow Method. Let me just tell you a little bit about that, and then mention some of its advantages but also shortcomings. So the Elbow Method, what we\'re going to do is vary K, which is the total number of clusters. So, we\'re going to run K-means with one cluster, that means really, everything gets grouped into a single cluster and compute the cost function or compute the distortion J and plot that here. And then we\'re going to run K means with two clusters, maybe with multiple random initial agents, maybe not. But then, you know, with two clusters we should get, hopefully, a smaller distortion, and so plot that there. And then run K-means with three clusters, hopefully, you get even smaller distortion and plot that there. I\'m gonna run K-means with four, five and so on. And so we end up with a curve showing how the distortion, you know, goes down as we increase the number of clusters. And so we get a curve that maybe looks like this. And if you look at this curve, what the Elbow Method does it says ""Well, let\'s look at this plot. Looks like there\'s a clear elbow there"". Right, this is, would be by analogy to the human arm where, you know, if you imagine that you reach out your arm, then, this is your shoulder joint, this is your elbow joint and I guess, your hand is at the end over here. And so this is the Elbow Method. Then you find this sort of pattern where the distortion goes down rapidly from 1 to 2, and 2 to 3, and then you reach an elbow at 3, and then the distortion goes down very slowly after that. And then it looks like, you know what, maybe using three clusters is the right number of clusters, because that\'s the elbow of this curve, right? That it goes down, distortion goes down rapidly until K equals 3, really goes down very slowly after that. So let\'s pick K equals 3. If you apply the Elbow Method, and if you get a plot that actually looks like this, then, that\'s pretty good, and this would be a reasonable way of choosing the number of clusters. It turns out the Elbow Method isn\'t used that often, and one reason is that, if you actually use this on a clustering problem, it turns out that fairly often, you know, you end up with a curve that looks much more ambiguous, maybe something like this. And if you look at this, I don\'t know, maybe there\'s no clear elbow, but it looks like distortion continuously goes down, maybe 3 is a good number, maybe 4 is a good number, maybe 5 is also not bad. And so, if you actually do this in a practice, you know, if your plot looks like the one on the left and that\'s great. It gives you a clear answer, but just as often, you end up with a plot that looks like the one on the right and is not clear where the ready location of the elbow is. It  makes it harder to choose a number of clusters using this method. So maybe the quick summary of the Elbow Method is that is worth the shot but I wouldn\'t necessarily, you know, have a very high expectation of it working for any particular problem. Finally, here\'s one other way of how, thinking about how you choose the value of K, very often people are running K-means in order you get clusters for some later purpose, or for some sort of downstream purpose. Maybe you want to use K-means in order to do market segmentation, like in the T-shirt sizing example that we talked about. Maybe you want K-means to organize a computer cluster better, or maybe a learning cluster for some different purpose, and so, if that later, downstream purpose, such as market segmentation. If that gives you an evaluation metric, then often, a better way to determine the number of clusters, is to see how well different numbers of clusters serve that later downstream purpose. Let me step through a specific example. Let me go through the T-shirt size example again, and I\'m trying to decide, do I want three T-shirt sizes? So, I choose K equals 3, then I might have small, medium and large T-shirts. Or maybe, I want to choose K equals 5, and then I might have, you know, extra small, small, medium, large and extra large T-shirt sizes. So, you can have like 3 T-shirt sizes or four or five T-shirt sizes. We could also have four T-shirt sizes, but I\'m just showing three and five here, just to simplify this slide for now. So, if I run K-means with K equals 3, maybe I end up with, that\'s my small and that\'s my medium and that\'s my large. Whereas, if I run K-means with 5 clusters, maybe I end up with, those are my extra small T-shirts, these are my small, these are my medium, these are my large and these are my extra large. And the nice thing about this example is that, this then maybe gives us another way to choose whether we want 3 or 4 or 5 clusters, and in particular, what you can do is, you know, think about this from the perspective of the T-shirt business and ask: ""Well if I have five segments, then how well will my T-shirts fit my customers and so, how many T-shirts can I sell? How happy will my customers be?"" What really makes sense, from the perspective of the T-shirt business, in terms of whether, I want to have Goer T-shirt sizes so that my T-shirts fit my customers better. Or do I want to have fewer T-shirt sizes so that I make fewer sizes of T-shirts. And I can sell them to the customers more cheaply. And so, the t-shirt selling business, that might give you a way to decide, between three clusters versus five clusters. So, that gives you an example of how a later downstream purpose like the problem of deciding what T-shirts to manufacture, how that can give you an evaluation metric for choosing the number of clusters. For those of you that are doing the program exercises, if you look at this week\'s program exercise associative K-means, that\'s an example there of using K-means for image compression. And so if you were trying to choose how many clusters to use for that problem, you could also, again use the evaluation metric of image compression to choose the number of clusters, K? So, how good do you want the image to look versus, how much do you want to compress the file size of the image, and, you know, if you do the programming exercise, what I\'ve just said will make more sense at that time. So, just summarize, for the most part, the number of customers K is still chosen by hand by human input or human insight. One way to try to do so is to use the Elbow Method, but I wouldn\'t always expect that to work well, but I think the better way to think about how to choose the number of clusters is to ask, for what purpose are you running K-means? And then to think, what is the number of clusters K that serves that, you know, whatever later purpose that you actually run the K-means for.'",81,0,1
coursera,stanford_university,machine-learning,motivation-i-data-compression,"b'In this video, I\'d like to start talking about a second type of unsupervised learning problem called dimensionality reduction. There are a couple of different reasons why one might want to do dimensionality reduction. One is data compression, and as we\'ll see later, a few videos later, data compression not only allows us to compress the data and have it therefore use up less computer memory or disk space, but it will also allow us to speed up our learning algorithms. But first, let\'s start by talking about what is dimensionality reduction. As a motivating example, let\'s say that we\'ve collected a data set with many, many, many features, and I\'ve plotted just two of them here. And let\'s say that unknown to us two of the features were actually the length of something in centimeters, and a different feature, x2, is the length of the same thing in inches. So, this gives us a highly redundant representation and maybe instead of having two separate features x1 then x2, both of which basically measure the length, maybe what we want to do is reduce the data to one-dimensional and just have one number measuring this length. In case this example seems a bit contrived, this centimeter and inches example is actually not that unrealistic, and not that different from things that I see happening in industry. If you have hundreds or thousands of features, it is often this easy to lose track of exactly what features you have. And sometimes may have a few different engineering teams, maybe one engineering team gives you two hundred features, a second engineering team gives you another three hundred features, and a third engineering team gives you five hundred features so you have a thousand features all together, and it actually becomes hard to keep track of you know, exactly which features you got from which team, and it\'s actually not that want to have highly redundant features like these. And so if the length in centimeters were rounded off to the nearest centimeter and lengthened inches was rounded off to the nearest inch. Then, that\'s why these examples don\'t lie perfectly on a straight line, because of, you know, round-off error to the nearest centimeter or the nearest inch. And if we can reduce the data to one dimension instead of two dimensions, that reduces the redundancy. For a different example, again maybe when there seems fairly less contrives. For may years I\'ve been working with autonomous helicopter pilots. Or I\'ve been working with pilots that fly helicopters. And so. If you were to measure--if you were to, you know, do a survey or do a test of these different pilots--you might have one feature, x1, which is maybe the skill of these helicopter pilots, and maybe ""x2"" could be the pilot enjoyment. That is, you know, how much they enjoy flying, and maybe these two features will be highly correlated. And what you really care about might be this sort of this sort of, this direction, a different feature that really measures pilot aptitude. And I\'m making up the name aptitude of course, but again, if you highly correlated features, maybe you really want to reduce the dimension. So, let me say a little bit more about what it really means to reduce the dimension of the data from 2 dimensions down from 2D to 1 dimensional or to 1D. Let me color in these examples by using different colors. And in this case by reducing the dimension what I mean is that I would like to find maybe this line, this, you know, direction on which most of the data seems to lie and project all the data onto that line which is true, and by doing so, what I can do is just measure the position of each of the examples on that line. And what I can do is come up with a new feature, z1, and to specify the position on the line I need only one number, so it says z1 is a new feature that specifies the location of each of those points on this green line. And what this means, is that where as previously if i had an example x1, maybe this was my first example, x1. So in order to represent x1 originally x1. I needed a two dimensional number, or a two dimensional feature vector. Instead now I can represent z1. I could use just z1 to represent my first example, and that\'s going to be a real number. And similarly x2 you know, if x2 is my second example there, then previously, whereas this required two numbers to represent if I instead compute the projection of that black cross onto the line. And now I only need one real number which is z2 to represent the location of this point z2 on the line. And so on through my M examples. So, just to summarize, if we allow ourselves to approximate the original data set by projecting all of my original examples onto this green line over here, then I need only one number, I need only real number to specify the position of a point on the line, and so what I can do is therefore use just one number to represent the location of each of my training examples after they\'ve been projected onto that green line. So this is an approximation to the original training self because I have projected all of my training examples onto a line. But now, I need to keep around only one number for each of my examples. And so this halves the memory requirement, or a space requirement, or what have you, for how to store my data. And perhaps more interestingly, more importantly, what we\'ll see later, in the later video as well is that this will allow us to make our learning algorithms run more quickly as well. And that is actually, perhaps, even the more interesting application of this data compression rather than reducing the memory or disk space requirement for storing the data. On the previous slide we showed an example of reducing data from 2D to 1D. On this slide, I\'m going to show another example of reducing data from three dimensional 3D to two dimensional 2D. By the way, in the more typical example of dimensionality reduction we might have a thousand dimensional data or 1000D data that we might want to reduce to let\'s say a hundred dimensional or 100D, but because of the limitations of what I can plot on the slide. I\'m going to use examples of 3D to 2D, or 2D to 1D. So, let\'s have a data set like that shown here. And so, I would have a set of examples x(i) which are points in r3. So, I have three dimension examples. I know it might be a little bit hard to see this on the slide, but I\'ll show a 3D point cloud in a little bit. And it might be hard to see here, but all of this data maybe lies roughly on the plane, like so. And so what we can do with dimensionality reduction, is take all of this data and project the data down onto a two dimensional plane. So, here what I\'ve done is, I\'ve taken all the data and I\'ve projected all of the data, so that it all lies on the plane. Now, finally, in order to specify the location of a point within a plane, we need two numbers, right? We need to, maybe, specify the location of a point along this axis, and then also specify it\'s location along that axis. So, we need two numbers, maybe called z1 and z2 to specify the location of a point within a plane. And so, what that means, is that we can now represent each example, each training example, using two numbers that I\'ve drawn here, z1, and z2. So, our data can be represented using vector z which are in r2. And these subscript, z subscript 1, z subscript 2, what I just mean by that is that my vectors here, z, you know, are two dimensional vectors, z1, z2. And so if I have some particular examples, z(i), or that\'s the two dimensional vector, z(i)1, z(i)2. And on the previous slide when I was reducing data to one dimensional data then I had only z1, right? And that is what a z1 subscript 1 on the previous slide was, but here I have two dimensional data, so I have z1 and z2 as the two components of the data. Now, let me just make sure that these figures make sense. So let me just reshow these exact three figures again but with 3D plots. So the process we went through was that shown in the lab is the optimal data set, in the middle the data set projects on the 2D, and on the right the 2D data sets with z1 and z2 as the axis. Let\'s look at them a little bit further. Here\'s my original data set, shown on the left, and so I had started off with a 3D point cloud like so, where the axis are labeled x1, x2, x3, and so there\'s a 3D point but most of the data, maybe roughly lies on some, you know, not too far from some 2D plain. So, what we can do is take this data and here\'s my middle figure. I\'m going to project it onto 2D. So, I\'ve projected this data so that all of it now lies on this 2D surface. As you can see all the data lies on a plane, \'cause we\'ve projected everything onto a plane, and so what this means is that now I need only two numbers, z1 and z2, to represent the location of point on the plane. And so that\'s the process that we can go through to reduce our data from three dimensional to two dimensional. So that\'s dimensionality reduction and how we can use it to compress our data. And as we\'ll see later this will allow us to make some of our learning algorithms run much later as well, but we\'ll get to that only in a later video.'",82,0,1
coursera,stanford_university,machine-learning,motivation-ii-visualization,"b""In the last video, we talked about dimensionality reduction for the purpose of compressing the data. In this video, I'd like to tell you about a second application of dimensionality reduction and that is to visualize the data. For a lot of machine learning applications, it really helps us to develop effective learning algorithms, if we can understand our data better. If there is some way of visualizing the data better, and so, dimensionality reduction offers us, often, another useful tool to do so. Let's start with an example. Let's say we've collected a large data set of many statistics and facts about different countries around the world. So, maybe the first feature, X1 is the country's GDP, or the Gross Domestic Product, and X2 is a per capita, meaning the per person GDP, X3 human development index, life expectancy, X5, X6 and so on. And we may have a huge data set like this, where, you know, maybe 50 features for every country, and we have a huge set of countries. So is there something we can do to try to understand our data better? I've given this huge table of numbers. How do you visualize this data? If you have 50 features, it's very difficult to plot 50-dimensional data. What is a good way to examine this data? Using dimensionality reduction, what we can do is, instead of having each country represented by this featured vector, xi, which is 50-dimensional, so instead of, say, having a country like Canada, instead of having 50 numbers to represent the features of Canada, let's say we can come up with a different feature representation that is these z vectors, that is in R2. If that's the case, if we can have just a pair of numbers, z1 and z2 that somehow, summarizes my 50 numbers, maybe what we can do  [xx] is to plot these countries in R2 and use that to try to understand the space in [xx] of features of different countries [xx]  the better and so, here, what you can do is reduce the data from 50 D, from 50 dimensions to 2D, so you can plot this as a 2 dimensional plot, and, when you do that, it turns out that, if you look at the output of the Dimensionality Reduction algorithms, It usually doesn't astride a physical meaning to these new features you want [xx] to. It's often up to us to figure out you know, roughly what these features means. But, And if you plot those features, here is what you might find. So, here, every country is represented by a point ZI, which is an R2 and so each of those. Dots, and this figure represents a country, and so, here's Z1 and here's Z2, and [xx] [xx] of these. So, you might find, for example, That the horizontial axis the Z1 axis corresponds roughly to the overall country size, or the overall economic activity of a country. So the overall GDP, overall economic size of a country. Whereas the vertical axis in our data might correspond to the per person GDP. Or the per person well being, or the per person economic activity, and, you might find that, given these 50 features, you know, these are really the 2 main dimensions of the deviation, and so, out here you may have a country like the U.S.A., which is a relatively large GDP, you know, is a very large GDP and a relatively high per-person GDP as well. Whereas here you might have a country like Singapore, which actually has a very high per person GDP as well, but because Singapore is a much smaller country the overall economy size of Singapore is much smaller than the US. And, over here, you would have countries where individuals are unfortunately some are less well off, maybe shorter life expectancy, less health care, less economic maturity that's why smaller countries, whereas a point like this will correspond to a country that has a fair, has a substantial amount of economic activity, but where individuals tend to be somewhat less well off. So you might find that the axes Z1 and Z2 can help you to most succinctly capture really what are the two main dimensions of the variations amongst different countries. Such as the overall economic activity of the country projected by the size of the country's overall economy as well as the per-person individual well-being, measured by per-person GDP, per-person healthcare, and things like that. So that's how you can use dimensionality reduction, in order to reduce data from 50 dimensions or whatever, down to two dimensions, or maybe down to three dimensions, so that you can plot it and understand your data better. In the next video, we'll start to develop a specific algorithm, called PCA, or Principal Component Analysis, which will allow us to do this and also do the earlier application I talked about of compressing the data.""",83,0,1
coursera,stanford_university,machine-learning,principal-component-analysis-problem-formulation,"b""For the problem of dimensionality\nreduction, by far the most popular, by far the most commonly used\nalgorithm is something called principle components analysis, or PCA. In this video, I'd like to start talking\nabout the problem formulation for PCA. In other words, let's try to formulate, precisely, exactly what\nwe would like PCA to do. Let's say we have a data set like this. So, this is a data set of examples x and\nR2 and let's say I want to reduce the dimension of the data from\ntwo-dimensional to one-dimensional. In other words, I would like to find\na line onto which to project the data. So what seems like a good line\nonto which to project the data, it's a line like this,\nmight be a pretty good choice. And the reason we think this might be\na good choice is that if you look at where the projected versions of the point\nscales, so I take this point and project it down here. Get that, this point gets projected here,\nto here, to here, to here. What we find is that the distance\nbetween each point and the projected version is pretty small. That is,\nthese blue line segments are pretty short. So what PCA does formally is it tries\nto find a lower dimensional surface, really a line in this case,\nonto which to project the data so that the sum of squares of these little\nblue line segments is minimized. The length of those blue line segments, that's sometimes also called\nthe projection error. And so what PCA does is it tries to find a\nsurface onto which to project the data so as to minimize that. As an aside, before applying PCA,\nit's standard practice to first perform mean normalization at feature\nscaling so that the features x1 and x2 should have zero mean, and\nshould have comparable ranges of values. I've already done this for this example,\nbut I'll come back to this later and talk more about feature scaling and the\nnormalization in the context of PCA later. But coming back to this example, in\ncontrast to the red line that I just drew, here's a different line onto\nwhich I could project my data, which is this magenta line. And, as we'll see, this magenta\nline is a much worse direction onto which to project my data, right? So if I were to project my\ndata onto the magenta line, we'd get a set of points like that. And the projection errors, that is\nthese blue line segments, will be huge. So these points have to move\na huge distance in order to get projected onto the magenta line. And so that's why PCA,\nprincipal components analysis, will choose something like the red line\nrather than the magenta line down here. Let's write out the PCA problem\na little more formally. The goal of PCA, if we want to\nreduce data from two-dimensional to one-dimensional is, we're going to try\nfind a vector that is a vector u1, which is going to be an Rn, so\nthat would be an R2 in this case. I'm gonna find the direction onto\nwhich to project the data, so it's to minimize the projection error. So, in this example I'm hoping\nthat PCA will find this vector, which l wanna call u(1), so\nthat when I project the data onto the line that I define by\nextending out this vector, I end up with pretty small\nreconstruction errors. And that reference of data\nthat looks like this. And by the way, I should mention\nthat where the PCA gives me u(1) or -u(1), doesn't matter. So if it gives me a positive vector\nin this direction, that's fine. If it gives me the opposite vector\nfacing in the opposite direction, so that would be like minus u(1). Let's draw that in blue instead, right? But it gives a positive u(1) or\nnegative u(1), it doesn't matter because each of these vectors defines the same red\nline onto which I'm projecting my data. So this is a case of reducing data from\ntwo-dimensional to one-dimensional. In the more general case we\nhave n-dimensional data and we'll want to reduce it to k-dimensions. In that case we want to find not just\na single vector onto which to project the data but we want to find k-dimensions\nonto which to project the data. So as to minimize this projection error. So here's the example. If I have a 3D point cloud like this, then\nmaybe what I want to do is find vectors. So find a pair of vectors. And I'm gonna call these vectors. Let's draw these in red. I'm going to find a pair of vectors,\nsustained from the origin. Here's u(1), and\nhere's my second vector, u(2). And together, these two vectors define a\nplane, or they define a 2D surface, right? Like this with a 2D surface onto\nwhich I am going to project my data. For those of you that are familiar\nwith linear algebra, for this year they're really experts in linear\nalgebra, the formal definition of this is that we are going to find the set of\nvectors u(1), u(2), maybe up to u(k). And what we're going to\ndo is project the data onto the linear subspace spanned\nby this set of k vectors. But if you're not familiar with\nlinear algebra, just think of it as finding k directions instead of just one\ndirection onto which to project the data. So finding a k-dimensional surface is\nreally finding a 2D plane in this case, shown in this figure, where we can define the position of\nthe points in a plane using k directions. And that's why for PCA we want to find k\nvectors onto which to project the data. And so more formally in PCA, what we want\nto do is find this way to project the data so as to minimize the sort\nof projection distance, which is the distance between\nthe points and the projections. And so in this 3D example too. Given a point we would take the point and\nproject it onto this 2D surface. We are done with that. And so the projection error would be,\nthe distance between the point and where it gets projected\ndown to my 2D surface. And so what PCA does is I try to find\nthe line, or a plane, or whatever, onto which to project the data,\nto try to minimize that square projection, that 90 degree or\nthat orthogonal projection error. Finally, one question I sometimes\nget asked is how does PCA relate to linear regression? Because when explaining PCA, I sometimes\nend up drawing diagrams like these and that looks a little bit\nlike linear regression. It turns out PCA is not linear regression,\nand despite some cosmetic similarity, these\nare actually totally different algorithms. If we were doing linear regression, what\nwe would do would be, on the left we would be trying to predict the value of some\nvariable y given some info features x. And so linear regression, what we're\ndoing is we're fitting a straight line so as to minimize the square error\nbetween point and this straight line. And so what we're minimizing would be\nthe squared magnitude of these blue lines. And notice that I'm drawing\nthese blue lines vertically. That these blue lines are the vertical\ndistance between the point and the value predicted by the hypothesis. Whereas in contrast, in PCA,\nwhat it does is it tries to minimize the magnitude of these blue lines,\nwhich are drawn at an angle. These are really the shortest\northogonal distances. The shortest distance between\nthe point x and this red line. And this gives very different\neffects depending on the dataset. And more generally,\nwhen you're doing linear regression, there is this distinguished variable\ny they we're trying to predict. All that linear regression as well\nas taking all the values of x and try to use that to predict y. Whereas in PCA,\nthere is no distinguish, or there is no special variable y\nthat we're trying to predict. And instead, we have a list of features,\nx1, x2, and so on, up to xn, and all of these features are treated equally,\nso no one of them is special. As one last example,\nif I have three-dimensional data and I want to reduce data from 3D to 2D,\nso maybe I wanna find two directions, u(1) and u(2),\nonto which to project my data. Then what I have is I have three features,\nx1, x2, x3, and all of these are treated alike. All of these are treated symmetrically and there's no special variable y\nthat I'm trying to predict. And so PCA is not a linear regression, and even though at some cosmetic\nlevel they might look related, these are actually very\ndifferent algorithms. So hopefully you now\nunderstand what PCA is doing. It's trying to find a lower dimensional\nsurface onto which to project the data, so as to minimize this\nsquared projection error. To minimize the square distance\nbetween each point and the location of where it gets projected. In the next video,\nwe'll start to talk about how to actually find this lower dimensional\nsurface onto which to project the data.""",84,0,1
coursera,stanford_university,machine-learning,principal-component-analysis-algorithm,"b'In this video I\'d like to tell you about the principle components analysis algorithm. And by the end of this video you know to implement PCA for yourself. And use it reduce the dimension of your data. Before applying PCA, there is a data pre-processing step which you should always do. Given the trading sets of the examples is important to always perform mean normalization, and then depending on your data, maybe perform feature scaling as well. this is very similar to the mean normalization and feature scaling process that we have for supervised learning. In fact it\'s exactly the same procedure except that we\'re doing it now to our unlabeled data, X1 through Xm. So for mean normalization we first compute the mean of each feature and then we replace each feature, X, with X minus its mean, and so this makes each feature now have exactly zero mean The different features have very different scales. So for example, if x1 is the size of a house, and x2 is the number of bedrooms, to use our earlier example, we then also scale each feature to have a comparable range of values. And so, similar to what we had with supervised learning, we would take x, i substitute j, that\'s the j feature and so we would subtract of the mean, now that\'s what we have on top, and then divide by sj. Here, sj is some measure of the beta values of feature j.  So, it could be the max minus min value, or more commonly, it is the standard deviation of feature j. Having done this sort of data pre-processing, here\'s what the PCA algorithm does. We saw from the previous video that what PCA does is, it tries to find a lower dimensional sub-space onto which to project the data, so as to minimize the squared projection errors, sum of the squared projection errors, as the square of the length of those blue lines that and so what we wanted to do specifically is find a vector, u1, which specifies that direction or in the 2D case we want to find two vectors, u1 and u2, to define this surface onto which to project the data. So, just as a quick reminder of what reducing the dimension of the data means, for this example on the left we were given the examples xI, which are in r2. And what we like to do is find a set of numbers zI in r push to represent our data. So that\'s what from reduction from 2D to 1D means. So specifically by projecting data onto this red line there. We need only one number to specify the position of the points on the line. So i\'m going to call that number z or z1. Z here  [xx] real number, so that\'s like a one dimensional vector. So z1 just refers to the first component of this, you know, one by one matrix, or this one dimensional vector. And so we need only one number to specify the position of a point. So if this example here was my example X1, then maybe that gets mapped here. And if this example was X2 maybe that example gets mapped And so this point here will be Z1 and this point here will be Z2, and similarly we would have those other points for These, maybe X3, X4, X5 get mapped to Z1, Z2, Z3. So What PCA has to do is we need to come up with a way to compute two things. One is to compute these vectors, u1, and in this case u1 and u2. And the other is how do we compute these numbers, Z. So on the example on the left we\'re reducing the data from 2D to 1D. In the example on the right, we would be reducing data from 3 dimensional as in r3, to zi, which is now two dimensional. So these z vectors would now be two dimensional. So it would be z1 z2 like so, and so we need to give away to compute these new representations, the z1 and z2 of the data as well. So how do you compute all of these quantities? It turns out that a mathematical derivation, also the mathematical proof, for what is the right value U1, U2, Z1, Z2, and so on. That mathematical proof is very complicated and beyond the scope of the course. But once you\'ve done  [xx] it turns out that the procedure to actually find the value of u1 that you want is not that hard, even though so that the mathematical proof that this value is the correct value is someone more involved and more than i want to get into. But let me just describe the specific procedure that you have to implement in order to compute all of these things, the vectors, u1, u2, the vector z.  Here\'s the procedure. Let\'s say we want to reduce the data to n dimensions to k dimension What we\'re going to do is first compute something called the covariance matrix, and the covariance matrix is commonly denoted by this Greek alphabet which is the capital Greek alphabet sigma. It\'s a bit unfortunate that the Greek alphabet sigma looks exactly like the summation symbols. So this is the Greek alphabet Sigma is used to denote a matrix and this here is a summation symbol. So hopefully in these slides there won\'t be ambiguity about which is Sigma Matrix, the matrix, which is a summation symbol, and hopefully it will be clear from context when I\'m using each one. How do you compute this matrix let\'s say we want to store it in an octave variable called sigma. What we need to do is compute something called the eigenvectors of the matrix sigma. And an octave, the way you do that is you use this command, u s v equals s v d of sigma. SVD, by the way, stands for singular value decomposition. This is a Much more advanced single value composition. It is much more advanced linear algebra than you actually need to know but now It turns out that when sigma is equal to matrix there is a few ways to compute these are high in vectors and If you are an expert in linear algebra and if you\'ve heard of high in vectors before you may know that there is another octet function called I, which can also be used to compute the same thing. and It turns out that the SVD function and the I function it will give you the same vectors, although SVD is a little more numerically stable. So I tend to use SVD, although I have a few friends that use the I function to do this as wellbut when you apply this to a covariance matrix sigma it gives you the same thing. This is because the covariance matrix always satisfies a mathematical Property called symmetric positive definite You really don\'t need to know what that means, but the SVD and I-functions are different functions but when they are applied to a covariance matrix which can be proved to always satisfy this mathematical property; they\'ll always give you the same thing. Okay, that was probably much more linear algebra than you needed to know. In case none of that made sense, don\'t worry about it. All you need to know is that this system command you should implement in Octave. And if you\'re implementing this in a different language than Octave or MATLAB, what you should do is find the numerical linear algebra library that can compute the SVD or singular value decomposition, and there are many such libraries for probably all of the major programming languages. People can use that to compute the matrices u, s, and d of the covariance matrix sigma. So just to fill in some more details, this covariance matrix sigma will be an n by n matrix. And one way to see that is if you look at the definition this is an n by 1 vector and this here I transpose is 1 by N so the product of these two things is going to be an N by N matrix. 1xN transfers, 1xN, so there\'s an NxN matrix and when we add up all of these you still have an NxN matrix. And what the SVD outputs three matrices, u, s, and v.  The thing you really need out of the SVD is the u matrix. The u matrix will also be a NxN matrix. And if we look at the columns of the U matrix it turns out that the columns of the U matrix will be exactly those vectors, u1, u2 and so on. So u, will be matrix. And if we want to reduce the data from n dimensions down to k dimensions, then what we need to do is take the first k vectors. that gives us u1 up to uK which gives us the K direction onto which we want to project the data. the rest of the procedure from this SVD numerical linear algebra routine we get this matrix u.  We\'ll call these columns u1-uN. So, just to wrap up the description of the rest of the procedure, from the SVD numerical linear algebra routine we get these matrices u, s, and d.  we\'re going to use the first K columns of this matrix to get u1-uK. Now the other thing we need to is take my original data set, X which is an RN And find a lower dimensional representation Z, which is a R K for this data. So the way we\'re going to do that is take the first K Columns of the U matrix. Construct this matrix. Stack up U1, U2 and so on up to U K in columns. It\'s really basically taking, you know, this part of the matrix, the first K columns of this matrix. And so this is going to be an N by K matrix. I\'m going to give this matrix a name. I\'m going to call this matrix U, subscript ""reduce,"" sort of a reduced version of the U matrix maybe. I\'m going to use it to reduce the dimension of my data. And the way I\'m going to compute Z is going to let Z be equal to this U reduce matrix transpose times X. Or alternatively, you know, to write down what this transpose means. When I take this transpose of this U matrix, what I\'m going to end up with is these vectors now in rows. I have U1 transpose down to UK transpose. Then take that times X, and that\'s how I get my vector Z. Just to make sure that these dimensions make sense, this matrix here is going to be k by n and x here is going to be n by 1 and so the product here will be k by 1. And so z is k dimensional, is a k dimensional vector, which is exactly what we wanted. And of course these x\'s here right, can be Examples in our training set can be examples in our cross validation set, can be examples in our test set, and for example if you know, I wanted to take training example i, I can write this as xi XI and that\'s what will give me ZI over there. So, to summarize, here\'s the PCA algorithm on one slide. After mean normalization, to ensure that every feature is zero mean and optional feature scaling whichYou really should do feature scaling if your features take on very different ranges of values. After this pre-processing we compute the carrier matrix Sigma like so by the way if your data is given as a matrix like hits if you have your data Given in rows like this. If you have a matrix X which is your time trading sets written in rows where x1 transpose down to x1 transpose, this covariance matrix sigma actually has a nice vectorizing implementation. You can implement in octave, you can even run sigma equals 1 over m, times x, which is this matrix up here, transpose times x and this simple expression, that\'s the vectorize implementation of how to compute the matrix sigma. I\'m not going to prove that today. This is the correct vectorization whether you want, you can either numerically test this on yourself by trying out an octave and making sure that both this and this implementations give the same answers or you Can try to prove it yourself mathematically. Either way but this is the correct vectorizing implementation, without compusingnext we can apply the SVD routine to get u, s, and d. And then we grab the first k columns of the u matrix you reduce and finally this defines how we go from a feature vector x to this reduce dimension representation z. And similar to k Means if you\'re apply PCA, they way you\'d apply this is with vectors X and RN. So, this is not done with X-0 1. So that was the PCA algorithm. One thing I didn\'t do is give a mathematical proof that this There it actually give the projection of the data onto the K dimensional subspace onto the K dimensional surface that actually minimizes the square projection error Proof of that is beyond the scope of this course. Fortunately the PCA algorithm can be implemented in not too many lines of code. and if you implement this in octave or algorithm, you actually get a very effective dimensionality reduction algorithm. So, that was the PCA algorithm. One thing I didn\'t do was give a mathematical proof that the U1 and U2 and so on and the Z and so on you get out of this procedure is really the choices that would minimize these squared projection error. Right, remember we said What PCA tries to do is try to find a surface or line onto which to project the data so as to minimize to square projection error. So I didn\'t prove that this that, and the mathematical proof of that is beyond the scope of this course. But fortunately the PCA algorithm can be implemented in not too many lines of octave code. And if you implement this, this is actually what will work, or this will work well, and if you implement this algorithm, you get a very effective dimensionality reduction algorithm. That does do the right thing of minimizing this square projection error.'",85,0,1
coursera,stanford_university,machine-learning,reconstruction-from-compressed-representation,"b""In some of the earlier videos, I was talking about PCA as a compression\nalgorithm where you may have say, 1,000-dimensional data and compress\nit to 100-dimensional feature vector. Or have three-dimensional data and compress it to a two-dimensional\nrepresentation. So, if this is a compression algorithm, there should be a way to go back\nfrom this compressed representation back to an approximation of your\noriginal high-dimensional data. So given zi, which may be 100-dimensional,\nhow do you go back to your original representation,\nxi which was maybe a 1000-dimensional. In this video,\nI'd like to describe how to do that. In the PCA algorithm,\nwe may have an example like this, so maybe that's my example x1, and\nmaybe that's my example x2. And what we do is we take these examples, and we project them onto this\none dimensional surface. And then now we need to use a real number,\nsay z1, to specify the location of these points after they've been projected\nonto this one dimensional surface. So, given the point z1, how can we go back\nto this original two dimensional space? In particular,\ngiven the point z, which is R, can we map this back to some\napproximate representation x and R2 of whatever the original\nvalue of the data was? So whereas z equals U reduce transpose x, if you want to go in the opposite\ndirection, the equation for that is, we're going to write x\napprox equals U reduce, times z. And again, just to check the dimensions,\nhere U reduce is going to be an n by k dimensional vector, z is going\nto be k by one dimensional vector. So you multiply these out\nthat's going to be n by one, so x approx is going to be\nan n dimensional vector. And so the intent of PCA, that is if the\nsquare projection error is not too big, is that this x approx\nwill be close to whatever was the original value of x that you have\nused to derive z in the first place. To show a picture of what this looks like,\nthis is what it looks like. What you get back of this procedure are\npoints that lie on the projection of that, onto the green line. So to take our early example, if we\nstarted off with this value of x1, and we got this value of z1, if you plug z1\nthrough this formula to get x1 approx, then this point here,\nthat would be x1 approx, which is going to be in R2. And similarly, if you do the same\nprocedure, this would be x2 approx. And that's a pretty decent\napproximation to the original data. So that's how you go back from your\nlow dimensional representation z, back to an uncompressed\nrepresentation of the data. We get back an approximation\nto your original data x. And we also call this process\nreconstruction of the original data where we think of trying to\nreconstruct the original value of x from the compressed representation. So, given an unlabeled data set, you now\nknow how to apply PCA and take your high dimensional features x and map that to\nthis lower-dimensional representation z. And from this video hopefully you now also\nknow how to take these low-representation z and map it back up to an approximation\nof your original high-dimensional data. Now that you know how to implement and\napply PCA, what I'd like to do next is talk about some of the mechanics\nof how to actually use PCA well. And in particular in the next video,\nI'd like to talk about how to choose k, which is how to choose the dimension\nof the reduced representation vector z.""",86,0,1
coursera,stanford_university,machine-learning,choosing-the-number-of-principal-components,"b'In the PCA algorithm we take N dimensional features and reduce them to some K dimensional feature representation. This number K is a parameter of the PCA algorithm. This number K is also called the number of principle components or the number of principle components that we\'ve retained. And in this video I\'d like to give you some guidelines, tell you about how people tend to think about how to choose this parameter K for PCA. In order to choose k, that is to choose the number of principal components, here are a couple of useful concepts. What PCA tries to do is it tries to minimize the average squared projection error. So it tries to minimize this quantity, which I\'m writing down, which is the difference between the original data X and the projected version, X-approx-i, which was defined last video, so it tries to minimize the squared distance between x and it\'s projection onto that lower dimensional surface. So that\'s the average square projection error. Also let me define the total variation in the data to be the average length squared of these examples Xi so the total variation in the data is the average of my training sets of the length of each of my training examples. And this one says, ""On average, how far are my training examples from the vector, from just being all zeros?"" How far is, how far on average are my training examples from the origin? When we\'re trying to choose k, a pretty common rule of thumb for choosing k is to choose the smaller values so that the ratio between these is less than 0.01. So in other words, a pretty common way to think about how we choose k is we want the average squared projection error. That is the average distance between x and it\'s projections divided by the total variation of the data. That is how much the data varies. We want this ratio to be less than, let\'s say, 0.01. Or to be less than 1%, which is another way of thinking about it. And the way most people think about choosing K is rather than choosing K directly the way most people talk about it is as what this number is, whether it is 0.01 or some other number. And if it is 0.01, another way to say this to use the language of PCA is that 99% of the variance is retained. I don\'t really want to, don\'t worry about what this phrase really means technically but this phrase ""99% of variance is retained"" just means that this quantity on the left is less than 0.01. And so, if you are using PCA and if you want to tell someone, you know, how many principle components you\'ve retained it would be more common to say well, I chose k so that 99% of the variance was retained. And that\'s kind of a useful thing to know, it means that you know, the average squared projection error divided by the total variation that was at most 1%. That\'s kind of an insightful thing to think about, whereas if you tell someone that, ""Well I had to 100 principle components"" or ""k was equal to 100 in a thousand dimensional data"" it\'s a little hard for people to interpret that. So this number 0.01 is what people often use. Other common values is 0.05, and so this would be 5%, and if you do that then you go and say well 95% of the variance is retained and, you know other numbers maybe 90% of the variance is retained, maybe as low as 85%. So 90% would correspond to say 0.10, kinda 10%. And so range of values from, you know, 90, 95, 99, maybe as low as 85% of the variables contained would be a fairly typical range in values. Maybe 95 to 99 is really the most common range of values that people use. For many data sets you\'d be surprised, in order to retain 99% of the variance, you can often reduce the dimension of the data significantly and still retain most of the variance. Because for most real life data says many features are just highly correlated, and so it turns out to be possible to compress the data a lot and still retain you know 99% of the variance or 95% of the variance. So how do you implement this? Well, here\'s one algorithm that you might use. You may start off, if you want to choose the value of k, we might start off with k equals 1. And then we run through PCA. You know, so we compute, you reduce, compute z1, z2, up to zm. Compute all of those x1 approx and so on up to xm approx and then we check if 99% of the variance is retained. Then we\'re good and we use k equals 1. But if it isn\'t then what we\'ll do we\'ll next try K equals 2. And then we\'ll again run through this entire procedure and check, you know is this expression satisfied. Is this less than 0.01. And if not then we do this again. Let\'s try k equals 3, then try k equals 4, and so on until maybe we get up to k equals 17 and we find 99% of the data have is retained and then we use k equals 17, right? That is one way to choose the smallest value of k, so that and 99% of the variance is retained. But as you can imagine, this procedure seems horribly inefficient we\'re trying k equals one, k equals two, we\'re doing all these calculations. Fortunately when you implement PCA it actually, in this step, it actually gives us a quantity that makes it much easier to compute these things as well. Specifically when you\'re calling SVD to get these matrices u, s, and d, when you\'re calling usvd on the covariance matrix sigma, it also gives us back this matrix S and what S is, is going to be a square matrix an N by N matrix in fact, that is diagonal. So is diagonal entries s one one, s two two, s three three down to s n n are going to be the only non-zero elements of this matrix, and everything off the diagonals is going to be zero. Okay? So those big O\'s that I\'m drawing, by that what I mean is that everything off the diagonal of this matrix all of those entries there are going to be zeros. And so, what is possible to show, and I won\'t prove this here, and it turns out that for a given value of k, this quantity over here can be computed much more simply. And that quantity can be computed as one minus sum from i equals 1 through K of Sii divided by sum from I equals 1 through N of Sii. So just to say that it words, or just to take another view of how to explain that, if K equals 3 let\'s say. What we\'re going to do to compute the numerator is sum from one--  I equals 1 through 3 of of Sii, so just compute the sum of these first three elements. So that\'s the numerator. And then for the denominator, well that\'s the sum of all of these diagonal entries. And one minus the ratio of that, that gives me this quantity over here, that I\'ve circled in blue. And so, what we can do is just test if this is less than or equal to 0.01. Or equivalently, we can test if the sum from i equals 1 through k, s-i-i divided by sum from i equals 1 through n, s-i-i if this is greater than or equal to 4.99, if you want to be sure that 99% of the variance is retained. And so what you can do is just slowly increase k, set k equals one, set k equals two, set k equals three and so on, and just test this quantity to see what is the smallest value of k that ensures that 99% of the variance is retained. And if you do this, then you need to call the SVD function only once. Because that gives you the S matrix and once you have the S matrix, you can then just keep on doing this calculation by increasing the value of K in the numerator and so you don\'t need keep to calling SVD over and over again to test out the different values of K. So this procedure is much more efficient, and this can allow you to select the value of K without needing to run PCA from scratch over and over. You just run SVD once, this gives you all of these diagonal numbers, all of these numbers S11, S22 down to SNN, and then you can just you know, vary K in this expression to find the smallest value of K, so that 99% of the variance is retained. So to summarize, the way that I often use, the way that I often choose K when I am using PCA for compression is I would call SVD once in the covariance matrix, and then I would use this formula and pick the smallest value of K for which this expression is satisfied. And by the way, even if you were to pick some different value of K, even if you were to pick the value of K manually, you know maybe you have a thousand dimensional data and I just want to choose K equals one hundred. Then, if you want to explain to others what you just did, a good way to explain the performance of your implementation of PCA to them, is actually to take this quantity and compute what this is, and that will tell you what was the percentage of variance retained. And if you report that number, then, you know, people that are familiar with PCA, and people can use this to get a good understanding of how well your hundred dimensional representation is approximating your original data set, because there\'s 99% of variance retained. That\'s really a measure of your square of construction error, that ratio being 0.01, just gives people a good intuitive sense of whether your implementation of PCA is finding a good approximation of your original data set. So hopefully, that gives you an efficient procedure for choosing the number K. For choosing what dimension to reduce your data to, and if you apply PCA to very high dimensional data sets, you know, to like a thousand dimensional data, very often, just because data sets tend to have highly correlated features, this is just a property of most of the data sets you see, you often find that PCA will be able to retain ninety nine per cent of the variance or say, ninety five ninety nine, some high fraction of the variance, even while compressing the data by a very large factor.'",87,0,1
coursera,stanford_university,machine-learning,advice-for-applying-pca,"b""In an earlier video, I had said that PCA can be sometimes used to speed up the running time of a learning algorithm. In this video, I'd like to explain how to actually do that, and also say some, just try to give some advice about how to apply PCA. Here's how you can use PCA to speed up a learning algorithm, and this supervised learning algorithm speed up is actually the most common use that I personally make of PCA. Let's say you have a supervised learning problem, note this is a supervised learning problem with inputs X and labels Y, and let's say that your examples xi are very high dimensional. So, lets say that your examples, xi are 10,000 dimensional feature vectors. One example of that, would be, if you were doing some computer vision problem, where you have a 100x100 images, and so if you have 100x100, that's 10000 pixels, and so if xi are, you know, feature vectors that contain your 10000 pixel intensity values, then you have 10000 dimensional feature vectors. So with very high-dimensional feature vectors like this, running a learning algorithm can be slow, right? Just, if you feed 10,000 dimensional feature vectors into logistic regression, or a new network, or support vector machine or what have you, just because that's a lot of data, that's 10,000 numbers, it can make your learning algorithm run more slowly. Fortunately with PCA we'll be able to reduce the dimension of this data and so make our algorithms run more efficiently. Here's how you do that. We are going first check our labeled training set and extract just the inputs, we're just going to extract the X's and temporarily put aside the Y's. So this will now give us an unlabelled training set x1 through xm which are maybe there's a ten thousand dimensional data, ten thousand dimensional examples we have. So just extract the input vectors x1 through xm. Then we're going to apply PCA and this will give me a reduced dimension representation of the data, so instead of 10,000 dimensional feature vectors I now have maybe one thousand dimensional feature vectors. So that's like a 10x savings. So this gives me, if you will, a new training set. So whereas previously I might have had an example x1, y1, my first training input, is now represented by z1. And so we'll have a new sort of training example, which is Z1 paired with y1. And similarly Z2, Y2, and so on, up to ZM, YM. Because my training examples are now represented with this much lower dimensional representation Z1, Z2, up to ZM. Finally, I can take this reduced dimension training set and feed it to a learning algorithm maybe a neural network, maybe logistic regression, and I can learn the hypothesis H, that takes this input, these low-dimensional representations Z and tries to make predictions. So if I were using logistic regression for example, I would train a hypothesis that outputs, you know, one over one plus E to the negative-theta transpose Z, that takes this input to one these z vectors, and tries to make a prediction. And finally, if you have a new example, maybe a new test example X. What you do is you would take your test example x, map it through the same mapping that was found by PCA to get you your corresponding z. And that z then gets fed to this hypothesis, and this hypothesis then makes a prediction on your input x. One final note, what PCA does is it defines a mapping from x to z and this mapping from x to z should be defined by running PCA only on the training sets. And in particular, this mapping that PCA is learning, right, this mapping, what that does is it computes the set of parameters. That's the feature scaling and mean normalization. And there's also computing this matrix U reduced. But all of these things that U reduce, that's like a parameter that is learned by PCA and we should be fitting our parameters only to our training sets and not to our cross validation or test sets and so these things the U reduced so on, that should be obtained by running PCA only on your training set. And then having found U reduced, or having found the parameters for feature scaling where the mean normalization and scaling the scale that you divide the features by to get them on to comparable scales. Having found all those parameters on the training set, you can then apply the same mapping to other examples that may be In your cross-validation sets or in your test sets, OK? Just to summarize, when you're running PCA, run your PCA only on the training set portion of the data not the cross-validation set or the test set portion of your data. And that defines the mapping from x to z and you can then apply that mapping to your cross-validation set and your test set and by the way in this example I talked about reducing the data from ten thousand dimensional to one thousand dimensional, this is actually not that unrealistic. For many problems we actually reduce the dimensional data. You know by 5x maybe by 10x and still retain most of the variance and we can do this barely effecting the performance, in terms of classification accuracy, let's say, barely affecting the classification accuracy of the learning algorithm. And by working with lower dimensional data our learning algorithm can often run much much faster. To summarize, we've so far talked about the following applications of PCA. First is the compression application where we might do so to reduce the memory or the disk space needed to store data and we just talked about how to use this to speed up a learning algorithm. In these applications, in order to choose K, often we'll do so according to, figuring out what is the percentage of variance retained, and so for this learning algorithm, speed up application often will retain 99%  of the variance. That would be a very typical choice for how to choose k. So that's how you choose k for these compression applications. Whereas for visualization applications while usually we know how to plot only two dimensional data or three dimensional data, and so for visualization applications, we'll usually choose k equals 2 or k equals 3, because we can plot only 2D and 3D data sets. So that summarizes the main applications of PCA, as well as how to choose the value of k for these different applications. I should mention that there is often one frequent misuse of PCA and you sometimes hear about others doing this hopefully not too often. I just want to mention this so that you know not to do it. And there is one bad use of PCA, which iss to try to use it to prevent over-fitting. Here's the reasoning. This is not a great way to use PCA, but here's the reasoning behind this method, which is,you know if we have Xi, then maybe we'll have n features, but if we compress the data, and use Zi instead and that reduces the number of features to k, which could be much lower dimensional. And so if we have a much smaller number of features, if k is 1,000 and n is 10,000, then if we have only 1,000 dimensional data, maybe we're less likely to over-fit than if we were using 10,000-dimensional data with like a thousand features. So some people think of PCA as a way to prevent over-fitting. But just to emphasize this is a bad application of PCA and I do not recommend doing this. And it's not that this method works badly. If you want to use this method to reduce the dimensional data, to try to prevent over-fitting, it might actually work OK. But this just is not a good way to address over-fitting and instead, if you're worried about over-fitting, there is a much better way to address it, to use regularization instead of using PCA to reduce the dimension of the data. And the reason is, if you think about how PCA works, it does not use the labels y. You are just looking at your inputs xi, and you're using that to find a lower-dimensional approximation to your data. So what PCA does, is it throws away some information. It throws away or reduces the dimension of your data without knowing what the values of y is, so this is probably okay using PCA this way is probably okay if, say 99 percent of the variance is retained, if you're keeping most of the variance, but it might also throw away some valuable information. And it turns out that if you're retaining 99% of the variance or 95% of the variance or whatever, it turns out that just using regularization will often give you at least as good a method for preventing over-fitting and regularization will often just work better, because when you are applying linear regression or logistic regression or some other method with regularization, well, this minimization problem actually knows what the values of y are, and so is less likely to throw away some valuable information, whereas PCA doesn't make use of the labels and is more likely to throw away valuable information. So, to summarize, it is a good use of PCA, if your main motivation to speed up your learning algorithm, but using PCA to prevent over-fitting, that is not a good use of PCA, and using regularization instead is really what many people would recommend doing instead. Finally, one last misuse of PCA. And so I should say PCA is a very useful algorithm, I often use it for the compression on the visualization purposes. But, what I sometimes see, is also people sometimes use PCA where it shouldn't be. So, here's a pretty common thing that I see, which is if someone is designing a machine-learning system, they may write down the plan like this: let's design a learning system. Get a training set and then, you know, what I'm going to do is run PCA, then train logistic regression and then test on my test data. So often at the very start of a project, someone will just write out a project plan than says lets do these four steps with PCA inside. Before writing down a project plan the incorporates PCA like this, one very good question to ask is, well, what if we were to just do the whole without using PCA. And often people do not consider this step before coming up with a complicated project plan and implementing PCA and so on. And sometime, and so specifically, what I often advise people is, before you implement PCA, I would first suggest that, you know, do whatever it is, take whatever it is you want to do and first consider doing it with your original raw data xi, and only if that doesn't do what you want, then implement PCA before using Zi. So, before using PCA you know, instead of reducing the dimension of the data, I would consider well, let's ditch this PCA step, and I would consider, let's just train my learning algorithm on my original data. Let's just use my original raw inputs xi, and I would recommend, instead of putting PCA into the algorithm, just try doing whatever it is you're doing with the xi first. And only if you have a reason to believe that doesn't work, so that only if your learning algorithm ends up running too slowly, or only if the memory requirement or the disk space requirement is too large, so you want to compress your representation, but if only using the xi doesn't work, only if you have evidence or strong reason to believe that using the xi won't work, then implement PCA and consider using the compressed representation. Because what I do see, is sometimes people start off with a project plan that incorporates PCA inside, and sometimes they, whatever they're doing will work just fine, even with out using PCA instead. So, just consider that as an alternative as well, before you go to spend a lot of time to get PCA in, figure out what k is and so on. So, that's it for PCA. Despite these last sets of comments, PCA is an incredibly useful algorithm, when you use it for the appropriate applications and I've actually used PCA pretty often and for me, I use it mostly to speed up the running time of my learning algorithms. But I think, just as common an application of PCA, is to use it to compress data, to reduce the memory or disk space requirements, or to use it to visualize data. And PCA is one of the most commonly used and one of the most powerful unsupervised learning algorithms. And with what you've learned in these videos, I think hopefully you'll be able to implement PCA and use them through all of these purposes as well.""",88,0,1
coursera,stanford_university,machine-learning,problem-motivation,"b""In this next set of videos, I'd like to tell you about a problem called Anomaly Detection. This is a reasonably commonly use you type machine learning. And one of the interesting aspects is that it's mainly for unsupervised problem, that there's some aspects of it that are also very similar to sort of the supervised learning problem. So, what is anomaly detection? To explain it. Let me use the motivating example of: Imagine that you're a manufacturer of aircraft engines, and let's say that as your aircraft engines roll off the assembly line, you're doing, you know, QA or quality assurance testing, and as part of that testing you measure features of your aircraft engine, like maybe, you measure the heat generated, things like the vibrations and so on. I share some friends that worked on this problem a long time ago, and these were actually the sorts of features that they were collecting off actual aircraft engines so you now have a data set of X1 through Xm, if you have manufactured m aircraft engines, and if you plot your data, maybe it looks like this. So, each point here, each cross here as one of your unlabeled examples. So, the anomaly detection problem is the following. Let's say that on, you know, the next day, you have a new aircraft engine that rolls off the assembly line and your new aircraft engine has some set of features x-test. What the anomaly detection problem is, we want to know if this aircraft engine is anomalous in any way, in other words, we want to know if, maybe, this engine should undergo further testing because, or if it looks like an okay engine, and so it's okay to just ship it to a customer without further testing. So, if your new aircraft engine looks like a point over there, well, you know, that looks a lot like the aircraft engines we've seen before, and so maybe we'll say that it looks okay. Whereas, if your new aircraft engine, if x-test, you know, were a point that were out here, so that if X1 and X2 are the features of this new example. If x-tests were all the way out there, then we would call that an anomaly. and maybe send that aircraft engine for further testing before we ship it to a customer, since it looks very different than the rest of the aircraft engines we've seen before. More formally in the anomaly detection problem, we're give some data sets, x1 through Xm of examples, and we usually assume that these end examples are normal or non-anomalous examples, and we want an algorithm to tell us if some new example x-test is anomalous. The approach that we're going to take is that given this training set, given the unlabeled training set, we're going to build a model for p of x. In other words, we're going to build a model for the probability of x, where x are these features of, say, aircraft engines. And so, having built a model of the probability of x we're then going to say that for the new aircraft engine, if p of x-test is less than some epsilon then we flag this as an anomaly. So we see a new engine that, you know, has very low probability under a model p of x that we estimate from the data, then we flag this anomaly, whereas if p of x-test is, say, greater than or equal to some small threshold. Then we say that, you know, okay, it looks okay. And so, given the training set, like that plotted here, if you build a model, hopefully you will find that aircraft engines, or hopefully the model p of x will say that points that lie, you know, somewhere in the middle, that's pretty high probability, whereas points a little bit further out have lower probability. Points that are even further out have somewhat lower probability, and the point that's way out here, the point that's way out there, would be an anomaly. Whereas the point that's way in there, right in the middle, this would be okay because p of x right in the middle of that would be very high cause we've seen a lot of points in that region. Here are some examples of applications of anomaly detection. Perhaps the most common application of anomaly detection is actually for detection if you have many users, and if each of your users take different activities, you know maybe on your website or in the physical plant or something, you can compute features of the different users activities. And what you can do is build a model to say, you know, what is the probability of different users behaving different ways. What is the probability of a particular vector of features of a users behavior so you know examples of features of a users activity may be on the website it'd be things like, maybe x1 is how often does this user log in, x2, you know, maybe the number of what pages visited, or the number of transactions, maybe x3 is, you know, the number of posts of the users on the forum, feature x4 could be what is the typing speed of the user and some websites can actually track that was the typing speed of this user in characters per second. And so you can model p of x based on this sort of data. And finally having your model p of x, you can try to identify users that are behaving very strangely on your website by checking which ones have probably effects less than epsilon and maybe send the profiles of those users for further review. Or demand additional identification from those users, or some such to guard against you know, strange behavior or fraudulent behavior on your website. This sort of technique will tend of flag the users that are behaving unusually, not just users that maybe behaving fraudulently. So not just constantly having stolen or users that are trying to do funny things, or just find unusual users. But this is actually the technique that is used by many online websites that sell things to try identify users behaving strangely that might be indicative of either fraudulent behavior or of computer accounts that have been stolen. Another example of anomaly detection is manufacturing. So, already talked about the aircraft engine thing where you can find unusual, say, aircraft engines and send those for further review. A third application would be monitoring computers in a data center. I actually have some friends who work on this too. So if you have a lot of machines in a computer cluster or in a data center, we can do things like compute features at each machine. So maybe some features capturing you know, how much memory used, number of disc accesses, CPU load. As well as more complex features like what is the CPU load on this machine divided by the amount of network traffic on this machine? Then given the dataset of how your computers in your data center usually behave, you can model the probability of x, so you can model the probability of these machines having different amounts of memory use or probability of these machines having different numbers of disc accesses or different CPU loads and so on. And if you ever have a machine whose probability of x, p of x, is very small then you know that machine is behaving unusually and maybe that machine is about to go down, and you can flag that for review by a system administrator. And this is actually being used today by various data centers to watch out for unusual things happening on their machines. So, that's anomaly detection. In the next video, I'll talk a bit about the Gaussian distribution and review properties of the Gaussian probability distribution, and in videos after that, we will apply it to develop an anomaly detection algorithm.""",89,0,1
coursera,stanford_university,machine-learning,gaussian-distribution,"b""In this video, I'd like to talk about\nthe Gaussian distribution which is also called the normal distribution. In case you're already intimately\nfamiliar with the Gaussian distribution, it's probably okay to skip this video,\nbut if you're not sure or if it has been a while since you've\nworked with the Gaussian distribution or normal distribution then please do watch\nthis video all the way to the end. And in the video after this we'll start\napplying the Gaussian distribution to developing an anomaly\ndetection algorithm. Let's say x is a row value's random\nvariable, so x is a row number. If the probability distribution\nof x is Gaussian with mean mu and variance sigma squared. Then, we'll write this as x,\nthe random variable. Tilde, this little tilde,\nthis is distributed as. And then to denote\na Gaussian distribution, sometimes I'm going to write script\nN parentheses mu comma sigma script. So this script N stands for\nnormal since Gaussian and normal they mean the thing are synonyms. And the Gaussian distribution is\nparametarized by two parameters, by a mean parameter which we denote mu and a variance parameter which\nwe denote via sigma squared. If we plot the Gaussian distribution or\nGaussian probability density. It'll look like the bell shaped curve\nwhich you may have seen before. And so this bell shaped curve is paramafied\nby those two parameters, mu and sequel. And the location of the center of this\nbell shaped curve is the mean mu. And the width of this bell shaped curve,\nroughly that, is this parameter, sigma,\nis also called one standard deviation, and so this specifies the probability\nof x taking on different values. So, x taking on values here in\nthe middle here it's pretty high, since the Gaussian density here is pretty\nhigh, whereas x taking on values further, and further away will be\ndiminishing in probability. Finally just for completeness\nlet me write out the formula for the Gaussian distribution. So the probability of x, and\nI'll sometimes write this as the p (x) when we write this as P ( x ; mu,\nsigma squared), and so this denotes that the probability of X is parameterized by\nthe two parameters mu and sigma squared. And the formula for\nthe Gaussian density is this 1/ root 2 pi, sigma e (-(x-mu/g)\nsquared/2 sigma squared. So there's no need to\nmemorize this formula. This is just the formula for the\nbell-shaped curve over here on the left. There's no need to memorize it, and if you ever need to use this,\nyou can always look this up. And so that figure on the left, that is\nwhat you get if you take a fixed value of mu and take a fixed value of sigma,\nand you plot P(x) so this curve here. This is really p(x) plotted\nas a function of X for a fixed value of Mu and of sigma squared. And by the way sometimes it's easier to\nthink in terms of sigma squared that's called the variance. And sometimes is easier to\nthink in terms of sigma. So sigma is called the standard deviation,\nand so it specifies the width of this\nGaussian probability density, where as the square sigma, or\nsigma squared, is called the variance. Let's look at some examples of what\nthe Gaussian distribution looks like. If mu equals zero, sigma equals one. Then we have a Gaussian distribution\nthat's centered around zero, because that's mu and the width of this Gaussian, so that's one\nstandard deviation is sigma over there. Let's look at some examples of Gaussians. If mu is equal to zero and\nsigma equals one, then that corresponds to a Gaussian\ndistribution that is centered at zero, since mu is zero, and\nthe width of this Gaussian is is controlled by sigma by\nthat variance parameter sigma. Here's another example. That same mu is equal to 0 and\nsigma is equal to .5 so the standard deviation is .5 and the\nvariance sigma squared would therefore be the square of 0.5 would be 0.25 and\nin that case the Gaussian distribution, the Gaussian probability\ndensity goes like this. Is also sent as zero. But now the width of this is much\nsmaller because the smaller the area is, the width of this Gaussian\ndensity is roughly half as wide. But because this is a probability\ndistribution, the area under the curve, that's the shaded area there. That area must integrate to one this is\na property of probability distributing. So this is a much taller Gaussian\ndensity because this half is Y but half the standard deviation but\nit twice as tall. Another example is sigma is equal to\n2 then you get a much fatter a much wider Gaussian density and so here the sigma parameter controls that\nGaussian distribution has a wider width. And once again, the area under the curve,\nthat is the shaded area, will always integrate to one, that's the property of\nprobability distributions and because it's wider it's also half as tall in order\nto still integrate to the same thing. And finally one last example would be if\nwe now change the mu parameters as well. Then instead of being centered at 0 we\nnow have a Gaussian distribution that's centered at 3 because this shifts over\nthe entire Gaussian distribution. Next, let's talk about\nthe Parameter estimation problem. So what's the parameter\nestimation problem? Let's say we have a dataset of\nm examples so exponents x m and lets say each of this\nexample is a row number. Here in the figure I've plotted\nan example of the dataset so the horizontal axis is the x axis and\neither will have a range of examples of x, and I've just plotted\nthem on this figure here. And the parameter estimation problem is, let's say I suspect that these examples\ncame from a Gaussian distribution. So let's say I suspect that each of\nmy examples, x i, was distributed. That's what this tilde thing means. Let's not suspect that each of these\nexamples were distributed according to a normal distribution, or Gaussian\ndistribution, with some parameter mu and some parameter sigma square. But I don't know what the values\nof these parameters are. The problem of parameter estimation is,\ngiven my data set, I want to try to figure out, well I want to estimate what\nare the values of mu and sigma squared. So if you're given a data set like this, it looks like maybe if I estimate what\nGaussian distribution the data came from, maybe that might be roughly\nthe Gaussian distribution it came from. With mu being the center of\nthe distribution, sigma standing for the deviation controlling the width\nof this Gaussian distribution. Seems like a reasonable fit to the data. Because, you know, looks like the data\nhas a very high probability of being in the central region, and\na low probability of being further out, even though probability of\nbeing further out, and so on. So maybe this is a reasonable\nestimate of mu and sigma squared. That is, if it corresponds to\na Gaussian distribution function that looks like this. So what I'm going to do is just write out\nthe formula the standard formulas for estimating the parameters Mu and\nsigma squared. Our estimate or\nthe way we're going to estimate mu is going to be just\nthe average of my example. So mu is the mean parameter. Just take my training set,\ntake my m examples and average them. And that just means the center\nof this distribution. How about sigma squared? Well, the variance, I'll just write\nout the standard formula again, I'm going to estimate as sum over one\nthrough m of x i minus mu squared. And so this mu here is actually\nthe mu that I compute over here using this formula. And what the variance is, or one interpretation of the variance\nis that if you look at this term, that's the square difference between the\nvalue I got in my example minus the mean. Minus the center,\nminus the mean of the distribution. And so\nin the variance I'm gonna estimate as just the average of the square differences\nbetween my examples, minus the mean. And as a side comment, only for those\nof you that are experts in statistics. If you're an expert in statistics,\nand if you've heard of maximum likelihood estimation,\nthen these parameters, these estimates, are actually the maximum likelihood\nestimates of the primes of mu and sigma squared but if you haven't heard\nof that before don't worry about it, all you need to know is that these\nare the two standard formulas for how to figure out what are mu and\nSigma squared given the data set. Finally one last side comment again only\nfor those of you that have maybe taken the statistics class before but if you've\ntaken statistics This class before. Some of you may have seen the formula\nhere where this is M-1 instead of M so this first term becomes\n1/M-1 instead of 1/M. In machine learning people tend to learn\n1/M formula but in practice whether it is 1/M or 1/M-1 it makes essentially no\ndifference assuming M is reasonably large. a reasonably large training set size. So just in case you've seen\nthis other version before. In either version it works just about\nequally well but in machine learning most people tend to use 1/M in this\nformula.And the two versions have slightly different theoretical properties\nlike these are different math properties. Bit of practice it really makes makes\nvery little difference, if any. So, hopefully you now have a good sense of\nwhat the Gaussian distribution looks like, as well as how to estimate\nthe parameters mu and sigma squared of Gaussian distribution\nif you're given a training set, that is if you're given a set of data\nthat you suspect comes from a Gaussian distribution with unknown parameters,\nmu and sigma squared. In the next video,\nwe'll start to take this and apply it to develop an anomaly\ndetection algorithm.""",90,0,1
coursera,stanford_university,machine-learning,algorithm,"b""In the last video, we talked about the Gaussian distribution. In this video lets apply that to develop an anomaly detection algorithm. Let's say that we have an unlabeled training set of M examples, and each of these examples is going to be a feature in Rn so your training set could be, feature vectors from the last M aircraft engines being manufactured. Or it could be features from m users or something else. The way we are going to address anomaly detection, is we are going to model p of x from the data sets. We're going to try to figure out what are high probability features, what are lower probability types of features. So, x is a vector and what we are going to do is model p of x, as probability of x1, that is of the first component of x, times the probability of x2, that is the probability of the second feature, times the probability of the third feature, and so on up to the probability of the final feature of Xn. Now I'm leaving space here cause I'll fill in something in a minute. So, how do we model each of these terms, p of X1, p of X2, and so on. What we're going to do, is assume that the feature, X1, is distributed according to a Gaussian distribution, with some mean, which you want to write as mu1 and some variance, which I'm going to write as sigma squared 1, and so p of X1 is going to be a Gaussian probability distribution, with mean mu1 and variance sigma squared 1. And similarly I'm going to assume that X2 is distributed, Gaussian, that's what this little tilda stands for, that means distributed Gaussian with mean mu2 and Sigma squared 2, so it's distributed according to a different Gaussian, which has a different set of parameters, mu2 sigma square 2. And similarly, you know, X3 is yet another Gaussian, so this can have a different mean and a different standard deviation than the other features, and so on, up to XN. And so that's my model. Just as a side comment for those of you that are experts in statistics, it turns out that this equation that I just wrote out actually corresponds to an independence assumption on the values of the features x1 through xn. But in practice it turns out that the algorithm of this fragment, it works just fine, whether or not these features are anywhere close to independent and even if independence assumption doesn't hold true this algorithm works just fine. But in case you don't know those terms I just used independence assumptions and so on, don't worry about it. You'll be able to understand it and implement this algorithm just fine and that comment was really meant only for the experts in statistics. Finally, in order to wrap this up, let me take this expression and write it a little bit more compactly. So, we're going to write this is a product from J equals one through N, of P of XJ parameterized by mu j comma sigma squared j. So this funny symbol here, there is capital Greek alphabet pi, that funny symbol there corresponds to taking the product of a set of values. And so, you're familiar with the summation notation, so the sum from i equals one through n, of i. This means 1 + 2 + 3 plus dot dot dot, up to n. Where as this funny symbol here, this product symbol, right product from i equals 1 through n of i.  Then this means that, it's just like summation except that we're now multiplying. This becomes 1 times 2 times 3 times up to N. And so using this product notation, this product from j equals 1 through n of this expression. It's just more compact, it's just shorter way for writing out this product of of all of these terms up there. Since we're are taking these p of x j given mu j comma sigma squared j terms and multiplying them together. And, by the way the problem of estimating this distribution p of x, they're sometimes called the problem of density estimation. Hence the title of the slide. So putting everything together, here is our anomaly detection algorithm. The first step is to choose features, or come up with features xi that we think might be indicative of anomalous examples. So what I mean by that, is, try to come up with features, so that when there's an unusual user in your system that may be doing fraudulent things, or when the aircraft engine examples, you know there's something funny, something strange about one of the aircraft engines. Choose features X I, that you think might take on unusually large values, or unusually small values, for what an anomalous example might look like. But more generally, just try to choose features that describe general properties of the things that you're collecting data on. Next, given a training set, of M, unlabled examples, X1 through X M, we then fit the parameters, mu 1 through mu n, and sigma squared 1 through sigma squared n, and so these were the formulas similar to the formulas we have in the previous video, that we're going to use the estimate each of these parameters, and just to give some interpretation, mu J, that's my average value of the j feature. Mu j goes in this term p of xj. which is parametrized by mu J and sigma squared J. And so this says for the mu J just take the mean over my training set of the values of the j feature. And, just to mention, that you do this, you compute these formulas for j equals one through n. So use these formulas to estimate mu 1, to estimate mu 2, and so on up to mu n, and similarly for sigma squared, and it's also possible to come up with vectorized versions of these. So if you think of mu as a vector, so mu if is a vector there's mu 1, mu 2, down to mu n, then a vectorized version of that set of parameters can be written like so sum from 1 equals one through n xi. So, this formula that I just wrote out estimates this xi as the feature vectors that estimates mu for all the values of n simultaneously. And it's also possible to come up with a vectorized formula for estimating sigma squared j. Finally, when you're given a new example, so when you have a new aircraft engine and you want to know is this aircraft engine anomalous. What we need to do is then compute p of x, what's the probability of this new example? So, p of x is equal to this product, and what you implement, what you compute, is this formula and where over here, this thing here this is just the formula for the Gaussian probability, so you compute this thing, and finally if this probability is very small, then you flag this thing as an anomaly. Here's an example of an application of this method. Let's say we have this data set plotted on the upper left of this slide. if you look at this, well, lets look the feature of x1. If you look at this data set, it looks like on average, the features x1 has a mean of about 5 and the standard deviation, if you only look at just the x1 values of this data set has the standard deviation of maybe 2. So that sigma 1 and looks like x2 the values of the features as measured on the vertical axis, looks like it has an average value of about 3, and a standard deviation of about 1. So if you take this data set and if you estimate mu1, mu2, sigma1, sigma2, this is what you get. And again, I'm writing sigma here, I'm think about standard deviations, but the formula on the previous 5 actually gave the estimates of the squares of theses things, so sigma squared 1 and sigma squared 2. So, just be careful whether you are using sigma 1, sigma 2, or sigma squared 1 or sigma squared 2. So, sigma squared 1 of course would be equal to 4, for example, as the square of 2. And in pictures what p of x1 parametrized by mu1 and sigma squared 1 and p of x2, parametrized by mu 2 and sigma squared 2, that would look like these two distributions over here. And, turns out that if were to plot of p of x, right, which is the product of these two things, you can actually get a surface plot that looks like this. This is a plot of p of x, where the height above of this, where the height of this surface at a particular point, so given a particular x1 x2 values of x2 if x1 equals 2, x equal 2, that's this point. And the height of this 3-D surface here, that's p of x. So p of x, that is the height of this plot, is literally just p of x1 parametrized by mu 1 sigma squared 1, times p of x2 parametrized by mu 2 sigma squared 2. Now, so this is how we fit the parameters to this data. Let's see if we have a couple of new examples. Maybe I have a new example there. Is this an anomaly or not? Or, maybe I have a different example, maybe I have a different second example over there. So, is that an anomaly or not? They way we do that is, we would set some value for Epsilon, let's say I've chosen Epsilon equals 0.02. I'll say later how we choose Epsilon. But let's take this first example, let me call this example X1 test. And let me call the second example X2 test. What we do is, we then compute p of X1 test, so we use this formula to compute it and this looks like a pretty large value. In particular, this is greater than, or greater than or equal to epsilon. And so this is a pretty high probability at least bigger than epsilon, so we'll say that X1 test is not an anomaly. Whereas, if you compute p of X2 test, well that is just a much smaller value. So this is less than epsilon and so we'll say that that is indeed an anomaly, because it is much smaller than that epsilon that we then chose. And in fact, I'd improve it here. What this is really saying is that, you look through the 3d surface plot. It's saying that all the values of x1 and x2 that have a high height above the surface, corresponds to an a non-anomalous example of an OK or normal example. Whereas all the points far out here, all the points out here, all of those points have very low probability, so we are going to flag those points as anomalous, and so it's gonna define some region, that maybe looks like this, so that everything outside this, it flags as anomalous, whereas the things inside this ellipse I just drew, if it considers okay, or non-anomalous, not anomalous examples. And so this example x2 test lies outside that region, and so it has very small probability, and so we consider it an anomalous example. In this video we talked about how to estimate p of x, the probability of x, for the purpose of developing an anomaly detection algorithm. And in this video, we also stepped through an entire process of giving data set, we have, fitting the parameters, doing parameter estimations. We get mu and sigma parameters, and then taking new examples and deciding if the new examples are anomalous or not. In the next few videos we will delve deeper into this algorithm, and talk a bit more about how to actually get this to work well.""",91,0,1
coursera,stanford_university,machine-learning,developing-and-evaluating-an-anomaly-detection-system,"b""In the last video, we developed an anomaly detection algorithm. In this video, I like to talk about the process of how to go about developing a specific application of anomaly detection to a problem and in particular this will focus on the problem of how to evaluate an anomaly detection algorithm. In previous videos, we've already talked about the importance of real number evaluation and this captures the idea that when you're trying to develop a learning algorithm for a specific application, you need to often make a lot of choices like, you know, choosing what features to use and then so on. And making decisions about all of these choices is often much easier, and if you have a way to evaluate your learning algorithm that just gives you back a number. So if you're trying to decide, you know, I have an idea for one extra feature, do I include this feature or not. If you can run the algorithm with the feature, and run the algorithm without the feature, and just get back a number that tells you, you know, did it improve or worsen performance to add this feature? Then it gives you a much better way, a much simpler way, with which to decide whether or not to include that feature. So in order to be able to develop an anomaly detection system quickly, it would be a really helpful to have a way of evaluating an anomaly detection system. In order to do this, in order to evaluate an anomaly detection system, we're actually going to assume have some labeled data. So, so far, we'll be treating anomaly detection as an unsupervised learning problem, using unlabeled data. But if you have some labeled data that specifies what are some anomalous examples, and what are some non-anomalous examples, then this is how we actually think of as the standard way of evaluating an anomaly detection algorithm. So taking the aircraft engine example again. Let's say that, you know, we have some label data of just a few anomalous examples of some aircraft engines that were manufactured in the past that turns out to be anomalous. Turned out to be flawed or strange in some way. Let's say we use we also have some non-anomalous examples, so some perfectly okay examples. I'm going to use y equals 0 to denote the normal or the non-anomalous example and y equals 1 to denote the anomalous examples. The process of developing and evaluating an anomaly detection algorithm is as follows. We're going to think of it as a training set and talk about the cross validation in test sets later, but the training set we usually think of this as still the unlabeled training set. And so this is our large collection of normal, non-anomalous or not anomalous examples. And usually we think of this as being as non-anomalous, but it's actually okay even if a few anomalies slip into your unlabeled training set. And next we are going to define a cross validation set and a test set, with which to evaluate a particular anomaly detection algorithm. So, specifically, for both the cross validation test sets we're going to assume that, you know, we can include a few examples in the cross validation set and the test set that contain examples that are known to be anomalous. So the test sets say we have a few examples with y equals 1 that correspond to anomalous aircraft engines. So here's a specific example. Let's say that, altogether, this is the data that we have. We have manufactured 10,000 examples of engines that, as far as we know we're perfectly normal, perfectly good aircraft engines. And again, it turns out to be okay even if a few flawed engine slips into the set of 10,000 is actually okay, but we kind of assumed that the vast majority of these 10,000 examples are, you know, good and normal non-anomalous engines. And let's say that, you know, historically, however long we've been running on manufacturing plant, let's say that we end up getting features, getting 24 to 28 anomalous engines as well. And for a pretty typical application of anomaly detection, you know, the number non-anomalous examples, that is with y equals 1, we may have anywhere from, you know, 20 to 50. It would be a pretty typical range of examples, number of examples that we have with y equals 1. And usually we will have a much larger number of good examples. So, given this data set, a fairly typical way to split it into the training set, cross validation set and test set would be as follows. Let's take 10,000 good aircraft engines and put 6,000 of that into the unlabeled training set. So, I'm calling this an unlabeled training set but all of these examples are really ones that correspond to y equals 0, as far as we know. And so, we will use this to fit p of x, right. So, we will use these 6000 engines to fit p of x, which is that p of x one parametrized by Mu 1, sigma squared 1, up to p of Xn parametrized by Mu N sigma squared n. And so it would be these 6,000 examples that we would use to estimate the parameters Mu 1, sigma squared 1, up to Mu N, sigma squared N. And so that's our training set of all, you know, good, or the vast majority of good examples. Next we will take our good aircraft engines and put some number of them in a cross validation set plus some number of them in the test sets. So 6,000 plus 2,000 plus 2,000, that's how we split up our 10,000 good aircraft engines. And then we also have 20 flawed aircraft engines, and we'll take that and maybe split it up, you know, put ten of them in the cross validation set and put ten of them in the test sets. And in the next slide we will talk about how to actually use this to evaluate the anomaly detection algorithm. So what I have just described here is a you know probably the recommend a good way of splitting the labeled and unlabeled example. The good and the flawed aircraft engines. Where we use like a 60, 20, 20% split for the good engines and we take the flawed engines, and we put them just in the cross validation set, and just in the test set, then we'll see in the next slide why that's the case. Just as an aside, if you look at how people apply anomaly detection algorithms, sometimes you see other peoples' split the data differently as well. So, another alternative, this is really not a recommended alternative, but some people want to take off your 10,000 good engines, maybe put 6000 of them in your training set and then put the same 4000 in the cross validation set and the test set. And so, you know, we like to think of the cross validation set and the test set as being completely different data sets to each other. But you know, in anomaly detection, you know, for sometimes you see people, sort of, use the same set of good engines in the cross validation sets, and the test sets, and sometimes you see people use exactly the same sets of anomalous engines in the cross validation set and the test set. And so, all of these are considered, you know, less good practices and definitely less recommended. Certainly using the same data in the cross validation set and the test set, that is not considered a good machine learning practice. But, sometimes you see people do this too. So, given the training cross validation and test sets, here's how you evaluate or here is how you develop and evaluate an algorithm. First, we take the training sets and we fit the model p of x. So, we fit, you know, all these Gaussians to my m unlabeled examples of aircraft engines, and these, I am calling them unlabeled examples, but these are really examples that we're assuming our goods are the normal aircraft engines. Then imagine that your anomaly detection algorithm is actually making prediction. So, on the cross validation of the test set, given that, say, test example X, think of the algorithm as predicting that y is equal to 1, p of x is less than epsilon, we must be taking zero, if p of x is greater than or equal to epsilon. So, given x, it's trying to predict, what is the label, given y equals 1 corresponding to an anomaly or is it y equals 0 corresponding to a normal example? So given the training, cross validation, and test sets. How do you develop an algorithm? And more specifically, how do you evaluate an anomaly detection algorithm? Well, to this whole, the first step is to take the unlabeled training set, and to fit the model p of x lead training data. So you take this, you know on I'm coming, unlabeled training set, but really, these are examples that we are assuming, vast majority of which are normal aircraft engines, not because they're not anomalies and it will fit the model p of x. It will fit all those parameters for all the Gaussians on this data. Next on the cross validation of the test set, we're going to think of the anomaly detention algorithm as trying to predict the value of y. So in each of like say test examples. We have these X-I tests, Y-I test, where y is going to be equal to 1 or 0 depending on whether this was an anomalous example. So given input x in my test set, my anomaly detection algorithm think of it as predicting the y as 1 if p of x is less than epsilon. So predicting that it is an anomaly, it is probably is very low. And we think of the algorithm is predicting that y is equal to 0. If p of x is greater then or equals epsilon. So predicting those normal example if the p of x is reasonably large. And so we can now think of the anomaly detection algorithm as making predictions for what are the values of these y labels in the test sets or on the cross validation set. And this puts us somewhat more similar to the supervised learning setting, right? Where we have label test set and our algorithm is making predictions on these labels and so we can evaluate it you know by seeing how often it gets these labels right. Of course these labels are will be very skewed because y equals zero, that is normal examples, usually be much more common than y equals 1 than anomalous examples. But, you know, this is much closer to the source of evaluation metrics we can use in supervised learning. So what's a good evaluation metric to use. Well, because the data is very skewed, because y equals 0 is much more common, classification accuracy would not be a good the evaluation metrics. So, we talked about this in the earlier video. So, if you have a very skewed data set, then predicting y equals 0 all the time, will have very high classification accuracy. Instead, we should use evaluation metrics, like computing the fraction of true positives, false positives, false negatives, true negatives or compute the position of the v curve of this algorithm or do things like compute the f1 score, right, which is a single real number way of summarizing the position and the recall numbers. And so these would be ways to evaluate an anomaly detection algorithm on your cross validation set or on your test set. Finally, earlier in the anomaly detection algorithm, we also had this parameter epsilon, right? So, epsilon is this threshold that we would use to decide when to flag something as an anomaly. And so, if you have a cross validation set, another way to and to choose this parameter epsilon, would be to try a different, try many different values of epsilon, and then pick the value of epsilon that, let's say, maximizes f1 score, or that otherwise does well on your cross validation set. And more generally, the way to reduce the training, testing, and cross validation sets, is that when we are trying to make decisions, like what features to include, or trying to, you know, tune the parameter epsilon, we would then continually evaluate the algorithm on the cross validation sets and make all those decisions like what features did you use, you know, how to set epsilon, use that, evaluate the algorithm on the cross validation set, and then when we've picked the set of features, when we've found the value of epsilon that we're happy with, we can then take the final model and evaluate it, you know, do the final evaluation of the algorithm on the test sets. So, in this video, we talked about the process of how to evaluate an anomaly detection algorithm, and again, having being able to evaluate an algorithm, you know, with a single real number evaluation, with a number like an F1 score that often allows you to much more efficient use of your time when you are trying to develop an anomaly detection system. And we try to make these sorts of decisions. I have to chose epsilon, what features to include, and so on. In this video, we started to use a bit of labeled data in order to evaluate the anomaly detection algorithm and this takes us a little bit closer to a supervised learning setting. In the next video, I'm going to say a bit more about that. And in particular we'll talk about when should you be using an anomaly detection algorithm and when should we be thinking about using supervised learning instead, and what are the differences between these two formalisms.""",92,0,1
coursera,stanford_university,machine-learning,anomaly-detection-vs-supervised-learning,"b""In the last video we talked\nabout the process of evaluating an anomaly detection algorithm. And there we started to use some\nlabel data with examples that we knew were either anomalous or not anomalous\nwith Y equals one, or Y equals 0. And so, the question then arises of, and\nif we have the label data, that we have some examples and know the anomalies,\nand some of them will not be anomalies. Why don't we just use\na supervisor on half of them? So why don't we just use\nlogistic regression, or a neuro network to try to learn\ndirectly from our labeled data to predict whether Y equals one or\nY equals 0. In this video, I'll try to share with you\nsome of the thinking and some guidelines for when you should probably use\nan anomaly detection algorithm, and whether it might be more fruitful instead\nof using a supervisor in the algorithm. This slide shows what are the settings\nunder which you should maybe use anomaly detection versus when supervised\nlearning might be more fruitful. If you have a problem with a very\nsmall number of positive examples, and remember the examples of y equals\none are the anomaly examples. Then you might consider using\nan anomaly detection algorithm instead. So, having 0 to 20,\nit may be up to 50 positive examples, might be pretty typical. And usually we have such a small positive,\nset of positive examples, we're going to save the positive examples just for\nthe cross validation set in the test set. And in contrast, in a typical\nnormal anomaly detection setting, we will often have a relatively\nlarge number of negative examples of the normal examples of\nnormal aircraft engines. And we can then use this very large\nnumber of negative examples With which to fit the model p(x). And so there's this idea that in\nmany anomaly detection applications, you have very few positive examples and\nlots of negative examples. And when we're doing\nthe process of estimating p(x), affecting all those Gaussian parameters,\nwe need only negative examples to do that. So if you have a lot negative data,\nwe can still fit p(x) pretty well. In contrast, for supervised learning,\nmore typically we would have a reasonably large number of both positive and\nnegative examples. And so this is one way to\nlook at your problem and decide if you should use an anomaly\ndetection algorithm or a supervised. Here's another way that people often\nthink about anomaly detection. So for anomaly detection applications, often there are very\ndifferent types of anomalies. So think about so\nmany different ways for go wrong. There are so many things that could go\nwrong that could the aircraft engine. And so if that's the case, and if you have\na pretty small set of positive examples, then it can be hard for an algorithm,\ndifficult for an algorithm to learn from your small set of positive\nexamples what the anomalies look like. And in particular, you know future anomalies may look\nnothing like the ones you've seen so far. So maybe in your set of positive examples,\nmaybe you've seen 5 or 10 or 20 different ways that an aircraft\nengine could go wrong. But maybe tomorrow,\nyou need to detect a totally new set, a totally new type of anomaly. A totally new way for an aircraft engine to be broken,\nthat you've just never seen before. And if that's the case, it might be more promising to just model\nthe negative examples with this sort of calcium model p of x instead of try to\nhard to model the positive examples. Because tomorrow's anomaly may be nothing\nlike the ones you've seen so far. In contrast, in some other problems,\nyou have enough positive examples for an algorithm to get a sense of what\nthe positive examples are like. In particular, if you think that future\npositive examples are likely to be similar to ones in the training\nset; then in that setting, it might be more reasonable to have a\nsupervisor in the algorithm that looks at all of the positive examples,\nlooks at all of the negative examples, and uses that to try to distinguish\nbetween positives and negatives. Hopefully, this gives you a sense\nof if you have a specific problem, should you think about using\nan anomaly detection algorithm, or a supervised learning algorithm. And a key difference really is that\nin anomaly detection, often we have such a small number of positive\nexamples that it is not possible for a learning algorithm to learn that\nmuch from the positive examples. And so what we do instead is take\na large set of negative examples and have it just learn a lot, learn p(x)\nfrom just the negative examples. Of the normal [INAUDIBLE] and we've reserved the small number of\npositive examples for evaluating our algorithms to use in the either\nthe transvalidation set or the test set. And just as a side comment about\nthis many different types of easier. In some earlier videos we talked\nabout the email spam examples. In those examples, there are actually many\ndifferent types of spam email, right? There's spam email that's\ntrying to sell you things. Spam email trying to steal your passwords,\nthis is called phishing emails and many different types of spam emails. But for the spam problem we usually\nhave enough examples of spam email to see most of these\ndifferent types of spam email because we have a large\nset of examples of spam. And that's why we usually think of\nspam as a supervised learning setting even though there are many\ndifferent types of. If we look at some applications of anomaly\ndetection versus supervised learning we'll find fraud detection. If you have many different types of ways\nfor people to try to commit fraud and a relatively small number of\nfraudulent users on your website, then I use an anomaly detection algorithm. I should say, if you have,\nif you're a very major online retailer and if you actually have had a lot of\npeople commit fraud on your website, so you actually have a lot of\nexamples of y=1, then sometimes fraud detection could actually shift\nover to the supervised learning column. But, if you haven't seen that many\nexamples of users doing strange things on your website, then more frequently\nfraud detection is actually treated as an anomaly detection algorithm rather\nthan a supervised learning algorithm. Other examples,\nwe've talked about manufacturing already. Hopefully, you see more and more\nexamples are not that many anomalies but if again for some manufacturing processes, if you manufacture in very large volumes\nand you see a lot of bad examples, maybe manufacturing can shift to\nthe supervised learning column as well. But if you haven't seen that many bad\nexamples of so to do the anomaly detection monitoring machines in a data center\n[INAUDIBLE] similar source of apply. Whereas, you must have classification, weather prediction, and\nclassifying cancers. If you have equal numbers of positive and\nnegative examples. Your positive and your negative examples, then we would tend to treat all\nof these as supervisor problems. So hopefully, that gives you a sense of one of the properties of a learning\nproblem that would cause you to treat it as an anomaly detection\nproblem versus a supervisory problem. And for many other problems that are faced\nby various technology companies and so on, we actually are in the settings\nwhere we have very few or sometimes zero positive training examples. There's just so many different types of anomalies\nthat we've never seen them before. And for those sorts of problems, very often the algorithm that is used\nis an anomaly detection algorithm.""",93,0,1
coursera,stanford_university,machine-learning,choosing-what-features-to-use,"b""By now you've seen the anomaly detection algorithm and we've also talked about how to evaluate an anomaly detection algorithm. It turns out, that when you're applying anomaly detection, one of the things that has a huge effect on how well it does, is what features you use, and what features you choose, to give the anomaly detection algorithm. So in this video, what I'd like to do is say a few words, give some suggestions and guidelines for how to go about designing or selecting features give to an anomaly detection algorithm. In our anomaly detection algorithm, one of the things we did was model the features using this sort of Gaussian distribution. With xi to mu i, sigma squared i, lets say. And so one thing that I often do would be to plot the data or the histogram of the data, to make sure that the data looks vaguely Gaussian before feeding it to my anomaly detection algorithm. And, it'll usually work okay, even if your data isn't Gaussian, but this is sort of a nice sanitary check to run. And by the way, in case your data looks non-Gaussian, the algorithms will often work just find. But, concretely if I plot the data like this, and if it looks like a histogram like this, and the way to plot a histogram is to use the HIST, or the HIST command in Octave, but it looks like this, this looks vaguely Gaussian, so if my features look like this, I would be pretty happy feeding into my algorithm. But if i were to plot a histogram of my data, and it were to look like this well, this doesn't look at all like a bell shaped curve, this is a very asymmetric distribution, it has a peak way off to one side. If this is what my data looks like, what I'll often do is play with different transformations of the data in order to make it look more Gaussian. And again the algorithm will usually work okay, even if you don't. But if you use these transformations to make your data more gaussian, it might work a bit better. So given the data set that looks like this, what I might do is take a log transformation of the data and if i do that and re-plot the histogram, what I end up with in this particular example, is a histogram that looks like this. And this looks much more Gaussian, right? This looks much more like the classic bell shaped curve, that we can fit with some mean and variance paramater sigma. So what I mean by taking a log transform, is really that if I have some feature x1 and then the histogram of x1 looks like this then I might take my feature x1 and replace it with log of x1 and this is my new x1 that I'll plot to the histogram over on the right, and this looks much more Guassian. Rather than just a log transform some other things you can do, might be, let's say I have a different feature x2, maybe I'll replace that will log x plus 1, or more generally with log x with x2 and some constant c and this constant could be something that I play with, to try to make it look as Gaussian as possible. Or for a different feature x3, maybe I'll replace it with x3, I might take the square root. The square root is just x3 to the power of one half, right? And this one half is another example of a parameter I can play with. So, I might have x4 and maybe I might instead replace that with x4 to the power of something else, maybe to the power of 1/3. And these, all of these, this one, this exponent parameter, or the C parameter, all of these are examples of parameters that you can play with in order to make your data look a little bit more Gaussian. So, let me show you a live demo of how I actually go about playing with my data to make it look more Gaussian. So, I have already loaded in to octave here a set of features x I have a thousand examples loaded over there. So let's pull up the histogram of my data. Use the hist x command. So there's my histogram. By default, I think this uses 10 bins of histograms, but I want to see a more fine grid histogram. So we do hist to the x, 50, so, this plots it in 50 different bins. Okay, that looks better. Now, this doesn't look very Gaussian, does it? So, lets start playing around with the data. Lets try a hist of x to the 0.5. So we take the square root of the data, and plot that histogram. And, okay, it looks a little bit more Gaussian, but not quite there, so let's play at the 0.5 parameter. Let's see. Set this to 0.2. Looks a little bit more Gaussian. Let's reduce a little bit more 0.1. Yeah, that looks pretty good. I could actually just use 0.1. Well, let's reduce it to 0.05. And, you know? Okay, this looks pretty Gaussian, so I can define a new feature which is x mu equals x to the 0.05, and now my new feature x Mu looks more Gaussian than my previous one and then I might instead use this new feature to feed into my anomaly detection algorithm. And of course, there is more than one way to do this. You could also have hist of log of x, that's another example of a transformation you can use. And, you know, that also look pretty Gaussian. So, I can also define x mu equals log of x. and that would be another pretty good choice of a feature to use. So to summarize, if you plot a histogram with the data, and find that it looks pretty non-Gaussian, it's worth playing around a little bit with different transformations like these, to see if you can make your data look a little bit more Gaussian, before you feed it to your learning algorithm, although even if you don't, it might work okay. But I usually do take this step. Now, the second thing I want to talk about is, how do you come up with features for an anomaly detection algorithm. And the way I often do so, is via an error analysis procedure. So what I mean by that, is that this is really similar to the error analysis procedure that we have for supervised learning, where we would train a complete algorithm, and run the algorithm on a cross validation set, and look at the examples it gets wrong, and see if we can come up with extra features to help the algorithm do better on the examples that it got wrong in the cross-validation set. So lets try to reason through an example of this process. In anomaly detection, we are hoping that p of x will be large for the normal examples and it will be small for the anomalous examples. And so a pretty common problem would be if p of x is comparable, maybe both are large for both the normal and the anomalous examples. Lets look at a specific example of that. Let's say that this is my unlabeled data. So, here I have just one feature, x1 and so I'm gonna fit a Gaussian to this. And maybe my Gaussian that I fit to my data looks like that. And now let's say I have an anomalous example, and let's say that my anomalous example takes on an x value of 2.5. So I plot my anomalous example there. And you know, it's kind of buried in the middle of a bunch of normal examples, and so, just this anomalous example that I've drawn in green, it gets a pretty high probability, where it's the height of the blue curve, and the algorithm fails to flag this as an anomalous example. Now, if this were maybe aircraft engine manufacturing or something, what I would do is, I would actually look at my training examples and look at what went wrong with that particular aircraft engine, and see, if looking at that example can inspire me to come up with a new feature x2, that helps to distinguish between this bad example, compared to the rest of my red examples, compared to all of my normal aircraft engines. And if I managed to do so, the hope would be then, that, if I can create a new feature, X2, so that when I re-plot my data, if I take all my normal examples of my training set, hopefully I find that all my training examples are these red crosses here. And hopefully, if I find that for my anomalous example, the feature x2 takes on the the unusual value. So for my green example here, this anomaly, right, my X1 value, is still 2.5. Then maybe my X2 value, hopefully it takes on a very large value like 3.5 over there, or a very small value. But now, if I model my data, I'll find that my anomaly detection algorithm gives high probability to data in the central regions, slightly lower probability to that, sightly lower probability to that. An example that's all the way out there, my algorithm will now give very low probability to. And so, the process of this is, really look at the mistakes that it is making. Look at the anomaly that the algorithm is failing to flag, and see if that inspires you to create some new feature. So find something unusual about that aircraft engine and use that to create a new feature, so that with this new feature it becomes easier to distinguish the anomalies from your good examples. And so that's the process of error analysis and using that to create new features for anomaly detection. Finally, let me share with you my thinking on how I usually go about choosing features for anomaly detection. So, usually, the way I think about choosing features is I want to choose features that will take on either very, very large values, or very, very small values, for examples that I think might turn out to be anomalies. So let's use our example again of monitoring the computers in a data center. And so you have lots of machines, maybe thousands, or tens of thousands of machines in a data center. And we want to know if one of the machines, one of our computers is acting up, so doing something strange. So here are examples of features you may choose, maybe memory used, number of disc accesses, CPU load, network traffic. But now, lets say that I suspect one of the failure cases, let's say that in my data set I think that CPU load the network traffic tend to grow linearly with each other. Maybe I'm running a bunch of web servers, and so, here if one of my servers is serving a lot of users, I have a very high CPU load, and have a very high network traffic. But let's say, I think, let's say I have a suspicion, that one of the failure cases is if one of my computers has a job that gets stuck in some infinite loop. So if I think one of the failure cases, is one of my machines, one of my web servers--server code-- gets stuck in some infinite loop, and so the CPU load grows, but the network traffic doesn't because it's just spinning it's wheels and doing a lot of CPU work, you know, stuck in some infinite loop. In that case, to detect that type of anomaly, I might create a new feature, X5, which might be CPU load divided by network traffic. And so here X5 will take on a unusually large value if one of the machines has a very large CPU load but not that much network traffic and so this will be a feature that will help your anomaly detection capture, a certain type of anomaly. And you can also get creative and come up with other features as well. Like maybe I have a feature x6 thats CPU load squared divided by network traffic. And this would be another variant of a feature like x5 to try to capture anomalies where one of your machines has a very high CPU load, that maybe doesn't have a commensurately large network traffic. And by creating features like these, you can start to capture anomalies that correspond to unusual combinations of values of the features. So in this video we talked about how to and take a feature, and maybe transform it a little bit, so that it becomes a bit more Gaussian, before feeding into an anomaly detection algorithm. And also the error analysis in this process of creating features to try to capture different types of anomalies. And with these sorts of guidelines hopefully that will help you to choose good features, to give to your anomaly detection algorithm, to help it capture all sorts of anomalies.""",94,0,1
coursera,stanford_university,machine-learning,multivariate-gaussian-distribution,"b""In this and the next video, I'd like to tell you about one possible extension to the anomaly detection algorithm that we've developed so far. This extension uses something called the multivariate Gaussian distribution, and it has some advantages, and some disadvantages, and it can sometimes catch some anomalies that the earlier algorithm didn't. To motivate this, let's start with an example. Let's say that so our unlabeled data looks like what I have plotted here. And I'm going to use the example of monitoring machines in the data center, monitoring computers in the data center. So my two features are x1 which is the CPU load and x2 which is maybe the memory use. So if I take my two features, x1 and x2, and I model them as Gaussians then here's a plot of my X1 features, here's a plot of my X2 features, and so if I fit a Gaussian to that, maybe I'll get a Gaussian like this, so here's P of X 1, which depends on the parameters mu 1, and sigma squared 1, and here's my memory used, and, you know, maybe I'll get a Gaussian that looks like this, and this is my P of X 2, which depends on mu 2 and sigma squared 2. And so this is how the anomaly detection algorithm models X1 and X2. Now let's say that in the test sets I have an example that looks like this. The location of that green cross, so the value of X 1 is about 0.4, and the value of X 2 is about 1.5. Now, if you look at the data, it looks like, yeah, most of the data data lies in this region, and so that green cross is pretty far away from any of the data I've seen. It looks like that should be raised as an anomaly. So, in my data, in my, in the data of my good examples, it looks like, you know, the CPU load, and the memory use, they sort of grow linearly with each other. So if I have a machine using lots of CPU, you know memory use will also be high, whereas this example, this green example it looks like here, the CPU load is very low, but the memory use is very high, and I just have not seen that before in my training set. It looks like that should be an anomaly. But let's see what the anomaly detection algorithm will do. Well, for the CPU load, it puts it at around there 0.5 and this reasonably high probability is not that far from other examples we've seen, maybe, whereas, for the memory use, this appointment, 0.5, whereas for the memory use, it's about 1.5, which is there. Again, you know, it's all to us, it's not terribly Gaussian, but the value here and the value here is not that different from many other examples we've seen, and so P of X 1, will be pretty high, reasonably high. P of X 2 reasonably high. I mean, if you look at this plot right, this point here, it doesn't look that bad, and if you look at this plot, you know across here, doesn't look that bad. I mean, I have had examples with even greater memory used, or with even less CPU use, and so this example doesn't look that anomalous. And so, an anomaly detection algorithm will fail to flag this point as an anomaly. And it turns out what our anomaly detection algorithm is doing is that it is not realizing that this blue ellipse shows the high probability region, is that, one of the thing is that, examples here, a high probability, and the examples, the next circle of from a lower probably, and examples here are even lower probability, and somehow, here are things that are, green cross there, it's pretty high probability, and in particular, it tends to think that, you know, everything in this region, everything on the line that I'm circling over, has, you know, about equal probability, and it doesn't realize that something out here actually has much lower probability than something over there. So, in order to fix this, we can, we're going to develop a modified version of the anomaly detection algorithm, using something called the multivariate Gaussian distribution also called the multivariate normal distribution. So here's what we're going to do. We have features x which are in Rn and instead of P of X 1, P of X 2, separately, we're going to model P of X, all in one go, so model P of X, you know, all at the same time. So the parameters of the multivariate Gaussian distribution are mu, which is a vector, and sigma, which is an n by n matrix, called a covariance matrix, and this is similar to the covariance matrix that we saw when we were working with the PCA, with the principal components analysis algorithm. For the second complete is, let me just write out the formula for the multivariate Gaussian distribution. So we say that probability of X, and this is parameterized by my parameters mu and sigma that the probability of x is equal to once again there's absolutely no need to memorize this formula. You know, you can look it up whenever you need to use it, but this is what the probability of X looks like. Transverse, 2nd inverse, X minus mu. And this thing here, the absolute value of sigma, this thing here when you write this symbol, this is called the determent of sigma and this is a mathematical function of a matrix and you really don't need to know what the determinant of a matrix is, but really all you need to know is that you can compute it in octave by using the octave command DET of sigma. Okay, and again, just be clear, alright? In this expression, these sigmas here, these are just n by n matrix. This is not a summation and you know, the sigma there is an n by n matrix. So that's the formula for P of X, but it's more interestingly, or more importantly, what does P of X actually looks like? Lets look at some examples of multivariate Gaussian distributions. So let's take a two dimensional example, say if I have N equals 2, I have two features, X 1 and X 2. Lets say I set MU to be equal to 0 and sigma to be equal to this matrix here. With 1s on the diagonals and 0s on the off-diagonals, this matrix is sometimes also called the identity matrix. In that case, p of x will look like this, and what I'm showing in this figure is, you know, for a specific value of X1 and for a specific value of X2, the height of this surface the value of p of x. And so with this setting the parameters p of x is highest when X1 and X2 equal zero 0, so that's the peak of this Gaussian distribution, and the probability falls off with this sort of two dimensional Gaussian or this bell shaped two dimensional bell-shaped surface. Down below is the same thing but plotted using a contour plot instead, or using different colors, and so this heavy intense red in the middle, corresponds to the highest values, and then the values decrease with the yellow being slightly lower values the cyan being lower values and this deep blue being the lowest values so this is really the same figure but plotted viewed from the top instead, using colors instead. And so, with this distribution, you see that it faces most of the probability near 0,0 and then as you go out from 0,0 the probability of X1 and X2 goes down. Now lets try varying some of the parameters and see what happens. So let's take sigma and change it so let's say sigma shrinks a little bit. Sigma is a covariance matrix and so it measures the variance or the variability of the features X1 X2. So if the shrink sigma then what you get is what you get is that the width of this bump diminishes and the height also increases a bit, because the area under the surface is equal to 1. So the integral of the volume under the surface is equal to 1, because probability distribution must integrate to one. But, if you shrink the variance, it's kinda like shrinking sigma squared, you end up with a narrower distribution, and one that's a little bit taller. And so you see here also the concentric ellipsis has shrunk a little bit. Whereas in contrast if you were to increase sigma to 2 2 on the diagonals, so it is now two times the identity then you end up with a much wider and much flatter Gaussian. And so the width of this is much wider. This is hard to see but this is still a bell shaped bump, it's just flattened down a lot, it has become much wider and so the variance or the variability of X1 and X2 just becomes wider. Here are a few more examples. Now lets try varying one of the elements of sigma at the time. Let's say I send sigma to 0.6 there, and 1 over there. What this does, is this reduces the variance of the first feature, X 1, while keeping the variance of the second feature X 2, the same. And so with this setting of parameters, you can model things like that. X 1 has smaller variance, and X 2 has larger variance. Whereas if I do this, if I set this matrix to 2, 1 then you can also model examples where you know here we'll say X1 can have take on a large range of values whereas X2 takes on a relatively narrower range of values. And that's reflected in this figure as well, you know where, the distribution falls off more slowly as X 1 moves away from 0, and falls off very rapidly as X 2 moves away from 0. And similarly if we were to modify this element of the matrix instead, then similar to the previous slide, except that here where you know playing around here saying that X2 can take on a very small range of values and so here if this is 0.6, we notice now X2 tends to take on a much smaller range of values than the original example, whereas if we were to set sigma to be equal to 2 then that's like saying X2 you know, has a much larger range of values. Now, one of the cool things about the multivariate Gaussian distribution is that you can also use it to model correlations between the data. That is we can use it to model the fact that X1 and X2 tend to be highly correlated with each other for example. So specifically if you start to change the off diagonal entries of this covariance matrix you can get a different type of Gaussian distribution. And so as I increase the off-diagonal entries from .5 to .8, what I get is this distribution that is more and more thinly peaked along this sort of x equals y line. And so here the contour says that x and y tend to grow together and the things that are with large probability are if either X1 is large and Y2 is large or X1 is small and Y2 is small. Or somewhere in between. And as this entry, 0.8 gets large, you get a Gaussian distribution, that's sort of where all the probability lies on this sort of narrow region, where x is approximately equal to y. This is a very tall, thin distribution you know line mostly along this line central region where x is close to y. So this is if we set these entries to be positive entries. In contrast if we set these to negative values, as I decreases it to -.5 down to -.8, then what we get is a model where we put most of the probability in this sort of negative X one in the next 2 correlation region, and so, most of the probability now lies in this region, where X 1 is about equal to -X 2, rather than X 1 equals X 2. And so this captures a sort of negative correlation between x1 and x2. And so this is a hopefully this gives you a sense of the different distributions that the multivariate Gaussian distribution can capture. So follow up in varying, the covariance matrix sigma, the other thing you can do is also, vary the mean parameter mu, and so operationally, we have mu equal 0 0, and so the distribution was centered around X 1 equals 0, X2 equals 0, so the peak of the distribution is here, whereas, if we vary the values of mu, then that varies the peak of the distribution and so, if mu equals 0, 0.5, the peak is at, you know, X1 equals zero, and X2 equals 0.5, and so the peak or the center of this distribution has shifted, and if mu was 1.5 minus 0.5 then OK, and similarly the peak of the distribution has now shifted to a different location, corresponding to where, you know, X1 is 1.5 and X2 is -0.5, and so varying the mu parameter, just shifts around the center of this whole distribution. So, hopefully, looking at all these different pictures gives you a sense of the sort of probability distributions that the Multivariate Gaussian Distribution allows you to capture. And the key advantage of it is it allows you to capture, when you'd expect two different features to be positively correlated, or maybe negatively correlated. In the next video, we'll take this multivariate Gaussian distribution and apply it to anomaly detection.""",95,0,1
coursera,stanford_university,machine-learning,anomaly-detection-using-the-multivariate-gaussian-distribution,"b""In the last video we talked about the Multivariate Gaussian Distribution and saw some examples of the sorts of distributions you can model, as you vary the parameters, mu and sigma. In this video, let's take those ideas, and apply them to develop a different anomaly detection algorithm. To recap the multivariate Gaussian distribution and the multivariate normal distribution has two parameters, mu and sigma. Where mu this an n dimensional vector and sigma, the covariance matrix, is an n by n matrix. And here's the formula for the probability of X, as parameterized by mu and sigma, and as you vary mu and sigma, you can get a range of different distributions, like, you know, these are three examples of the ones that we saw in the previous video. So let's talk about the parameter fitting or the parameter estimation problem. The question, as usual, is if I have a set of examples X1 through XM and here each of these examples is an n dimensional vector and I think my examples come from a multivariate Gaussian distribution. How do I try to estimate my parameters mu and sigma? Well the standard formulas for estimating them is you set mu to be just the average of your training examples. And you set sigma to be equal to this. And this is actually just like the sigma that we had written out, when we were using the PCA or the Principal Components Analysis algorithm. So you just plug in these two formulas and this would give you your estimated parameter mu and your estimated parameter sigma. So given the data set here is how you estimate mu and sigma. Let's take this method and just plug it into an anomaly detection algorithm. So how do we put all of this together to develop an anomaly detection algorithm? Here 's what we do. First we take our training set, and we fit the model, we fit P of X, by, you know, setting mu and sigma as described on the previous slide. Next when you are given a new example X. So if you are given a test example, lets take an earlier example to have a new example out here. And that is my test example. Given the new example X, what we are going to do is compute P of X, using this formula for the multivariate Gaussian distribution. And then, if P of X is very small, then we flagged it as an anomaly, whereas, if P of X is greater than that parameter epsilon, then we don't flag it as an anomaly. So it turns out, if we were to fit a multivariate Gaussian distribution to this data set, so just the red crosses, not the green example, you end up with a Gaussian distribution that places lots of probability in the central region, slightly less probability here, slightly less probability here, slightly less probability here, and very low probability at the point that is way out here. And so, if you apply the multivariate Gaussian distribution to this example, it will actually correctly flag that example. as an anomaly. Finally it's worth saying a few words about what is the relationship between the multivariate Gaussian distribution model, and the original model, where we were modeling P of X as a product of this P of X1, P of X2, up to P of Xn. It turns out that you can prove mathematically, I'm not going to do the proof here, but you can prove mathematically that this relationship, between the multivariate Gaussian model and this original one. And in particular, it turns out that the original model corresponds to multivariate Gaussians, where the contours of the Gaussian are always axis aligned. So all three of these are examples of Gaussian distributions that you can fit using the original model. It turns out that that corresponds to multivariate Gaussian, where, you know, the ellipsis here, the contours of this distribution--it turns out that this model actually corresponds to a special case of a multivariate Gaussian distribution. And in particular, this special case is defined by constraining the distribution of p of x, the multivariate a Gaussian distribution of p of x, so that the contours of the probability density function, of the probability distribution function, are axis aligned. And so you can get a p of x with a multivariate Gaussian that looks like this, or like this, or like this. And you notice, that in all 3 of these examples, these ellipses, or these ovals that I'm drawing, have their axes aligned with the X1 X2 axes. And what we do not have, is a set of contours that are at an angle, right? And this corresponded to examples where sigma is equal to 1 1, 0.8, 0.8. Let's say, with non-0 elements on the off diagonals. So, it turns out that it's possible to show mathematically that this model actually is the same as a multivariate Gaussian distribution but with a constraint. And the constraint is that the covariance matrix sigma must have 0's on the off diagonal elements. In particular, the covariance matrix sigma, this thing here, it would be sigma squared 1, sigma squared 2, down to sigma squared n, and then everything on the off diagonal entries, all of these elements above and below the diagonal of the matrix, all of those are going to be zero. And in fact if you take these values of sigma, sigma squared 1, sigma squared 2, down to sigma squared n, and plug them into here, and you know, plug them into this covariance matrix, then the two models are actually identical. That is, this new model, using a multivariate Gaussian distribution, corresponds exactly to the old model, if the covariance matrix sigma, has only 0 elements off the diagonals, and in pictures that corresponds to having Gaussian distributions, where the contours of this distribution function are axis aligned. So you aren't allowed to model the correlations between the diffrent features. So in that sense the original model is actually a special case of this multivariate Gaussian model. So when would you use each of these two models? So when would you the original model and when would you use the multivariate Gaussian model? The original model is probably used somewhat more often, and whereas the multivariate Gaussian distribution is used somewhat less but it has the advantage of being able to capture correlations between features. So suppose you want to capture anomalies where you have different features say where features x1, x2 take on unusual combinations of values so in the earlier example, we had that example where the anomaly was with the CPU load and the memory use taking on unusual combinations of values, if you want to use the original model to capture that, then what you need to do is create an extra feature, such as X3 equals X1/X2, you know equals maybe the CPU load divided by the memory used, or something, and you need to create extra features if there's unusual combinations of values where X1 and X2 take on an unusual combination of values even though X1 by itself and X2 by itself looks like it's taking a perfectly normal value. But if you're willing to spend the time to manually create an extra feature like this, then the original model will work fine. \nWhereas in contrast, the multivariate Gaussian model can automatically capture correlations between different features. But the original model has some other more significant advantages, too, and one huge advantage of the original model is that it is computationally cheaper, and another view on this is that is scales better to very large values of n and very large numbers of features, and so even if n were ten thousand, or even if n were equal to a hundred thousand, the original model will usually work just fine. Whereas in contrast for the multivariate Gaussian model notice here, for example, that we need to compute the inverse of the matrix sigma where sigma is an n by n matrix and so computing sigma if sigma is a hundred thousand by a hundred thousand matrix that is going to be very computationally expensive. And so the multivariate Gaussian model scales less well to large values of N. And finally for the original model, it turns out to work out ok even if you have a relatively small training set this is the small unlabeled examples that we use to model p of x of course, and this works fine, even if M is, you know, maybe 50, 100, works fine. Whereas for the multivariate Gaussian, it is sort of a mathematical property of the algorithm that you must have m greater than n, so that the number of examples is greater than the number of features you have. And there's a mathematical property of the way we estimate the parameters that if this is not true, so if m is less than or equal to n, then this matrix isn't even invertible, that is this matrix is singular, and so you can't even use the multivariate Gaussian model unless you make some changes to it. But a typical rule of thumb that I use is, I will use the multivariate Gaussian model only if m is much greater than n, so this is sort of the narrow mathematical requirement, but in practice, I would use the multivariate Gaussian model, only if m were quite a bit bigger than n. \nSo if m were greater than or equal to 10 times n, let's say, might be a reasonable rule of thumb, and if it doesn't satisfy this, then the multivariate Gaussian model has a lot of parameters, right, so this covariance matrix sigma is an n by n matrix, so it has, you know, roughly n squared parameters, because it's a symmetric matrix, it's actually closer to n squared over 2 parameters, but this is a lot of parameters, so you need make sure you have a fairly large value for m, make sure you have enough data to fit all these parameters. And m greater than or equal to 10 n would be a reasonable rule of thumb to make sure that you can estimate this covariance matrix sigma reasonably well. So in practice the original model shown on the left that is used more often. And if you suspect that you need to capture correlations between features what people will often do is just manually design extra features like these to capture specific unusual combinations of values. But in problems where you have a very large training set or m is very large and n is not too large, then the multivariate Gaussian model is well worth considering and may work better as well, and can save you from having to spend your time to manually create extra features in case the anomalies turn out to be captured by unusual combinations of values of the features. Finally I just want to briefly mention one somewhat technical property, but if you're fitting multivariate Gaussian model, and if you find that the covariance matrix sigma is singular, or you find it's non-invertible, they're usually 2 cases for this. One is if it's failing to satisfy this m greater than n condition, and the second case is if you have redundant features. So by redundant features, I mean, if you have 2 features that are the same. Somehow you accidentally made two copies of the feature, so your x1 is just equal to x2. Or if you have redundant features like maybe your features X3 is equal to feature X4, plus feature X5. Okay, so if you have highly redundant features like these, you know, where if X3 is equal to X4 plus X5, well X3 doesn't contain any extra information, right? You just take these 2 other features, and add them together. And if you have this sort of redundant features, duplicated features, or this sort of features, than sigma may be non-invertible. And so there's a debugging set-- this should very rarely happen, so you probably won't run into this, it is very unlikely that you have to worry about this-- but in case you implement a multivariate Gaussian model you find that sigma is non-invertible. What I would do is first make sure that M is quite a bit bigger than N, and if it is then, the second thing I do, is just check for redundant features. And so if there are 2 features that are equal, just get rid of one of them, or if you have redundant if these , X3 equals X4 plus X5, just get rid of the redundant feature, and then it should work fine again. As an aside for those of you who are experts in linear algebra, by redundant features, what I mean is the formal term is features that are linearly dependent. But in practice what that really means is one of these problems tripping up the algorithm if you just make you features non-redundant., that should solve the problem of sigma being non-invertable. But once again the odds of your running into this at all are pretty low so chances are, you can just apply the multivariate Gaussian model, without having to worry about sigma being non-invertible, so long as m is greater than or equal to n.\nSo that's it for anomaly detection, with the multivariate Gaussian distribution. And if you apply this method you would be able to have an anomaly detection algorithm that automatically captures positive and negative correlations between your different features and flags an anomaly if it sees is unusual combination of the values of the features.""",96,0,1
coursera,stanford_university,machine-learning,problem-formulation,"b""In this next set of videos, I would like to tell you about recommender systems. There are two reasons, I had two motivations for why I wanted to talk about recommender systems. The first is just that it is an important application of machine learning. Over the last few years, occasionally I visit different, you know, technology companies here in Silicon Valley and I often talk to people working on machine learning applications there and so I've asked people what are the most important applications of machine learning or what are the machine learning applications that you would most like to get an improvement in the performance of. And one of the most frequent answers I heard was that there are many groups out in Silicon Valley now, trying to build better recommender systems. So, if you think about what the websites are like Amazon, or what Netflix or what eBay, or what iTunes Genius, made by Apple does, there are many websites or systems that try to recommend new products to use. So, Amazon recommends new books to you, Netflix try to recommend new movies to you, and so on. And these sorts of recommender systems, that look at what books you may have purchased in the past, or what movies you have rated in the past, but these are the systems that are responsible for today, a substantial fraction of Amazon's revenue and for a company like Netflix, the recommendations that they make to the users is also responsible for a substantial fraction of the movies watched by their users. \nAnd so an improvement in performance of a recommender system can have a substantial and immediate impact on the bottom line of many of these companies. Recommender systems is kind of a funny problem, within academic machine learning so that we could go to an academic machine learning conference, the problem of recommender systems, actually receives relatively little attention, or at least it's sort of a smaller fraction of what goes on within Academia. But if you look at what's happening, many technology companies, the ability to build these systems seems to be a high priority for many companies. And that's one of the reasons why I want to talk about them in this class. The second reason that I want to talk about recommender systems is that as we approach the last few sets of videos of this class I wanted to talk about a few of the big ideas in machine learning and share with you, you know, some of the big ideas in machine learning. And we've already seen in this class that features are important for machine learning, the features you choose will have a big effect on the performance of your learning algorithm. So there's this big idea in machine learning, which is that for some problems, maybe not all problems, but some problems, there are algorithms that can try to automatically learn a good set of features for you. So rather than trying to hand design, or hand code the features, which is mostly what we've been doing so far, there are a few settings where you might be able to have an algorithm, just to learn what feature to use, and the recommender systems is just one example of that sort of setting. There are many others, but engraved through recommender systems, will be able to go a little bit into this idea of learning the features and you'll be able to see at least one example of this, I think, big idea in machine learning as well. So, without further ado, let's get started, and talk about the recommender system problem formulation. As my running example, I'm going to use the modern problem of predicting movie ratings. So, here's a problem. Imagine that you're a website or a company that sells or rents out movies, or what have you. And so, you know, Amazon, and Netflix, and I think iTunes are all examples of companies that do this, and let's say you let your users rate different movies, using a 1 to 5 star rating. So, users may, you know, something one, two, three, four or five stars. In order to make this example just a little bit nicer, I'm going to allow 0 to 5 stars as well, because that just makes some of the math come out just nicer. Although most of these websites use the 1 to 5 star scale. So here, I have 5 movies. You know, Love That Lasts, Romance Forever, Cute Puppies of Love, Nonstop Car Chases, and Swords vs. Karate. And we have 4 users, which, calling, you know, Alice, Bob, Carol, and Dave, with initials A, B, C, and D, we'll call them users 1, 2, 3, and 4. So, let's say Alice really likes Love That Lasts and rates that 5 stars, likes Romance Forever, rates it 5 stars. She did not watch Cute Puppies of Love, and did rate it, so we don't have a rating for that, and Alice really did not like Nonstop Car Chases or Swords vs. Karate. And a different user Bob, user two, maybe rated a different set of movies, maybe she likes to Love at Last, did not to watch Romance Forever, just have a rating of 4, a 0, a 0, and maybe our 3rd user, rates this 0, did not watch that one, 0, 5, 5, and, you know, let's just fill in some of the numbers. And so just to introduce a bit of notation, this notation that we'll be using throughout, I'm going to use NU to denote the number of users. So in this example, NU will be equal to 4. So the u-subscript stands for users and Nm, going to use to denote the number of movies, so here I have five movies so Nm equals equals 5. And you know for this example, I have for this example, I have loosely 3 maybe romantic or romantic comedy movies and 2 action movies and you know, if you look at this small example, it looks like Alice and Bob are giving high ratings to these romantic comedies or movies about love, and giving very low ratings about the action movies, and for Carol and Dave, it's the opposite, right? Carol and Dave, users three and four, really like the action movies and give them high ratings, but don't like the romance and love- type movies as much. Specifically, in the recommender system problem, we are given the following data. Our data comprises the following: we have these values r(i, j), and r(i, j) is 1 if user J has rated movie I. So our users rate only some of the movies, and so, you know, we don't have ratings for those movies. And whenever r(i, j) is equal to 1, whenever user j has rated movie i, we also get this number y(i, j), which is the rating given by user j to movie i. And so, y(i, j) would be a number from zero to five, depending on the star rating, zero to five stars that user gave that particular movie. So, the recommender system problem is given this data that has give these r(i, j)'s and the y(i, j)'s to look through the data and look at all the movie ratings that are missing and to try to predict what these values of the question marks should be. In the particular example, I have a very small number of movies and a very small number of users and so most users have rated most movies but in the realistic settings your users each of your users may have rated only a minuscule fraction of your movies but looking at this data, you know, if Alice and Bob both like the romantic movies maybe we think that Alice would have given this a five. Maybe we think Bob would have given this a 4.5 or some high value, as we think maybe Carol and Dave were doing these very low ratings. And Dave, well, if Dave really likes action movies, maybe he would have given Swords and Karate a 4 rating or maybe a 5 rating, okay? And so, our job in developing a recommender system is to come up with a learning algorithm that can automatically go fill in these missing values for us so that we can look at, say, the movies that the user has not yet watched, and recommend new movies to that user to watch. You try to predict what else might be interesting to a user. So that's the formalism of the recommender system problem. In the next video we'll start to develop a learning algorithm to address this problem.""",97,0,1
coursera,stanford_university,machine-learning,content-based-recommendations,"b""In the last video, we talked about\nthe recommender systems problem where for example you might have a set of movies and\nyou may have a set of users, each who have rated some\nsubset of the movies. They've rated the movies one to\nfive stars or zero to five stars. And what we would like to do\nis look at these users and predict how they would have rated other\nmovies that they have not yet rated. In this video I'd like to talk about\nour first approach to building a recommender system. This approach is called\ncontent based recommendations. Here's our data set from before and\njust to remind you of a bit of notation, I was using nu to denote the number\nof users and so that's equal to 4, and nm to denote the number of movies,\nI have 5 movies. So, how do I predict what\nthese missing values would be? Let's suppose that for each of these\nmovies I have a set of features for them. In particular, let's say that for each of the movies have two features\nwhich I'm going to denote x1 and x2. Where x1 measures the degree to which\na movie is a romantic movie and x2 measures the degree to which\na movie is an action movie. So, if you take a movie, Love at last, you\nknow it's 0.9 rating on the romance scale. This is a highly romantic movie,\nbut zero on the action scale. So, almost no action in that movie. Romance forever is a 1.0,\nlot of romance and 0.01 action. I don't know, maybe there's a minor\ncar crash in that movie or something. So there's a little bit of action. Skipping one, let's do Swords vs karate,\nmaybe that has a 0 romance rating and no romance at all in that but\nplenty of action. And Nonstop car chases, maybe again there's a tiny bit of\nromance in that movie but mainly action. And Cute puppies of love mainly\na romance movie with no action at all. So if we have features like these, then each movie can be represented\nwith a feature vector. Let's take movie one. So let's call these movies 1,\n2, 3, 4, and 5. But my first movie, Love at last,\nI have my two features, 0.9 and 0. And so these are features x1 and x2. And let's add an extra feature as usual,\nwhich is my interceptor feature x0 = 1. And so putting these together I\nwould then have a feature x1. The superscript 1 denotes it's the feature\nvector for my first movie, and this feature vector is equal to 1. The first 1 there is this interceptor. And then my two feature is 0.90 like so. So for Love at last I would\nhave a feature vector x1, for the movie Romance forever I may have a\nsoftware feature of vector x2, and so on, and for Swords vs karate I would have a\ndifferent feature vector x superscript 5. Also, consistence with our earlier\nnode notation that we were using, we're going to set n to be the number of\nfeatures not counting this x0 interceptor. So n is equal to 2 because it's\nwe have two features x1 and x2 capturing the degree of romance and\nthe degree of action in each movie. Now in order to make predictions here's\none thing that we do which is that we could treat predicting\nthe ratings of each user as a separate linear regression problem. So specifically, let's say that for\neach user j, we're going to learn the parameter vector theta j,\nwhich would be an R3 in this case. More generally,\ntheta (j) would be an R (n+1), where n is the number of features\nnot counting the set term. And we're going to predict user j as\nrating movie i with just the inner product between parameters vectors\ntheta and the features xi. So let's take a specific example. Let's take user 1, so that would be Alice. And associated with Alice would\nbe some parameter vector theta 1. And our second user, Bob, will be associated a different\nparameter vector theta 2. Carol will be associated with\na different parameter vector theta 3 and Dave a different parameter vector theta 4. So let's say you want to\nmake a prediction for what Alice will think of\nthe movie Cute puppies of love. Well that movie is going to\nhave some parameter vector x3 where we have that x3 is\ngoing to be equal to 1, which is my intercept term and\nthen 0.99 and then 0. And let's say, for this example, let's\nsay that we've somehow already gotten a parameter vector theta 1 for Alice. We'll say it later exactly how we\ncome up with this parameter vector. But let's just say for now that\nsome unspecified learning algorithm has learned the parameter vector\ntheta 1 and is equal to this 0,5,0. So our prediction for\nthis entry is going to be equal to theta 1, that is Alice's parameter vector,\ntranspose x3, that is the feature vector for\nthe Cute puppies of love movie, number 3. And so the inner product between these\ntwo vectors is gonna be 5 times 0.99, which is equal to 4.95. And so my prediction for\nthis value over here is going to be 4.95. And maybe that seems like a reasonable\nvalue if indeed this is my parameter vector theta 1. So, all we're doing here is we're\napplying a different copy of this linear regression for each user, and\nwe're saying that what Alice does is Alice has some parameter vector theta 1 that\nshe uses, that we use to predict her ratings as a function of how romantic and\nhow action packed a movie is. And Bob and Carol and Dave, each of\nthem have a different linear function of the romanticness and actionness, or degree\nof romance and degree of action in a movie and that that's how we're gonna\npredict that their star ratings. More formally,\nhere's how we can write down the problem. Our notation is that r(i,j) is equal\nto 1 if user j has rated movie i and y(i,j) is the rating of that movie,\nif that rating exists. That is, if that user has\nactually rated that movie. And, on the previous slide we also defined\nthese, theta j, which is a parameter for the user xi, which is a feature vector for\na specific movie. And for each user and each movie,\nwe predict that rating as follows. So let me introduce just temporarily\nintroduce one extra bit of notation mj. We're gonna use mj to denote\nthe number of users rated by movie j. We don't need this notation only for\nthis line. Now in order to learn the parameter\nvector for theta j, well how do we do so. This is basically a linear\nregression problem. So what we can do is just choose\na parameter vector theta j so that the predicted values here are as\nclose as possible to the values that we observed in our training sets and\nthe values we observed in our data. So let's write that down. In order to learn\nthe parameter vector theta j, let's minimize over the parameter\nvector theta j of sum, and I want to sum over all\nmovies that user j has rated. So we write it as sum\nover all values of i. That's a :r(i,j) equals 1. So the way to read this summation\nsyntax is this is summation over all the values of i, so\nthe r(i.j) is equal to 1. So you'll be summing over all\nthe movies that user j has rated. And then I'm going to compute theta j,\ntranspose x i. So that's the prediction of using\nj's rating on movie i,- y (i,j). So that's the actual\nobserved rating squared. And then, let me just divide by the number\nof movies that user j has actually rated. So let's just divide by 1 over 2m j. And so this is just like\nthe least squares regressions. It's just like linear regression, where\nwe want to choose the parameter vector theta j to minimize this\ntype of squared error term. And if you want, you can also add in\nirregularization terms so plus lambda over 2m and this is really 2mj\nbecause we have mj examples. User j has rated that many movies,\nit's not like we have that many data points with which to fit\nthe parameters of theta j. And then let me add in my\nusual regularization term here of theta j k squared. As usual, this sum is from k equals 1\nthrough n, so here, theta j is going to be an n plus 1 dimensional vector, where\nin our early example n was equal to 2. But more broadly, more generally n is\nthe number of features we have per movie. And so\nas usual we don't regularize over theta 0. We don't regularize over the bias terms. The sum is from k equals 1 through n. So if you minimize this as a function\nof theta j you get a good solution, you get a pretty good estimate\nof a parameter vector theta j with which to make predictions for\nuser j's movie ratings. For recommender systems, I'm gonna\nchange this notation a little bit. So to simplify the subsequent math,\nI with to get rid of this term mj. So that's just a constant, right? So I can delete it without changing\nthe value of theta j that I get out of this optimization. So if you imagine taking this whole\nequation, taking this whole expression and multiplying it by mj,\nget rid of that constant. And when I minimize this, I should still\nget the same value of theta j as before. So just to repeat what we\nwrote on the previous slide, here's our optimization objective. In order to learn theta j\nwhich is the parameter for user j, we're going to minimize over\ntheta j of this optimization objectives. So this is our usual squared error term\nand then this is our regularizations term. Now of course in building\na recommender system, we don't just want to learn parameters for\na single user. We want to learn parameters for\nall of our users. I have n subscript u users, so\nI want to learn all of these parameters. And so, what I'm going to do is take\nthis optimization objective and just add the mixture summation there. So this expression here with the one\nhalf on top of this is exactly the same as what we had on top. Except that now instead of just doing\nthis for a specific user theta j, I'm going to sum my objective\nover all of my users and then minimize this overall optimization\nobjective, minimize this overall cost on. And when I minimize this as\na function of theta 1, theta 2, up to theta nu, I will get a separate\nparameter vector for each user. And I can then use that to make\npredictions for all of my users, for all of my n subscript users. So putting everything together, this\nwas our optimization objective on top. And to give this thing a name, I'll\njust call this J(theta1, ..., theta nu). So j as usual is my optimization\nobjective, which I'm trying to minimize. Next, in order to actually do\nthe minimization, if you were to derive the gradient descent update, these\nare the equations that you would get. So you take theta j, k, and\nsubtract from an alpha, which is the learning rate,\ntimes these terms over here on the right. So there's slightly different cases when\nk equals 0 and when k does not equal 0. Because our regularization term here\nregularizes only the values of theta jk for k not equal to 0, so\nwe don't regularize theta 0, so with slightly different updates when\nk equals 0 and k is not equal to 0. And this term over here, for example, is just the partial derivative\nwith respect to your parameter, that of your optimization objective. Right and so\nthis is just gradient descent and I've already computed the derivatives and\nplugged them into here. And if this gradient descent update\nlook a lot like what we have here for linear regression. That's because these are essentially\nthe same as linear regression. The only minor difference is that for\nlinear regression we have these 1 over m terms,\nthis really would've been 1 over mj. But because earlier when we are deriving\nthe optimization objective, we got rid of this, that's why we\ndon't have this 1 over m term. But otherwise, it's really some of\nmy training examples of the ever times xk plus that regularization term, plus that term of regularization\ncontributes to the derivative. And so if you're using gradient\ndescent here's how you can minimize the cost function j\nto learn all the parameters. And using these formulas for\nthe derivative if you want, you can also plug them into a more\nadvanced optimization algorithm, like conjugate gradient or\nLBFGS or what have you. And use that to try to minimize\nthe cost function j as well. So hopefully you now know how you can\napply essentially a deviation on linear regression in order to predict different\nmovie ratings by different users. This particular algorithm is called\na content based recommendations, or a content based approach, because we assume that we have available\nto us features for the different movies. And so where features that capture\nwhat is the content of these movies, of how romantic is this movie,\nhow much action is in this movie. And we're really using features\nof a content of the movies to make our predictions. But for many movies,\nwe don't actually have such features. Or maybe very difficult\nto get such features for all of our movies, for all of\nwhatever items we're trying to sell. And so, in the next video, we'll start\nto talk about an approach to recommender systems that isn't content based and\ndoes not assume that we have someone else giving us all of these features for\nall of the movies in our data set.""",98,0,1
coursera,stanford_university,machine-learning,collaborative-filtering,"b""In this video we'll talk about an approach to building a recommender system that's called collaborative filtering. The algorithm that we're talking about has a very interesting property that it does what is called feature learning and by that I mean that this will be an algorithm that can start to learn for itself what features to use. Here was the data set that we had and we had assumed that for each movie, someone had come and told us how romantic that movie was and how much action there was in that movie. But as you can imagine it can be very difficult and time consuming and expensive to actually try to get someone to, you know, watch each movie and tell you how romantic each movie and how action packed is each movie, and often you'll want even more features than just these two. So where do you get these features from? So let's change the problem a bit and suppose that we have a data set where we do not know the values of these features. So we're given the data set of movies and of how the users rated them, but we have no idea how romantic each movie is and we have no idea how action packed each movie is so I've replaced all of these things with question marks. But now let's make a slightly different assumption. Let's say we've gone to each of our users, and each of our users has told has told us how much they like the romantic movies and how much they like action packed movies. So Alice has associated a current of theta 1. Bob theta 2. Carol theta 3. Dave theta 4. And let's say we also use this and that Alice tells us that she really likes romantic movies and so there's a five there which is the multiplier associated with X1 and lets say that Alice tells us she really doesn't like action movies and so there's a 0 there. And Bob tells us something similar so we have theta 2 over here. Whereas Carol tells us that she really likes action movies which is why there's a 5 there, that's the multiplier associated with X2, and remember there's also X0 equals 1 and let's say that Carol tells us she doesn't like romantic movies and so on, similarly for Dave. So let's assume that somehow we can go to users and each user J just tells us what is the value of theta J for them. And so basically specifies to us of how much they like different types of movies. If we can get these parameters theta from our users then it turns out that it becomes possible to try to infer what are the values of x1 and x2 for each movie. Let's look at an example. Let's look at movie 1. So that movie 1 has associated with it a feature vector x1. And you know this movie is called Love at last but let's ignore that. Let's pretend we don't know what this movie is about, so let's ignore the title of this movie. All we know is that Alice loved this move. Bob loved this movie. Carol and Dave hated this movie. So what can we infer? Well, we know from the feature vectors that Alice and Bob love romantic movies because they told us that there's a 5 here. Whereas Carol and Dave, we know that they hate romantic movies and that they love action movies. So because those are the parameter vectors that you know, uses 3 and 4, Carol and Dave, gave us. And so based on the fact that movie 1 is loved by Alice and Bob and hated by Carol and Dave, we might reasonably conclude that this is probably a romantic movie, it is probably not much of an action movie. this example is a little bit mathematically simplified but what we're really asking is what feature vector should X1 be so that theta 1 transpose x1 is approximately equal to 5, that's Alice's rating, and theta 2 transpose x1 is also approximately equal to 5, and theta 3 transpose x1 is approximately equal to 0, so this would be Carol's rating, and theta 4 transpose X1 is approximately equal to 0. And from this it looks like, you know, X1 equals one that's the intercept term, and then 1.0, 0.0, that makes sense given what we know of Alice, Bob, Carol, and Dave's preferences for movies and the way they rated this movie. And so more generally, we can go down this list and try to figure out what might be reasonable features for these other movies as well. Let's formalize this problem of learning the features XI. Let's say that our users have given us their preferences. So let's say that our users have come and, you know, told us these values for theta 1 through theta of NU and we want to learn the feature vector XI for movie number I. What we can do is therefore pose the following optimization problem. So we want to sum over all the indices J for which we have a rating for movie I because we're trying to learn the features for movie I that is this feature vector XI. So and then what we want to do is minimize this squared error, so we want to choose features XI, so that, you know, the predictive value of how user J rates movie I will be similar, will be not too far in the squared error sense of the actual value YIJ that we actually observe in the rating of user j on movie I.\nSo, just to summarize what this term does is it tries to choose features XI so that for all the users J that have rated that movie, the algorithm also predicts a value for how that user would have rated that movie that is not too far, in the squared error sense, from the actual value that the user had rated that movie. So that's the squared error term. As usual, we can also add this sort of regularization term to prevent the features from becoming too big. So this is how we would learn the features for one specific movie but what we want to do is learn all the features for all the movies and so what I'm going to do is add this extra summation here so I'm going to sum over all Nm movies, N subscript m movies, and minimize this objective on top that sums of all movies. And if you do that, you end up with the following optimization problem. And if you minimize this, you have hopefully a reasonable set of features for all of your movies. So putting everything together, what we, the algorithm we talked about in the previous video and the algorithm that we just talked about in this video. In the previous video, what we showed was that you know, if you have a set of movie ratings, so if you have the data the rij's and then you have the yij's that will be the movie ratings. Then given features for your different movies we can learn these parameters theta. So if you knew the features, you can learn the parameters theta for your different users. And what we showed earlier in this video is that if your users are willing to give you parameters, then you can estimate features for the different movies. So this is kind of a chicken and egg problem. Which comes first? You know, do we want if we can get the thetas, we can know the Xs. If we have the Xs, we can learn the thetas. And what you can do is, and then this actually works, what you can do is in fact randomly guess some value of the thetas. Now based on your initial random guess for the thetas, you can then go ahead and use the procedure that we just talked about in order to learn features for your different movies. Now given some initial set of features for your movies you can then use this first method that we talked about in the previous video to try to get an even better estimate for your parameters theta. Now that you have a better setting of the parameters theta for your users, we can use that to maybe even get a better set of features and so on. We can sort of keep iterating, going back and forth and optimizing theta, x theta, x theta, nd this actually works and if you do this, this will actually cause your album to converge to a reasonable set of features for you movies and a reasonable set of parameters for your different users. So this is a basic collaborative filtering algorithm. This isn't actually the final algorithm that we're going to use. In the next video we are going to be able to improve on this algorithm and make it quite a bit more computationally efficient. But, hopefully this gives you a sense of how you can formulate a problem where you can simultaneously learn the parameters and simultaneously learn the features from the different movies. And for this problem, for the recommender system problem, this is possible only because each user rates multiple movies and hopefully each movie is rated by multiple users. And so you can do this back and forth process to estimate theta and x. \nSo to summarize, in this video we've seen an initial collaborative filtering algorithm. The term collaborative filtering refers to the observation that when you run this algorithm with a large set of users, what all of these users are effectively doing are sort of collaboratively--or collaborating to get better movie ratings for everyone because with every user rating some subset with the movies, every user is helping the algorithm a little bit to learn better features, and then by helping-- by rating a few movies myself, I will be helping the system learn better features and then these features can be used by the system to make better movie predictions for everyone else. And so there is a sense of collaboration where every user is helping the system learn better features for the common good. This is this collaborative filtering. And, in the next video what we going to do is take the ideas that have worked out, and try to develop a better an even better algorithm, a slightly better technique for collaborative filtering.""",99,0,1
coursera,stanford_university,machine-learning,collaborative-filtering-algorithm,"b""In the last couple videos, we talked about the ideas of how, first, if you're given features for movies, you can use that to learn parameters data for users. And second, if you're given parameters for the users, you can use that to learn features for the movies. In this video we're going to take those ideas and put them together to come up with a collaborative filtering algorithm. So one of the things we worked out earlier is that if you have features for the movies then you can solve this minimization problem to find the parameters theta for your users. And then we also worked that out, if you are given the parameters theta, you can also use that to estimate the features x, and you can do that by solving this minimization problem. So one thing you could do is actually go back and forth. Maybe randomly initialize the parameters and then solve for theta, solve for x, solve for theta, solve for x. But, it turns out that there is a more efficient algorithm that doesn't need to go back and forth between the x's and the thetas, but that can solve for theta and x simultaneously. And here it is. What we are going to do, is basically take both of these optimization objectives, and put them into the same objective. So I'm going to define the new optimization objective j, which is a cost function, that is a function of my features x and a function of my parameters theta. And, it's basically the two optimization objectives I had on top, but I put together. So, in order to explain this, first, I want to point out that this term over here, this squared error term, is the same as this squared error term and the summations look a little bit different, but let's see what the summations are really doing. The first summation is sum over all users J and then sum over all movies rated by that user. So, this is really summing over all pairs IJ, that correspond to a movie that was rated by a user. Sum over J says, for every user, the sum of all the movies rated by that user. This summation down here, just does things in the opposite order. This says for every movie I, sum over all the users J that have rated that movie and so, you know these summations, both of these are just summations over all pairs ij for which r of i J is equal to 1. It's just something over all the user movie pairs for which you have a rating. and so those two terms up there is just exactly this first term, and I've just written the summation here explicitly, where I'm just saying the sum of all pairs IJ, such that RIJ is equal to 1. So what we're going to do is define a combined optimization objective that we want to minimize in order to solve simultaneously for x and theta. And then the other terms in the optimization objective are this, which is a regularization in terms of theta. So that came down here and the final piece is this term which is my optimization objective for the x's and that became this. And this optimization objective j actually has an interesting property that if you were to hold the x's constant and just minimize with respect to the thetas then you'd be solving exactly this problem, whereas if you were to do the opposite, if you were to hold the thetas constant, and minimize j only with respect to the x's, then it becomes equivalent to this. Because either this term or this term is constant if you're minimizing only the respective x's or only respective thetas. So here's an optimization objective that puts together my cost functions in terms of x and in terms of theta. And in order to come up with just one optimization problem, what we're going to do, is treat this cost function, as a function of my features x and of my user pro user parameters data and just minimize this whole thing, as a function of both the Xs and a function of the thetas. And really the only difference between this and the older algorithm is that, instead of going back and forth, previously we talked about minimizing with respect to theta then minimizing with respect to x, whereas minimizing with respect to theta, minimizing with respect to x and so on. In this new version instead of sequentially going between the 2 sets of parameters x and theta, what we are going to do is just minimize with respect to both sets of parameters simultaneously. Finally one last detail is that when we're learning the features this way. Previously we have been using this convention that we have a feature x0 equals one that corresponds to an interceptor. When we are using this sort of formalism where we're are actually learning the features, we are actually going to do away with this convention. And so the features we are going to learn x, will be in Rn. Whereas previously we had features x and Rn + 1 including the intercept term. By getting rid of x0 we now just have x in Rn. And so similarly, because the parameters theta is in the same dimension, we now also have theta in RN because if there's no x0, then there's no need parameter theta 0 as well. And the reason we do away with this convention is because we're now learning all the features, right? So there is no need to hard code the feature that is always equal to one. Because if the algorithm really wants a feature that is always equal to 1, it can choose to learn one for itself. So if the algorithm chooses, it can set the feature X1 equals 1. So there's no need to hard code the feature of 001, the algorithm now has the flexibility to just learn it by itself. So, putting everything together, here is our collaborative filtering algorithm. first we are going to initialize x and theta to small random values. And this is a little bit like neural network training, where there we were also initializing all the parameters of a neural network to small random values. Next we're then going to minimize the cost function using great intercepts or one of the advance optimization algorithms. So, if you take derivatives you find that the great intercept like these and so this term here is the partial derivative of the cost function, I'm not going to write that out, with respect to the feature value Xik and similarly this term here is also a partial derivative value of the cost function with respect to the parameter theta that we're minimizing. And just as a reminder, in this formula that we no longer have this X0 equals 1 and so we have that x is in Rn and theta is a Rn. In this new formalism, we're regularizing every one of our perimeters theta, you know, every one of our parameters Xn. There's no longer the special case theta zero, which was regularized differently, or which was not regularized compared to the parameters theta 1 down to theta. So there is now no longer a theta 0, which is why in these updates, I did not break out a special case for k equals 0. So we then use gradient descent to minimize the cost function j with respect to the features x and with respect to the parameters theta. And finally, given a user, if a user has some parameters, theta, and if there's a movie with some sort of learned features x, we would then predict that that movie would be given a star rating by that user of theta transpose j. Or just to fill those in, then we're saying that if user J has not yet rated movie I, then what we do is predict that user J is going to rate movie I according to theta J transpose Xi. So that's the collaborative filtering algorithm and if you implement this algorithm you actually get a pretty decent algorithm that will simultaneously learn good features for hopefully all the movies as well as learn parameters for all the users and hopefully give pretty good predictions for how different users will rate different movies that they have not yet rated""",100,0,1
coursera,stanford_university,machine-learning,vectorization-low-rank-matrix-factorization,"b""In the last few videos, we talked about a collaborative filtering algorithm. In this video I'm going to say a little bit about the vectorization implementation of this algorithm. And also talk a little bit about other things you can do with this algorithm. For example, one of the things you can do is, given one product can you find other products that are related to this so that for example, a user has recently been looking at one product. Are there other related products that you could recommend to this user? So let's see what we could do about that. What I'd like to do is work out an alternative way of writing out the predictions of the collaborative filtering algorithm. To start, here is our data set with our five movies and what I'm going to do is take all the ratings by all the users and group them into a matrix. So, here we have five movies and four users, and so this matrix y is going to be a 5 by 4 matrix. It's just you know, taking all of the elements, all of this data. Including question marks, and grouping them into this matrix. And of course the elements of this matrix of the (i, j) element of this matrix is really what we were previously writing as y superscript i, j. It's the rating given to movie i by user j. Given this matrix y of all the ratings that we have, there's an alternative way of writing out all the predictive ratings of the algorithm. And, in particular if you look at what a certain user predicts on a certain movie, what user j predicts on movie i is given by this formula. And so, if you have a matrix of the predicted ratings, what you would have is the following matrix where the i, j entry. So this corresponds to the rating that we predict using j will give to movie i is exactly equal to that theta j transpose XI, and so, you know, this is a matrix where this first element the one-one element is a predictive rating of user one or movie one and this element, this is the one-two element is the predicted rating of user two on movie one, and so on, and this is the predicted rating of user one on the last movie and if you want, you know, this rating is what we would have predicted for this value and this rating is what we would have predicted for that value, and so on. Now, given this matrix of predictive ratings there is then a simpler or vectorized way of writing these out. In particular if I define the matrix x, and this is going to be just like the matrix we had earlier for linear regression to be sort of x1 transpose x2 transpose down to x of nm transpose. So I'm take all the features for my movies and stack them in rows. So if you think of each movie as one example and stack all of the features of the different movies and rows. And if we also to find a matrix capital theta, and what I'm going to do is take each of the per user parameter vectors, and stack them in rows, like so. So that's theta 1, which is the parameter vector for the first user. And, you know, theta 2, and so, you must stack them in rows like this to define a matrix capital theta and so I have nu parameter vectors all stacked in rows like this. Now given this definition for the matrix x and this definition for the matrix theta in order to have a vectorized way of computing the matrix of all the predictions you can just compute x times the matrix theta transpose, and that gives you a vectorized way of computing this matrix over here. To give the collaborative filtering algorithm that you've been using another name. The algorithm that we're using is also called low rank matrix factorization. And so if you hear people talk about low rank matrix factorization that's essentially exactly the algorithm that we have been talking about. And this term comes from the property that this matrix x times theta transpose has a mathematical property in linear algebra called that this is a low rank matrix and so that's what gives rise to this name low rank matrix factorization for these algorithms, because of this low rank property of this matrix x theta transpose. In case you don't know what low rank means or in case you don't know what a low rank matrix is, don't worry about it. You really don't need to know that in order to use this algorithm. But if you're an expert in linear algebra, that's what gives this algorithm, this other name of low rank matrix factorization. Finally, having run the collaborative filtering algorithm here's something else that you can do which is use the learned features in order to find related movies. Specifically for each product i really for each movie i, we've learned a feature vector xi. So, you know, when you learn a certain features without really know that can the advance what the different features are going to be, but if you run the algorithm and perfectly the features will tend to capture what are the important aspects of these different movies or different products or what have you. What are the important aspects that cause some users to like certain movies and cause some users to like different sets of movies. So maybe you end up learning a feature, you know, where x1 equals romance, x2 equals action similar to an earlier video and maybe you learned a different feature x3 which is a degree to which this is a comedy. Then some feature x4 which is, you know, some other thing. And you have N features all together and after you have learned features it's actually often pretty difficult to go in to the learned features and come up with a human understandable interpretation of what these features really are. But in practice, you know, the features even though these features can be hard to visualize. It can be hard to figure out just what these features are. Usually, it will learn features that are very meaningful for capturing whatever are the most important or the most salient properties of a movie that causes you to like or dislike it. And so now let's say we want to address the following problem. Say you have some specific movie i and you want to find other movies j that are related to that movie. And so well, why would you want to do this? Right, maybe you have a user that's browsing movies, and they're currently watching movie j, than what's a reasonable movie to recommend to them to watch after they're done with movie j? Or if someone's recently purchased movie j, well, what's a different movie that would be reasonable to recommend to them for them to consider purchasing. So, now that you have learned these feature vectors, this gives us a very convenient way to measure how similar two movies are. In particular, movie i has a feature vector xi. and so if you can find a different movie, j, so that the distance between xi and xj is small, then this is a pretty strong indication that, you know, movies j and i are somehow similar. At least in the sense that some of them likes movie i, maybe more likely to like movie j as well. So, just to recap, if your user is looking at some movie i and if you want to find the 5 most similar movies to that movie in order to recommend 5 new movies to them, what you do is find the five movies j, with the smallest distance between the features between these different movies. And this could give you a few different movies to recommend to your user. So with that, hopefully, you now know how to use a vectorized implementation to compute all the predicted ratings of all the users and all the movies, and also how to do things like use learned features to find what might be movies and what might be products that aren't related to each other.""",101,0,1
coursera,stanford_university,machine-learning,implementational-detail-mean-normalization,"b""By now you've seen all of the main pieces of the recommender system algorithm or the collaborative filtering algorithm. In this video I want to just share one last implementational detail, namely mean normalization, which can sometimes just make the algorithm work a little bit better. To motivate the idea of mean normalization, let's consider an example of where there's a user that has not rated any movies. So, in addition to our four users, Alice, Bob, Carol, and Dave, I've added a fifth user, Eve, who hasn't rated any movies. Let's see what our collaborative filtering algorithm will do on this user. Let's say that n is equal to 2 and so we're going to learn two features and we are going to have to learn a parameter vector theta 5, which is going to be in R2, remember this is now vectors in Rn not Rn+1, we'll learn the parameter vector theta 5 for our user number 5, Eve. So if we look in the first term in this optimization objective, well the user Eve hasn't rated any movies, so there are no movies for which Rij is equal to one for the user Eve and so this first term plays no role at all in determining theta 5 because there are no movies that Eve has rated. And so the only term that effects theta 5 is this term. And so we're saying that we want to choose vector theta 5 so that the last regularization term is as small as possible. In other words we want to minimize this lambda over 2 theta 5 subscript 1 squared plus theta 5 subscript 2 squared so that's the component of the regularization term that corresponds to user 5, and of course if your goal is to minimize this term, then what you're going to end up with is just theta 5 equals 0 0. Because a regularization term is encouraging us to set parameters close to 0 and if there is no data to try to pull the parameters away from 0, because this first term doesn't effect theta 5, we just end up with theta 5 equals the vector of all zeros. And so when we go to predict how user 5 would rate any movie, we have that theta 5 transpose xi, for any i, that's just going to be equal to zero. Because theta 5 is 0 for any value of x, this inner product is going to be equal to 0. And what we're going to have therefore, is that we're going to predict that Eve is going to rate every single movie with zero stars. But this doesn't seem very useful does it? I mean if you look at the different movies, Love at Last, this first movie, a couple people rated it 5 stars. And for even the Swords vs. Karate, someone rated it 5 stars. So some people do like some movies. It seems not useful to just predict that Eve is going to rate everything 0 stars. And in fact if we're predicting that eve is going to rate everything 0 stars, we also don't have any good way of recommending any movies to her, because you know all of these movies are getting exactly the same predicted rating for Eve so there's no one movie with a higher predicted rating that we could recommend to her, so, that's not very good. The idea of mean normalization will let us fix this problem. So here's how it works. As before let me group all of my movie ratings into this matrix Y, so just take all of these ratings and group them into matrix Y.  And this column over here of all question marks corresponds to Eve's not having rated any movies. Now to perform mean normalization what I'm going to do is compute the average rating that each movie obtained. And I'm going to store that in a vector that we'll call mu. So the first movie got two 5-star and two 0-star ratings, so the average of that is a 2.5-star rating. The second movie had an average of 2.5-stars and so on. And the final movie that has 0, 0, 5, 0. And the average of 0, 0, 5, 0, that averages out to an average of 1.25 rating. And what I'm going to do is look at all the movie ratings and I'm going to subtract off the mean rating. So this first element 5 I'm going to subtract off 2.5 and that gives me 2.5. And the second element 5 subtract off of 2.5, get a 2.5. And then the 0, 0, subtract off 2.5 and you get -2.5, -2.5. In other words, what I'm going to do is take my matrix of movie ratings, take this wide matrix, and subtract form each row the average rating for that movie. So, what I'm doing is just normalizing each movie to have an average rating of zero. And so just one last example. If you look at this last row, 0 0 5 0. We're going to subtract 1.25, and so I end up with these values over here. So now and of course the question marks stay a question mark. So each movie in this new matrix Y has an average rating of 0. What I'm going to do then, is take this set of ratings and use it with my collaborative filtering algorithm. So I'm going to pretend that this was the data that I had gotten from my users, or pretend that these are the actual ratings I had gotten from the users, and I'm going to use this as my data set with which to learn my parameters theta J and my features XI - from these mean normalized movie ratings. When I want to make predictions of movie ratings, what I'm going to do is the following:  for user J on movie I, I'm gonna predict theta J transpose XI, where X and theta are the parameters that I've learned from this mean normalized data set. But, because on the data set, I had subtracted off the means in order to make a prediction on movie i, I'm going to need to add back in the mean, and so i'm going to add back in mu i. And so that's going to be my prediction where in my training data subtracted off all the means and so when we make predictions and we need to add back in these means mu i for movie i.  And so specifically if you user 5 which is Eve, the same argument as the previous slide still applies in the sense that Eve had not rated any movies and so the learned parameter for user 5 is still going to be equal to 0, 0. And so what we're going to get then is that on a particular movie i we're going to predict for Eve theta 5, transpose xi plus add back in mu i and so this first component is going to be equal to zero, if theta five is equal to zero. And so on movie i, we are going to end a predicting mu i. And, this actually makes sense. It means that on movie 1 we're going to predict Eve rates it 2.5. On movie 2 we're gonna predict Eve rates it 2.5. On movie 3 we're gonna predict Eve rates it at 2 and so on. This actually makes sense, because it says that if Eve hasn't rated any movies and we just don't know anything about this new user Eve, what we're going to do is just predict for each of the movies, what are the average rating that those movies got. Finally, as an aside, in this video we talked about mean normalization, where we normalized each row of the matrix y, to have mean 0. In case you have some movies with no ratings, so it is analogous to a user who hasn't rated anything, but in case you have some movies with no ratings, you can also play with versions of the algorithm, where you normalize the different columns to have means zero, instead of normalizing the rows to have mean zero, although that's maybe less important, because if you really have a movie with no rating, maybe you just shouldn't recommend that movie to anyone, anyway. And so, taking care of the case of a user who hasn't rated anything might be more important than taking care of the case of a movie that hasn't gotten a single rating. So to summarize, that's how you can do mean normalization as a sort of pre-processing step for collaborative filtering. Depending on your data set, this might some times make your implementation work just a little bit better.""",102,0,1
coursera,stanford_university,machine-learning,learning-with-large-datasets,"b""In the next few videos, we'll talk about large scale machine learning. That is, algorithms but viewing with big data sets. If you look back at a recent 5 or 10-year history of machine learning. One of the reasons that learning algorithms work so much better now than even say, 5-years ago, is just the sheer amount of data that we have now and that we can train our algorithms on. In these next few videos, we'll talk about algorithms for dealing when we have such massive data sets. So why do we want to use such large data sets? We've already seen that one of the best ways to get a high performance machine learning system, is if you take a low-bias learning algorithm, and train that on a lot of data. And so, one early example we have already seen was this example of classifying between confusable words. So, for breakfast, I ate two (TWO) eggs and we saw in this example, these sorts of results, where, you know, so long as you feed the algorithm a lot of data, it seems to do very well. And so it's results like these that has led to the saying in machine learning that often it's not who has the best algorithm that wins. It's who has the most data. So you want to learn from large data sets, at least when we can get such large data sets. But learning with large data sets comes with its own unique problems, specifically, computational problems. Let's say your training set size is M equals 100,000,000. And this is actually pretty realistic for many modern data sets. If you look at the US Census data set, if there are, you know, 300 million people in the US, you can usually get hundreds of millions of records. If you look at the amount of traffic that popular websites get, you easily get training sets that are much larger than hundreds of millions of examples. And let's say you want to train a linear regression model, or maybe a logistic regression model, in which case this is the gradient descent rule. And if you look at what you need to do to compute the gradient, which is this term over here, then when M is a hundred million, you need to carry out a summation over a hundred million terms, in order to compute these derivatives terms and to perform a single step of decent. Because of the computational expense of summing over a hundred million entries in order to compute just one step of gradient descent, in the next few videos we've spoken about techniques for either replacing this with something else or to find more efficient ways to compute this derivative. By the end of this sequence of videos on large scale machine learning, you know how to fit models, linear regression, logistic regression, neural networks and so on even today's data sets with, say, a hundred million examples. Of course, before we put in the effort into training a model with a hundred million examples, We should also ask ourselves, well, why not use just a thousand examples. Maybe we can randomly pick the subsets of a thousand examples out of a hundred million examples and train our algorithm on just a thousand examples. So before investing the effort into actually developing and the software needed to train these massive models is often a good sanity check, if training on just a thousand examples might do just as well. The way to sanity check of using a much smaller training set might do just as well, that is if using a much smaller n equals 1000 size training set, that might do just as well, it is the usual method of plotting the learning curves, so if you were to plot the learning curves and if your training objective were to look like this, that's J train theta. And if your cross-validation set objective, Jcv of theta would look like this, then this looks like a high-variance learning algorithm, and we will be more confident that adding extra training examples would improve performance. Whereas in contrast if you were to plot the learning curves, if your training objective were to look like this, and if your cross-validation objective were to look like that, then this looks like the classical high-bias learning algorithm. And in the latter case, you know, if you were to plot this up to, say, m equals 1000 and so that is m equals 500 up to m equals 1000, then it seems unlikely that increasing m to a hundred million will do much better and then you'd be just fine sticking to n equals 1000, rather than investing a lot of effort to figure out how the scale of the algorithm. Of course, if you were in the situation shown by the figure on the right, then one natural thing to do would be to add extra features, or add extra hidden units to your neural network and so on, so that you end up with a situation closer to that on the left, where maybe this is up to n equals 1000, and this then gives you more confidence that trying to add infrastructure to change the algorithm to use much more than a thousand examples that might actually be a good use of your time. So in large-scale machine learning, we like to come up with computationally reasonable ways, or computationally efficient ways, to deal with very big data sets. In the next few videos, we'll see two main ideas. The first is called stochastic gradient descent and the second is called Map Reduce, for viewing with very big data sets. And after you've learned about these methods, hopefully that will allow you to scale up your learning algorithms to big data and allow you to get much better performance on many different applications.""",103,0,1
coursera,stanford_university,machine-learning,stochastic-gradient-descent,"b""For many learning algorithms, among them linear regression, logistic regression and neural networks, the way we derive the algorithm was by coming up with a cost function or coming up with an optimization objective. And then using an algorithm like gradient descent to minimize that cost function. We have a very large training set gradient descent becomes a computationally very expensive procedure. In this video, we'll talk about a modification to the basic gradient descent algorithm called Stochastic gradient descent, which will allow us to scale these algorithms to much bigger training sets. Suppose you are training a linear regression model using gradient descent. As a quick recap, the hypothesis will look like this, and the cost function will look like this, which is the sum of one half of the average square error of your hypothesis on your m training examples, and the cost function we've already seen looks like this sort of bow-shaped function. So, plotted as function of the parameters theta 0 and theta 1, the cost function J is a sort of a bow-shaped function. And gradient descent looks like this, where in the inner loop of gradient descent you repeatedly update the parameters theta using that expression. Now in the rest of this video, I'm going to keep using linear regression as the running example. But the ideas here, the ideas of Stochastic gradient descent is fully general and also applies to other learning algorithms like logistic regression, neural networks and other algorithms that are based on training gradient descent on a specific training set. So here's a picture of what gradient descent does, if the parameters are initialized to the point there then as you run gradient descent different iterations of gradient descent will take the parameters to the global minimum. So take a trajectory that looks like that and heads pretty directly to the global minimum. Now, the problem with gradient descent is that if m is large. Then computing this derivative term can be very expensive, because the surprise, summing over all m examples. So if m is 300 million, alright. So in the United States, there are about 300 million people. And so the US or United States census data may have on the order of that many records. So you want to fit the linear regression model to that then you need to sum over 300 million records. And that's very expensive. To give the algorithm a name, this particular version of gradient descent is also called Batch gradient descent. And the term Batch refers to the fact that we're looking at all of the training examples at a time. We call it sort of a batch of all of the training examples. And it really isn't the, maybe the best name but this is what machine learning people call this particular version of gradient descent. And if you imagine really that you have 300 million census records stored away on disc. The way this algorithm works is you need to read into your computer memory all 300 million records in order to compute this derivative term. You need to stream all of these records through computer because you can't store all your records in computer memory. So you need to read through them and slowly, you know, accumulate the sum in order to compute the derivative. And then having done all that work, that allows you to take one step of gradient descent. And now you need to do the whole thing again. You know, scan through all 300 million records, accumulate these sums. And having done all that work, you can take another little step using gradient descent. And then do that again. And then you take yet a third step. And so on. And so it's gonna take a long time in order to get the algorithm to converge. In contrast to Batch gradient descent, what we are going to do is come up with a different algorithm that doesn't need to look at all the training examples in every single iteration, but that needs to look at only a single training example in one iteration. Before moving on to the new algorithm, here's just a Batch gradient descent algorithm written out again with that being the cost function and that being the update and of course this term here, that's used in the gradient descent rule, that is the partial derivative with respect to the parameters theta J of our optimization objective, J train of theta. Now, let's look at the more efficient algorithm that scales better to large data sets. In order to work off the algorithms called Stochastic gradient descent, this vectors the cost function in a slightly different way then they define the cost of the parameter theta with respect to a training example x(i), y(i) to be equal to one half times the squared error that my hypothesis incurs on that example, x(i), y(i). So this cost function term really measures how well is my hypothesis doing on a single example x(i), y(i). Now you notice that the overall cost function j train can now be written in this equivalent form. So j train is just the average over my m training examples of the cost of my hypothesis on that example x(i), y(i). Armed with this view of the cost function for linear regression, let me now write out what Stochastic gradient descent does. The first step of Stochastic gradient descent is to randomly shuffle the data set. So by that I just mean randomly shuffle, or randomly reorder your m training examples. It's sort of a standard pre-processing step, come back to this in a minute. But the main work of Stochastic gradient descent is then done in the following. We're going to repeat for i equals 1 through m. So we'll repeatedly scan through my training examples and perform the following update. Gonna update the parameter theta j as theta j minus alpha times h of x(i) minus y(i) times x(i)j. And we're going to do this update as usual for all values of j. Now, you notice that this term over here is exactly what we had inside the summation for Batch gradient descent. In fact, for those of you that are calculus is possible to show that that term here, that's this term here, is equal to the partial derivative with respect to my parameter theta j of the cost of the parameters theta on x(i), y(i). Where cost is of course this thing that was defined previously. And just the wrap of the algorithm, let me close my curly braces over there. So what Stochastic gradient descent is doing is it is actually scanning through the training examples. And first it's gonna look at my first training example x(1), y(1). And then looking at only this first example, it's gonna take like a basically a little gradient descent step with respect to the cost of just this first training example. So in other words, we're going to look at the first example and modify the parameters a little bit to fit just the first training example a little bit better. Having done this inside this inner for-loop is then going to go on to the second training example. And what it's going to do there is take another little step in parameter space, so modify the parameters just a little bit to try to fit just a second training example a little bit better. Having done that, is then going to go onto my third training example. And modify the parameters to try to fit just the third training example a little bit better, and so on until you know, you get through the entire training set. And then this ultra repeat loop may cause it to take multiple passes over the entire training set. This view of Stochastic gradient descent also motivates why we wanted to start by randomly shuffling the data set. This doesn't show us that when we scan through the training site here, that we end up visiting the training examples in some sort of randomly sorted order. Depending on whether your data already came randomly sorted or whether it came originally sorted in some strange order, in practice this would just speed up the conversions to Stochastic gradient descent just a little bit. So in the interest of safety, it's usually better to randomly shuffle the data set if you aren't sure if it came to you in randomly sorted order. But more importantly another view of Stochastic gradient descent is that it's a lot like descent but rather than wait to sum up these gradient terms over all m training examples, what we're doing is we're taking this gradient term using just one single training example and we're starting to make progress in improving the parameters already. So rather than, you know, waiting 'till taking a path through all 300,000 United States Census records, say, rather than needing to scan through all of the training examples before we can modify the parameters a little bit and make progress towards a global minimum. For Stochastic gradient descent instead we just need to look at a single training example and we're already starting to make progress in this case of parameters towards, moving the parameters towards the global minimum. So, here's the algorithm written out again where the first step is to randomly shuffle the data and the second step is where the real work is done, where that's the update with respect to a single training example x(i), y(i). So, let's see what this algorithm does to the parameters. Previously, we saw that when we are using Batch gradient descent, that is the algorithm that looks at all the training examples in time, Batch gradient descent will tend to, you know, take a reasonably straight line trajectory to get to the global minimum like that. In contrast with Stochastic gradient descent every iteration is going to be much faster because we don't need to sum up over all the training examples. But every iteration is just trying to fit single training example better. So, if we were to start stochastic gradient descent, oh, let's start stochastic gradient descent at a point like that. The first iteration, you know, may take the parameters in that direction and maybe the second iteration looking at just the second example maybe just by chance, we get more unlucky and actually head in a bad direction with the parameters like that. In the third iteration where we tried to modify the parameters to fit just the third training examples better, maybe we'll end up heading in that direction. And then we'll look at the fourth training example and we will do that. The fifth example, sixth example, 7th and so on. And as you run Stochastic gradient descent, what you find is that it will generally move the parameters in the direction of the global minimum, but not always. And so take some more random-looking, circuitous path to watch the global minimum. And in fact as you run Stochastic gradient descent it doesn't actually converge in the same same sense as Batch gradient descent does and what it ends up doing is wandering around continuously in some region that's in some region close to the global minimum, but it doesn't just get to the global minimum and stay there. But in practice this isn't a problem because, you know, so long as the parameters end up in some region there maybe it is pretty close to the global minimum. So, as parameters end up pretty close to the global minimum, that will be a pretty good hypothesis and so usually running Stochastic gradient descent we get a parameter near the global minimum and that's good enough for, you know, essentially any, most practical purposes. Just one final detail. In Stochastic gradient descent, we had this outer loop repeat which says to do this inner loop multiple times. So, how many times do we repeat this outer loop? Depending on the size of the training set, doing this loop just a single time may be enough. And up to, you know, maybe 10 times may be typical so we may end up repeating this inner loop anywhere from once to ten times. So if we have a you know, truly massive data set like the this US census gave us that example that I've been talking about with 300 million examples, it is possible that by the time you've taken just a single pass through your training set. So, this is for i equals 1 through 300 million. It's possible that by the time you've taken a single pass through your data set you might already have a perfectly good hypothesis. In which case, you know, this inner loop you might need to do only once if m is very, very large. But in general taking anywhere from 1 through 10 passes through your data set, you know, maybe fairly common. But really it depends on the size of your training set. And if you contrast this to Batch gradient descent. With Batch gradient descent, after taking a pass through your entire training set, you would have taken just one single gradient descent steps. So one of these little baby steps of gradient descent where you just take one small gradient descent step and this is why Stochastic gradient descent can be much faster. So, that was the Stochastic gradient descent algorithm. And if you implement it, hopefully that will allow you to scale up many of your learning algorithms to much bigger data sets and get much more performance that way.""",104,0,1
coursera,stanford_university,machine-learning,mini-batch-gradient-descent,"b'In the previous video, we talked about Stochastic gradient descent, and how that can be much faster than Batch gradient descent. In this video, let\'s talk about another variation on these ideas is called Mini-batch gradient descent they can work sometimes even a bit faster than stochastic gradient descent. To summarize the algorithms we talked about so far. In Batch gradient descent we will use all m examples in each generation. Whereas in Stochastic gradient descent we will use a single example in each generation. What Mini-batch gradient descent does is somewhere in between. Specifically, with this algorithm we\'re going to use b examples in each iteration where b is a parameter called the ""mini batch size"" so the idea is that this is somewhat in-between Batch gradient descent and Stochastic gradient descent. This is just like batch gradient descent, except that I\'m going to use a much smaller batch size. A typical choice for the value of b might be b equals 10, lets say, and a typical range really might be anywhere from b equals 2 up to b equals 100. So that will be a pretty typical range of values for the Mini-batch size. And the idea is that rather than using one example at a time or m examples at a time we will use b examples at a time. So let me just write this out informally, we\'re going to get, let\'s say, b. For this example, let\'s say b equals 10. So we\'re going to get, the next 10 examples from my training set so that may be some set of examples xi, yi. If it\'s 10 examples then the indexing will be up to x (i+9), y (i+9) so that\'s 10 examples altogether and then we\'ll perform essentially a gradient descent update using these 10 examples. So, that\'s any rate times one tenth times sum over k equals i through i+9 of h subscript theta of x(k) minus y(k) times x(k)j. And so in this expression, where summing the gradient terms over my ten examples. So, that\'s number ten, that\'s, you know, my mini batch size and just i+9 again, the 9 comes from the choice of the parameter b, and then after this we will then increase, you know, i by tenth, we will go on to the next ten examples and then keep moving like this. So just to write out the entire algorithm in full. In order to simplify the indexing for this one at the right top, I\'m going to assume we have a mini-batch size of ten and a training set size of a thousand, what we\'re going to do is have this sort of form, for i equals 1 and that in 21\'s the stepping, in steps of 10 because we look at 10 examples at a time. And then we perform this sort of gradient descent update using ten examples at a time so this 10 and this i+9 those are consequence of having chosen my mini-batch to be ten. And you know, this ultimate four-loop, this ends at 991 here because if I have 1000 training samples then I need 100 steps of size 10 in order to get through my training set. So this is mini-batch gradient descent. Compared to batch gradient descent, this also allows us to make progress much faster. So we have again our running example of, you know, U.S. Census data with 300 million training examples, then what we\'re saying is after looking at just the first 10 examples we can start to make progress in improving the parameters theta so we don\'t need to scan through the entire training set. We just need to look at the first 10 examples and this will start letting us make progress and then we can look at the second ten examples and modify the parameters a little bit again and so on. So, that is why Mini-batch gradient descent can be faster than batch gradient descent. Namely, you can start making progress in modifying the parameters after looking at just ten examples rather than needing to wait \'till you\'ve scan through every single training example of 300 million of them. So, how about Mini-batch gradient descent versus Stochastic gradient descent. So, why do we want to look at b examples at a time rather than look at just a single example at a time as the Stochastic gradient descent? The answer is in vectorization. In particular, Mini-batch gradient descent is likely to outperform Stochastic gradient descent only if you have a good vectorized implementation. In that case, the sum over 10 examples can be performed in a more vectorized way which will allow you to partially parallelize your computation over the ten examples. So, in other words, by using appropriate vectorization to compute the rest of the terms, you can sometimes partially use the good numerical algebra libraries and parallelize your gradient computations over the b examples, whereas if you were looking at just a single example of time with Stochastic gradient descent then, you know, just looking at one example at a time their isn\'t much to parallelize over. At least there is less to parallelize over. One disadvantage of Mini-batch gradient descent is that there is now this extra parameter b, the Mini-batch size which you may have to fiddle with, and which may therefore take time. But if you have a good vectorized implementation this can sometimes run even faster that Stochastic gradient descent. So that was Mini-batch gradient descent which is an algorithm that in some sense does something that\'s somewhat in between what Stochastic gradient descent does and what Batch gradient descent does. And if you choose their reasonable value of b. I usually use b equals 10, but, you know, other values, anywhere from say 2 to 100, would be reasonably common. So we choose value of b and if you use a good vectorized implementation, sometimes it can be faster than both Stochastic gradient descent and faster than Batch gradient descent.'",105,0,1
coursera,stanford_university,machine-learning,stochastic-gradient-descent-convergence,"b""You now know about the stochastic gradient descent algorithm. But when you're running the algorithm, how do you make sure that it's completely debugged and is converging okay? Equally important, how do you tune the learning rate alpha with Stochastic Gradient Descent. In this video we'll talk about some techniques for doing these things, for making sure it's converging and for picking the learning rate alpha. Back when we were using batch gradient descent, our standard way for making sure that gradient descent was converging was we would plot the optimization cost function as a function of the number of iterations. So that was the cost function and we would make sure that this cost function is decreasing on every iteration. When the training set sizes were small, we could do that because we could compute the sum pretty efficiently. But when you have a massive training set size then you don't want to have to pause your algorithm periodically. You don't want to have to pause stochastic gradient descent periodically in order to compute this cost function since it requires a sum of your entire training set size. And the whole point of stochastic gradient was that you wanted to start to make progress after looking at just a single example without needing to occasionally scan through your entire training set right in the middle of the algorithm, just to compute things like the cost function of the entire training set. So for stochastic gradient descent, in order to check the algorithm is converging, here's what we can do instead. Let's take the definition of the cost that we had previously. So the cost of the parameters theta with respect to a single training example is just one half of the square error on that training example. Then, while stochastic gradient descent is learning, right before we train on a specific example. So, in stochastic gradient descent we're going to look at the examples xi, yi, in order, and then sort of take a little update with respect to this example. And we go on to the next example, xi plus 1, yi plus 1, and so on, right? That's what stochastic gradient descent does. So, while the algorithm is looking at the example xi, yi, but before it has updated the parameters theta using that an example, let's compute the cost of that example. Just to say the same thing again, but using slightly different words. A stochastic gradient descent is scanning through our training set right before we have updated theta using a specific training example x(i) comma y(i) let's compute how well our hypothesis is doing on that training example. And we want to do this before updating theta because if we've just updated theta using example, you know, that it might be doing better on that example than what would be representative. Finally, in order to check for the convergence of  stochastic gradient descent, what we can do is every, say, every thousand iterations, we can plot these costs that we've been computing in the previous step. We can plot those costs average over, say, the last thousand examples processed by the algorithm. And if you do this, it kind of gives you a running estimate of how well the algorithm is doing. on, you know, the last 1000 training examples that your algorithm has seen. So, in contrast to computing J<u>train periodically which needed to scan through the entire training set.</u> With this other procedure, well, as part of stochastic gradient descent, it doesn't cost much to compute these costs as well right before updating to parameter theta. And all we're doing is every thousand integrations or so, we just average the last 1,000 costs that we computed and plot that. And by looking at those plots, this will allow us to check if stochastic gradient descent is converging. So here are a few examples of what these plots might look like. Suppose you have plotted the cost average over the last thousand examples, because these are averaged over just a thousand examples, they are going to be a little bit noisy and so, it may not decrease on every single iteration. Then if you get a figure that looks like this, So the plot is noisy because it's average over, you know, just a small subset, say a thousand training examples. If you get a figure that looks like this, you know that would be a pretty decent run with the algorithm, maybe, where it looks like the cost has gone down and then this plateau that looks kind of flattened out, you know, starting from around that point. look like, this is what your cost looks like then maybe your learning algorithm has converged. If you want to try using a smaller learning rate, something you might see is that the algorithm may initially learn more slowly so the cost goes down more slowly. But then eventually you have a smaller learning rate is actually possible for the algorithm to end up at a, maybe very slightly better solution. So the red line may represent the behavior of stochastic gradient descent using a slower, using a smaller leaning rate. And the reason this is the case is because, you remember, stochastic gradient descent doesn't just converge to the global minimum, is that what it does is the parameters will oscillate a bit around the global minimum. And so by using a smaller learning rate, you'll end up with smaller oscillations. And sometimes this little difference will be negligible and sometimes with a smaller than you can get a slightly better value for the parameters. Here are some other things that might happen. Let's say you run stochastic gradient descent and you average over a thousand examples when plotting these costs. So, you know, here might be the result of another one of these plots. Then again, it kind of looks like it's converged. If you were to take this number, a thousand, and increase to averaging over 5 thousand examples. Then it's possible that you might get a smoother curve that looks more like this. And by averaging over, say 5,000 examples instead of 1,000, you might be able to get a smoother curve like this. And so that's the effect of increasing the number of examples you average over. The disadvantage of making this too big of course is that now you get one date point only every 5,000 examples. And so the feedback you get on how well your learning learning algorithm is doing is, sort of, maybe it's more delayed because you get one data point on your plot only every 5,000 examples rather than every 1,000 examples. Along a similar vein some times you may run a gradient descent and end up with a plot that looks like this. And with a plot that looks like this, you know, it looks like the cost just is not decreasing at all. It looks like the algorithm is just not learning. It's just, looks like this here a flat curve and the cost is just not decreasing. But again if you were to increase this to averaging over a larger number of examples it is possible that you see something like this red line it looks like the cost actually is decreasing, it's just that the blue line averaging over 2, 3 examples, the blue line was too noisy so you couldn't see the actual trend in the cost actually decreasing and possibly averaging over 5,000 examples instead of 1,000 may help. Of course we averaged over a larger number examples that we've averaged here over 5,000 examples, I'm just using a different color, it is also possible that you that see a learning curve ends up looking like this. That it's still flat even when you average over a larger number of examples. And as you get that, then that's maybe just a more firm verification that unfortunately the algorithm just isn't learning much for whatever reason. And you need to either change the learning rate or change the features or change something else about the algorithm. Finally, one last thing that you might see would be if you were to plot these curves and you see a curve that looks like this, where it actually looks like it's increasing. And if that's the case then this is a sign that the algorithm is diverging. And what you really should do is use a smaller value of the learning rate alpha. So hopefully this gives you a sense of the range of phenomena you might see when you plot these cost average over some range of examples as well as suggests the sorts of things you might try to do in response to seeing different plots. So if the plots looks too noisy, or if it wiggles up and down too much, then try increasing the number of examples you're averaging over so you can see the overall trend in the plot better. And if you see that the errors are actually increasing, the costs are actually increasing, try using a smaller value of alpha. Finally, it's worth examining the issue of the learning rate just a little bit more. We saw that when we run stochastic gradient descent, the algorithm will start here and sort of meander towards the minimum And then it won't really converge, and instead it'll wander around the minimum forever. And so you end up with a parameter value that is hopefully close to the global minimum that won't be exact at the global minimum. In most typical implementations of stochastic gradient descent, the learning rate alpha is typically held constant. And so what you we end up is exactly a picture like this. If you want stochastic gradient descent to actually converge to the global minimum, there's one thing which you can do which is you can slowly decrease the learning rate alpha over time. So, a pretty typical way of doing that would be to set alpha equals some constant 1 divided by iteration number plus constant 2. So, iteration number is the number of iterations you've run of stochastic gradient descent, so it's really the number of training examples you've seen And const 1 and const 2 are additional parameters of the algorithm that you might have to play with a bit in order to get good performance. One of the reasons people tend not to do this is because you end up needing to spend time playing with these 2 extra parameters, constant 1 and constant 2, and so this makes the algorithm more finicky. You know, it's just more parameters able to fiddle with in order to make the algorithm work well. But if you manage to tune the parameters well, then the picture you can get is that the algorithm will actually around towards the minimum, but as it gets closer because you're decreasing the learning rate the meanderings will get smaller and smaller until it pretty much just to the global minimum. I hope this makes sense, right? And the reason this formula makes sense is because as the algorithm runs, the iteration number becomes large So alpha will slowly become small, and so you take smaller and smaller steps until it hopefully converges to the global minimum. So If you do slowly decrease alpha to zero you can end up with a slightly better hypothesis. But because of the extra work needed to fiddle with the constants and because frankly usually we're pretty happy with any parameter value that is, you know, pretty close to the global minimum. Typically this process of decreasing alpha slowly is usually not done and keeping the learning rate alpha constant is the more common application of stochastic gradient descent although you will see people use either version. To summarize in this video we talk about a way for approximately monitoring how the stochastic gradient descent is doing in terms for optimizing the cost function. And this is a method that does not require scanning over the entire training set periodically to compute the cost function on the entire training set. But instead it looks at say only the last thousand examples or so. And you can use this method both to make sure the stochastic gradient descent is okay and is converging or to use it to tune the learning rate alpha.""",106,0,1
coursera,stanford_university,machine-learning,online-learning,"b'In this video, I\'d like to talk about a new large-scale machine learning setting called the online learning setting. The online learning setting allows us to model problems where we have a continuous flood or a continuous stream of data coming in and we would like an algorithm to learn from that. Today, many of the largest websites, or many of the largest website companies use different versions of online learning algorithms to learn from the flood of users that keep on coming to, back to the website. Specifically, if you have a continuous stream of data generated by a continuous stream of users coming to your website, what you can do is sometimes use an online learning algorithm to learn user preferences from the stream of data and use that to optimize some of the decisions on your website. Suppose you run a shipping service, so, you know, users come and ask you to help ship their package from location A to location B and suppose you run a website, where users repeatedly come and they tell you where they want to send the package from, and where they want to send it to (so the origin and destination) and your website offers to ship the package for some asking price, so I\'ll ship your package for $50, I\'ll ship it for $20. And based on the price that you offer to the users, the users sometimes chose to use a shipping service; that\'s a positive example and sometimes they go away and they do not choose to purchase your shipping service. So let\'s say that we want a learning algorithm to help us to optimize what is the asking price that we want to offer to our users. And specifically, let\'s say we come up with some sort of features that capture properties of the users. If we know anything about the demographics, they capture, you know, the origin and destination of the package, where they want to ship the package. And what is the price that we offer to them for shipping the package. and what we want to do is learn what is the probability that they will elect to ship the package, using our shipping service given these features, and again just as a reminder these features X also captures the price that we\'re asking for. And so if we could estimate the chance that they\'ll agree to use our service for any given price, then we can try to pick a price so that they have a pretty high probability of choosing our website while simultaneously hopefully offering us a fair return, offering us a fair profit for shipping their package. So if we can learn this property of y equals 1 given any price and given the other features we could really use this to choose appropriate prices as new users come to us. So in order to model the probability of y equals 1, what we can do is use logistic regression or neural network or some other algorithm like that. But let\'s start with logistic regression. Now if you have a website that just runs continuously, here\'s what an online learning algorithm would do. I\'m gonna write repeat forever. This just means that our website is going to, you know, keep on staying up. What happens on the website is occasionally a user will come and for the user that comes we\'ll get some x,y pair corresponding to a customer or to a user on the website. So the features x are, you know, the origin and destination specified by this user and the price that we happened to offer to them this time around, and y is either one or zero depending one whether or not they chose to use our shipping service. Now once we get this {x,y} pair, what an online learning algorithm does is then update the parameters theta using just this example x,y, and in particular we would update my parameters theta as Theta j get updated as Theta j minus the learning rate alpha times my usual gradient descent rule for logistic regression. So we do this for j equals zero up to n, and that\'s my close curly brace. So, for other learning algorithms instead of writing X-Y, right, I was writing things like Xi, Yi but in this online learning setting where actually discarding the notion of there being a fixed training set instead we have an algorithm. Now what happens as we get an example and then we learn using that example like so and then we throw that example away. We discard that example and we never use it again and so that\'s why we just look at one example at a time. We learn from that example. We discard it. Which is why, you know, we\'re also doing away with this notion of there being this sort of fixed training set indexed by i. And, if you really run a major website where you really have a continuous stream of users coming, then this sort of online learning algorithm is actually a pretty reasonable algorithm. Because of data is essentially free if you have so much data, that data is essentially unlimited then there is really may be no need to look at a training example more than once. Of course if we had only a small number of users then rather than using an online learning algorithm like this, you might be better off saving away all your data in a fixed training set and then running some algorithm over that training set. But if you really have a continuous stream of data, then an online learning algorithm can be very effective. I should mention also that one interesting effect of this sort of online learning algorithm is that it can adapt to changing user preferences. And in particular, if over time because of changes in the economy maybe users start to become more price sensitive and willing to pay, you know, less willing to pay high prices. Or if they become less price sensitive and they\'re willing to pay higher prices. Or if different things become more important to users, if you start to have new types of users coming to your website. This sort of online learning algorithm can also adapt to changing user preferences and kind of keep track of what your changing population of users may be willing to pay for. And it does that because if your pool of users changes, then these updates to your parameters theta will just slowly adapt your parameters to whatever your latest pool of users looks like. Here\'s another example of a sort of application to which you might apply online learning. this is an application in product search in which we want to apply learning algorithm to learn to give good search listings to a user. Let\'s say you run an online store that sells phones - that sells mobile phones or sells cell phones. And you have a user interface where a user can come to your website and type in the query like ""Android phone 1080p camera"". So 1080p is a type of a specification for a video camera that you might have on a phone, a cell phone, a mobile phone. Suppose, suppose we have a hundred phones in our store. And because of the way our website is laid out, when a user types in a query, if it was a search query, we would like to find a choice of ten different phones to show what to offer to the user. What we\'d like to do is have a learning algorithm help us figure out what are the ten phones out of the 100 we should return the user in response to a user-search query like the one here. Here\'s how we can go about the problem. For each phone and given a specific user query; we can construct a feature vector X. So the feature vector X might capture different properties of the phone. It might capture things like, how similar the user search query is in the phones. We capture things like how many words in the user search query match the name of the phone, how many words in the user search query match the description of the phone and so on. So the features x capture properties of the phone and it captures things about how similar or how well the phone matches the user query along different dimensions. What we like to do is estimate the probability that a user will click on the link for a specific phone, because we want to show the user phones that they are likely to want to buy, want to show the user phones that they have high probability of clicking on in the web browser. So I\'m going to define y equals one if the user clicks on the link for a phone and y equals zero otherwise and what I would like to do is learn the probability the user will click on a specific phone given, you know, the features x, which capture properties of the phone and how well the query matches the phone. To give this problem a name in the language of people that run websites like this, the problem of learning this is actually called the problem of learning the predicted click-through rate, the predicted CTR. It just means learning the probability that the user will click on the specific link that you offer them, so CTR is an abbreviation for click through rate. And if you can estimate the predicted click-through rate for any particular phone, what we can do is use this to show the user the ten phones that are most likely to click on, because out of the hundred phones, we can compute this for each of the 100 phones and just select the 10 phones that the user is most likely to click on, and this will be a pretty reasonable way to decide what ten results to show to the user. Just to be clear, suppose that every time a user does a search, we return ten results what that will do is it will actually give us ten x,y pairs, this actually gives us ten training examples every time a user comes to our website because, because for the ten phone that we chose to show the user, for each of those 10 phones we get a feature vector X, and for each of those 10 phones we show the user we will also get a value for y, we will also observe the value of y, depending on whether or not we clicked on that url or not and so, one way to run a website like this would be to continuously show the user, you know, your ten best guesses for what other phones they might like and so, each time a user comes you would get ten examples, ten x,y pairs, and then use an online learning algorithm to update the parameters using essentially 10 steps of gradient descent on these 10 examples, and then you can throw the data away, and if you really have a continuous stream of users coming to your website, this would be a pretty reasonable way to learn parameters for your algorithm so as to show the ten phones to your users that may be most promising and the most likely to click on. So, this is a product search problem or learning to rank phones, learning to search for phones example. So, I\'ll quickly mention a few others. One is, if you have a website and you\'re trying to decide, you know, what special offer to show the user, this is very similar to phones, or if you have a website and you show different users different news articles. So, if you\'re a news aggregator website, then you can again use a similar system to select, to show to the user, you know, what are the news articles that they are most likely to be interested in and what are the news articles that they are most likely to click on. Closely related to special offers, will we profit from recommendations. And in fact, if you have a collaborative filtering system, you can even imagine a collaborative filtering system giving you additional features to feed into a logistic regression classifier to try to predict the click through rate for different products that you might recommend to a user. Of course, I should say that any of these problems could also have been formulated as a standard machine learning problem, where you have a fixed training set. Maybe, you can run your website for a few days and then save away a training set, a fixed training set, and run a learning algorithm on that. But these are the actual sorts of problems, where you do see large companies get so much data, that there\'s really maybe no need to save away a fixed training set, but instead you can use an online learning algorithm to just learn continuously. from the data that users are generating on your website. So, that was the online learning setting and as we saw, the algorithm that we apply to it is really very similar to this schotastic gradient descent algorithm, only instead of scanning through a fixed training set, we\'re instead getting one example from a user, learning from that example, then discarding it and moving on. And if you have a continuous stream of data for some application, this sort of algorithm may be well worth considering for your application. And of course, one advantage of online learning is also that if you have a changing pool of users, or if the things you\'re trying to predict are slowly changing like your user taste is slowly changing, the online learning algorithm can slowly adapt your learned hypothesis to whatever the latest sets of user behaviors are like as well.'",107,0,1
coursera,stanford_university,machine-learning,map-reduce-and-data-parallelism,"b""In the last few videos, we talked about stochastic gradient descent, and, you know, other variations of the stochastic gradient descent algorithm, including those adaptations to online learning, but all of those algorithms could be run on one machine, or could be run on one computer. And some machine learning problems are just too big to run on one machine, sometimes maybe you just so much data you just don't ever want to run all that data through a single computer, no matter what algorithm you would use on that computer. So in this video I'd like to talk about different approach to large scale machine learning, called the map reduce approach. And even though we have quite a few videos on stochastic gradient descent and we're going to spend relative less time on map reduce--don't judge the relative importance of map reduce versus the gradient descent based on the amount amount of time I spend on these ideas in particular. Many people will say that map reduce is at least an equally important, and some would say an even more important idea compared to gradient descent, only it's relatively simpler to explain, which is why I'm going to spend less time on it, but using these ideas you might be able to scale learning algorithms to even far larger problems than is possible using stochastic gradient descent. Here's the idea. Let's say we want to fit a linear regression model or a logistic regression model or some such, and let's start again with batch gradient descent, so that's our batch gradient descent learning rule. And to keep the writing on this slide tractable, I'm going to assume throughout that we have m equals 400 examples. Of course, by our standards, in terms of large scale machine learning, you know m might be pretty small and so, this might be more commonly applied to problems, where you have maybe closer to 400 million examples, or some such, but just to make the writing on the slide simpler, I'm going to pretend we have 400 examples. So in that case, the batch gradient descent learning rule has this 400 and the sum from i equals 1 through 400 through my 400 examples here, and if m is large, then this is a computationally expensive step. So, what the MapReduce idea does is the following, and I should say the map reduce idea is due to two researchers, Jeff Dean and Sanjay Gimawat. Jeff Dean, by the way, is one of the most legendary engineers in all of Silicon Valley and he kind of built a large fraction of the architectural infrastructure that all of Google runs on today. But here's the map reduce idea. So, let's say I have some training set, if we want to denote by this box here of X Y pairs, where it's X1, Y1, down to my 400 examples, Xm, Ym. So, that's my training set with 400 training examples. In the MapReduce idea, one way to do, is split this training set in to different subsets. I'm going to. assume for this example that I have 4 computers, or 4 machines to run in parallel on my training set, which is why I'm splitting this into 4 machines. If you have 10 machines or 100 machines, then you would split your training set into 10 pieces or 100 pieces or what have you. And what the first of my 4 machines is to do, say, is use just the first one quarter of my training set--so use just the first 100 training examples. And in particular, what it's going to do is look at this summation, and compute that summation for just the first 100 training examples. So let me write that up I'm going to compute a variable temp 1 to superscript 1 the first machine J equals sum from equals 1 through 100, and then I'm going to plug in exactly that term there--so I have X-theta, Xi, minus Yi times Xij, right? So that's just that gradient descent term up there. And then similarly, I'm going to take the second quarter of my data and send it to my second machine, and my second machine will use training examples 101 through 200 and you will compute similar variables of a temp to j which is the same sum for index from examples 101 through 200. And similarly machines 3 and 4 will use the third quarter and the fourth quarter of my training set. So now each machine has to sum over 100 instead of over 400 examples and so has to do only a quarter of the work and thus presumably it could do it about four times as fast. Finally, after all these machines have done this work, I am going to take these temp variables and put them back together. So I take these variables and send them all to a You know centralized master server and what the master will do is combine these results together. and in particular, it will update my parameters theta j according to theta j gets updated as theta j minus Of the learning rate alpha times one over 400 times temp, 1, J, plus temp 2j plus temp 3j plus temp 4j and of course we have to do this separately for J equals 0. You know, up to and within this number of features. So operating this equation into I hope it's clear. So what this equation is doing is exactly the same is that when you have a centralized master server that takes the results, the ten one j the ten two j ten three j and ten four j and adds them up and so of course the sum of these four things. Right, that's just the sum of this, plus the sum of this, plus the sum of this, plus the sum of that, and those four things just add up to be equal to this sum that we're originally computing a batch stream descent. And then we have the alpha times 1 of 400, alpha times 1 of 100, and this is exactly equivalent to the batch gradient descent algorithm, only, instead of needing to sum over all four hundred training examples on just one machine, we can instead divide up the work load on four machines. So, here's what the general picture of the MapReduce technique looks like. We have some training sets, and if we want to paralyze across four machines, we are going to take the training set and split it, you know, equally. Split it as evenly as we can into four subsets. Then we are going to take the 4 subsets of the training data and send them to 4 different computers. And each of the 4 computers can compute a summation over just one quarter of the training set, and then finally take each of the computers takes the results, sends them to a centralized server, which then combines the results together. So, on the previous line in that example, the bulk of the work in gradient descent, was computing the sum from i equals 1 to 400 of something. So more generally, sum from i equals 1 to m of that formula for gradient descent. And now, because each of the four computers can do just a quarter of the work, potentially you can get up to a 4x speed up. In particular, if there were no network latencies and no costs of the network communications to send the data back and forth, you can potentially get up to a 4x speed up. Of course, in practice, because of network latencies, the overhead of combining the results afterwards and other factors, in practice you get slightly less than a 4x speedup. But, none the less, this sort of macro juice approach does offer us a way to process much larger data sets than is possible using a single computer. If you are thinking of applying Map Reduce to some learning algorithm, in order to speed this up. By paralleling the computation over different computers, the key question to ask yourself is, can your learning algorithm be expressed as a summation over the training set? And it turns out that many learning algorithms can actually be expressed as computing sums of functions over the training set and the computational expense of running them on large data sets is because they need to sum over a very large training set. So, whenever your learning algorithm can be expressed as a sum of the training set and whenever the bulk of the work of the learning algorithm can be expressed as the sum of the training set, then map reviews might a good candidate for scaling your learning algorithms through very, very good data sets. Lets just look at one more example. Let's say that we want to use one of the advanced optimization algorithm. So, things like, you know, l, b, f, g, s constant gradient and so on, and let's say we want to train a logistic regression of the algorithm. For that, we need to compute two main quantities. One is for the advanced optimization algorithms like, you know, LPF and constant gradient. We need to provide it a routine to compute the cost function of the optimization objective. And so for logistic regression, you remember that a cost function has this sort of sum over the training set, and so if youre paralizing over ten machines, you would split up the training set onto ten machines and have each of the ten machines compute the sum of this quantity over just one tenth of the training data. Then, the other thing that the advanced optimization algorithms need, is a routine to compute these partial derivative terms. Once again, these derivative terms, for which it's a logistic regression, can be expressed as a sum over the training set, and so once again, similar to our earlier example, you would have each machine compute that summation over just some small fraction of your training data. And finally, having computed all of these things, they could then send their results to a centralized server, which can then add up the partial sums. This corresponds to adding up those tenth i or tenth ij variables, which were computed locally on machine number i, and so the centralized server can sum these things up and get the overall cost function and get the overall partial derivative, which you can then pass through the advanced optimization algorithm. So, more broadly, by taking other learning algorithms and expressing them in sort of summation form or by expressing them in terms of computing sums of functions over the training set, you can use the MapReduce technique to parallelize other learning algorithms as well, and scale them to very large training sets. Finally, as one last comment, so far we have been discussing MapReduce algorithms as allowing you to parallelize over multiple computers, maybe multiple computers in a computer cluster or over multiple computers in the data center. It turns out that sometimes even if you have just a single computer, MapReduce can also be applicable. In particular, on many single computers now, you can have multiple processing cores. You can have multiple CPUs, and within each CPU you can have multiple proc cores. If you have a large training set, what you can do if, say, you have a computer with 4 computing cores, what you can do is, even on a single computer you can split the training sets into pieces and send the training set to different cores within a single box, like within a single desktop computer or a single server and use MapReduce this way to divvy up work load. Each of the cores can then carry out the sum over, say, one quarter of your training set, and then they can take the partial sums and combine them, in order to get the summation over the entire training set. The advantage of thinking about MapReduce this way, as paralyzing over cause within a single machine, rather than parallelizing over multiple machines is that, this way you don't have to worry about network latency, because all the communication, all the sending of the  [xx] back and forth, all that happens within a single machine. And so network latency becomes much less of an issue compared to if you were using this to over different computers within the data sensor. Finally, one last caveat on parallelizing within a multi-core machine. Depending on the details of your implementation, if you have a multi-core machine and if you have certain numerical linear algebra libraries. It turns out that the sum numerical linear algebra libraries that can automatically parallelize their linear algebra operations across multiple cores within the machine. So if you're fortunate enough to be using one of those numerical linear algebra libraries and certainly this does not apply to every single library. If you're using one of those libraries and. If you have a very good vectorizing implementation of the learning algorithm. Sometimes you can just implement you standard learning algorithm in a vectorized fashion and not worry about parallelization and numerical linear algebra libararies could take care of some of it for you. So you don't need to implement [xx] but. for other any problems, taking advantage of this sort of map reducing commentation, finding and using this MapReduce formulation and to paralelize a cross coarse except yourself might be a good idea as well and could let you speed up your learning algorithm. In this video, we talked about the MapReduce approach to parallelizing machine learning by taking a data and spreading them across many computers in the data center. Although these ideas are critical to paralysing across multiple cores within a single computer as well. Today there are some good open source implementations of MapReduce, so there are many users in open source system called Hadoop and using either your own implementation or using someone else's open source implementation, you can use these ideas to parallelize learning algorithms and get them to run on much larger data sets than is possible using just a single machine.""",108,0,1
coursera,stanford_university,machine-learning,problem-description-and-pipeline,"b'In this and the next few videos, I want to tell you about a machine learning application example, or a machine learning application history centered around an application called Photo OCR  . There are three reasons why I want to do this, first I wanted to show you an example of how a complex machine learning system can be put together. Second, once told the concepts of a machine learning a type line and how to allocate resources when you\'re trying to decide what to do next. And this can either be in the context of you working by yourself on the big application Or it can be the context of a team of developers trying to build a complex application together. And then finally, the Photo OCR problem also gives me an excuse to tell you about just a couple more interesting ideas for machine learning. One is some ideas of how to apply machine learning to computer vision problems, and second is the idea of artificial data synthesis, which we\'ll see in a couple of videos. So, let\'s start by talking about what is the Photo OCR problem. Photo OCR stands for Photo Optical Character Recognition. With the growth of digital photography and more recently the growth of camera in our cell phones we now have tons of visual pictures that we take all over the place. And one of the things that has interested many developers is how to get our computers to understand the content of these pictures a little bit better. The photo OCR problem focuses on how to get computers to read the text to the purest in images that we take. Given an image like this it might be nice if a computer can read the text in this image so that if you\'re trying to look for this picture again you type in the words, lulu bees and and have it automatically pull up this picture, so that you\'re not spending lots of time digging through your photo collection Maybe hundreds of thousands of pictures in. The Photo OCR problem does exactly this, and it does so in several steps. First, given the picture it has to look through the image and detect where there is text in the picture. And after it has done that or if it successfully does that it then has to look at these text regions and actually read the text in those regions, and hopefully if it reads it correctly, it\'ll come up with these transcriptions of what is the text that appears in the image. Whereas OCR, or optical character recognition of scanned documents is relatively easier problem, doing OCR from photographs today is still a very difficult machine learning problem, and you can do this. Not only can this help our computers to understand the content of our though images better, there are also applications like helping blind people, for example, if you could provide to a blind person a camera that can look at what\'s in front of them, and just tell them the words that my be on the street sign in front of them. With car navigation systems. For example, imagine if your car could read the street signs and help you navigate to your destination. In order to perform photo OCR, here\'s what we can do. First we can go through the image and find the regions where there\'s text and image. So, shown here is one example of text and image that the photo OCR system may find. Second, given the rectangle around that text region, we can then do character segmentation, where we might take this text box that says ""Antique Mall"" and try to segment it out into the locations of the individual characters. And finally, having segmented out into individual characters, we can then run a crossfire, which looks at the images of the visual characters, and tries to figure out the first character\'s an A, the second character\'s an N, the third character is a T, and so on, so that up by doing all this how that hopefully you can then figure out that this phrase is Rulegee\'s antique mall and similarly for some of the other words that appear in that image. I should say that there are some photo OCR systems that do even more complex things, like a bit of spelling correction at the end. So if, for example, your character segmentation and character classification system tells you that it sees the word c 1 e a n i n g. Then, you know, a sort of spelling correction system might tell you that this is probably the word \'cleaning\', and your character classification algorithm had just mistaken the l for a 1. But for the purpose of what we want to do in this video, let\'s ignore this last step and just focus on the system that does these three steps of text detection, character segmentation, and character classification. A system like this is what we call a machine learning pipeline. In particular, here\'s a picture showing the photo OCR pipeline. We have an image, which then fed to the text detection system text regions, we then segment out the characters--the individual characters in the text--and then finally we recognize the individual characters. In many complex machine learning systems, these sorts of pipelines are common, where you can have multiple modules--in this example, the text detection, character segmentation, character recognition modules--each of which may be machine learning component, or sometimes it may not be a machine learning component but to have a set of modules that act one after another on some piece of data in order to produce the output you want, which in the photo OCR example is to find the transcription of the text that appeared in the image. If you\'re designing a machine learning system one of the most important decisions will often be what exactly is the pipeline that you want to put together. In other words, given the photo OCR problem, how do you break this problem down into a sequence of different modules. And you design the pipeline and each the performance of each of the modules in your pipeline. will often have a big impact on the final performance of your algorithm. If you have a team of engineers working on a problem like this is also very common to have different individuals work on different modules. So I could easily imagine tech easily being the of anywhere from 1 to 5 engineers, character segmentation maybe another 1-5 engineers, and character recognition being another 1-5 engineers, and so having a pipeline like often offers a natural way to divide up the workload amongst different members of an engineering team, as well. Although, or course, all of this work could also be done by just one person if that\'s how you want to do it. In complex machine learning systems the idea of a pipeline, of a machine of a pipeline, is pretty pervasive. And what you just saw is a specific example of how a Photo OCR pipeline might work. In the next few videos I\'ll tell you a little bit more about this pipeline, and we\'ll continue to use this as an example to illustrate--I think--a few more key concepts of machine learning.'",109,0,1
coursera,stanford_university,machine-learning,sliding-windows,"b'In the previous video, we talked about the photo OCR pipeline and how that worked. In which we would take an image and pass the Through a sequence of machine learning components in order to try to read the text that appears in an image. In this video I like to. A little bit more about how the individual components of the pipeline works. In particular most of this video will center around the discussion. of whats called a sliding windows. The first stage of the filter was the Text detection where we look at an image like this and try to find the regions of text that appear in this image. Text detection is an unusual problem in computer vision. Because depending on the length of the text you\'re trying to find, these rectangles that you\'re trying to find can have different aspect. So in order to talk about detecting things in images let\'s start with a simpler example of pedestrian detection and we\'ll then later go back to. Ideas that were developed in pedestrian detection and apply them to text detection. So in pedestrian detection you want to take an image that looks like this and the whole idea is the individual pedestrians that appear in the image. So there\'s one pedestrian that we found, there\'s a second one, a third one a fourth one, a fifth one. And a one. This problem is maybe slightly simpler than text detection just for the reason that the aspect ratio of most pedestrians are pretty similar. Just using a fixed aspect ratio for these rectangles that we\'re trying to find. So by aspect ratio I mean the ratio between the height and the width of these rectangles. They\'re all the same. for different pedestrians but for text detection the height and width ratio is different for different lines of text Although for pedestrian detection, the pedestrians can be different distances away from the camera and so the height of these rectangles can be different depending on how far away they are. but the aspect ratio is the same. In order to build a pedestrian detection system here\'s how you can go about it. Let\'s say that we decide to standardize on this aspect ratio of 82 by 36 and we could have chosen some rounded number like 80 by 40 or something, but 82 by 36 seems alright. What we would do is then go out and collect large training sets of positive and negative examples. Here are examples of 82 X 36 image patches that do contain pedestrians and here are examples of images that do not. On this slide I show 12 positive examples of y1 and 12 examples of y0. In a more typical pedestrian detection application, we may have anywhere from a 1,000 training examples up to maybe 10,000 training examples, or even more if you can get even larger training sets. And what you can do, is then train in your network or some other learning algorithm to take this input, an MS patch of dimension 82 by 36, and to classify  \'y\' and to classify that image patch as either containing a pedestrian or not. So this gives you a way of applying supervised learning in order to take an image patch can determine whether or not a pedestrian appears in that image capture. Now, lets say we get a new image, a test set image like this and we want to try to find a pedestrian\'s picture image. What we would do is start by taking a rectangular patch of this image. Like that shown up here, so that\'s maybe a 82 X 36 patch of this image, and run that image patch through our classifier to determine whether or not there is a pedestrian in that image patch, and hopefully our classifier will return y equals 0 for that patch, since there is no pedestrian. Next, we then take that green rectangle and we slide it over a bit and then run that new image patch through our classifier to decide if there\'s a pedestrian there. And having done that, we then slide the window further to the right and run that patch through the classifier again. The amount by which you shift the rectangle over each time is a parameter, that\'s sometimes called the step size of the parameter, sometimes also called the slide parameter, and if you step this one pixel at a time. So you can use the step size or stride of 1, that usually performs best, that is more cost effective, and so using a step size of maybe 4 pixels at a time, or eight pixels at a time or some large number of pixels might be more common, since you\'re then moving the rectangle a little bit more each time. So, using this process, you continue stepping the rectangle over to the right a bit at a time and running each of these patches through a classifier, until eventually, as you slide this window over the different locations in the image, first starting with the first row and then we go further rows in the image, you would then run all of these different image patches at some step size or some stride through your classifier. Now, that was a pretty small rectangle, that would only detect pedestrians of one specific size. What we do next is start to look at larger image patches. So now let\'s take larger images patches, like those shown here and run those through the crossfire as well. And by the way when I say take a larger image patch, what I really mean is when you take an image patch like this, what you\'re really doing is taking that image patch, and resizing it down to 82 X 36, say. So you take this larger patch and re-size it to be smaller image and then it would be the smaller size image that is what you would pass through your classifier to try and decide if there is a pedestrian in that patch. And finally you can do this at an even larger scales and run that side of Windows to the end And after this whole process hopefully your algorithm will detect whether theres pedestrian appears in the image, so thats how you train a the classifier, and then use a sliding windows classifier, or use a sliding windows detector in order to find pedestrians in the image. Let\'s have a turn to the text detection example and talk about that stage in our photo OCR pipeline, where our goal is to find the text regions in unit. similar to pedestrian detection you can come up with a label training set with positive examples and negative examples with examples corresponding to regions where text appears. So instead of trying to detect pedestrians, we\'re now trying to detect texts. And so positive examples are going to be patches of images where there is text. And negative examples is going to be patches of images where there isn\'t text. Having trained this we can now apply it to a new image, into a test set image. So here\'s the image that we\'ve been using as example. Now, last time we run, for this example we are going to run a sliding windows at just one fixed scale just for purpose of illustration, meaning that I\'m going to use just one rectangle size. But lets say I run my little sliding windows classifier on lots of little image patches like this if I do that, what Ill end up with is a result like this where the white region show where my text detection system has found text and so the axis\' of these two figures are the same. So there is a region up here, of course also a region up here, so the fact that this black up here represents that the classifier does not think it\'s found any texts up there, whereas the fact that there\'s a lot of white stuff here, that reflects that classifier thinks that it\'s found a bunch of texts. over there on the image. What i have done on this image on the lower left is actually use white to show where the classifier thinks it has found text. And different shades of grey correspond to the probability that was output by the classifier, so like the shades of grey corresponds to where it thinks it might have found text but has lower confidence the bright white response to whether the classifier, up with a very high probability, estimated probability of there being pedestrians in that location. We aren\'t quite done yet because what we actually want to do is draw rectangles around all the region where this text in the image, so were going to take one more step which is we take the output of the classifier and apply to it what is called an expansion operator. So what that does is, it take the image here, and it takes each of the white blobs, it takes each of the white regions and it expands that white region. Mathematically, the way you implement that is, if you look at the image on the right, what we\'re doing to create the image on the right is, for every pixel we are going to ask, is it withing some distance of a white pixel in the left image. And so, if a specific pixel is within, say, five pixels or ten pixels of a white pixel in the leftmost image, then we\'ll also color that pixel white in the rightmost image. And so, the effect of this is, we\'ll take each of the white blobs in the leftmost image and expand them a bit, grow them a little bit, by seeing whether the nearby pixels, the white pixels, and then coloring those nearby pixels in white as well. Finally, we are just about done. We can now look at this right most image and just look at the connecting components and look at the as white regions and draw bounding boxes around them. And in particular, if we look at all the white regions, like this one, this one, this one, and so on, and if we use a simple heuristic to rule out rectangles whose aspect ratios look funny because we know that boxes around text should be much wider than they are tall. And so if we ignore the thin, tall blobs like this one and this one, and we discard these ones because they are too tall and thin, and we then draw a the rectangles around the ones whose aspect ratio thats a height to what ratio looks like for text regions, then we can draw rectangles, the bounding boxes around this text region, this text region, and that text region, corresponding to the Lula B\'s antique mall logo, the Lula B\'s, and this little open sign. Of over there. This example by the actually misses one piece of text. This is very hard to read, but there is actually one piece of text there. That says [xx] are corresponding to this but the aspect ratio looks wrong so we discarded that one. So you know it\'s ok on this image, but in this particular example the classifier actually missed one piece of text. It\'s very hard to read because there\'s a piece of text written against a transparent window. So that\'s text detection using sliding windows. And having found these rectangles with the text in it, we can now just cut out these image regions and then use later stages of pipeline to try to meet the texts. Now, you recall that the second stage of pipeline was character segmentation, so given an image like that shown on top, how do we segment out the individual characters in this image? So what we can do is again use a supervised learning algorithm with some set of positive and some set of negative examples, what were going to do is look in the image patch and try to decide if there is split between two characters right in the middle of that image match. So for initial positive examples. This first cross example, this image patch looks like the middle of it is indeed the middle has splits between two characters and the second example again this looks like a positive example, because if I split two characters by putting a line right down the middle, that\'s the right thing to do. So, these are positive examples, where the middle of the image represents a gap or a split between two distinct characters, whereas the negative examples, well, you know, you don\'t want to split two characters right in the middle, and so these are negative examples because they don\'t represent the midpoint between two characters. So what we will do is, we will train a classifier, maybe using new network, maybe using a different learning algorithm, to try to classify between the positive and negative examples. Having trained such a classifier, we can then run this on this sort of text that our text detection system has pulled out. As we start by looking at that rectangle, and we ask, ""Gee, does it look like the middle of that green rectangle, does it look like the midpoint between two characters?"". And hopefully, the classifier will say no, then we slide the window over and this is a one dimensional sliding window classifier, because were going to slide the window only in one straight line from left to right, theres no different rows here. There\'s only one row here. But now, with the classifier in this position, we ask, well, should we split those two characters or should we put a split right down the middle of this rectangle. And hopefully, the classifier will output y equals one, in which case we will decide to draw a line down there, to try to split two characters. Then we slide the window over again, optic process, don\'t close the gap, slide over again, optic says yes, do split there and so on, and we slowly slide the classifier over to the right and hopefully it will classify this as another positive example and so on. And we will slide this window over to the right, running the classifier at every step, and hopefully it will tell us, you know, what are the right locations to split these characters up into, just split this image up into individual characters. And so thats 1D sliding windows for character segmentation. So, here\'s the overall photo OCR pipe line again. In this video we\'ve talked about the text detection step, where we use sliding windows to detect text. And we also use a one-dimensional sliding windows to do character segmentation to segment out, you know, this text image in division of characters. The final step through the pipeline is the character qualification step and that step you might already be much more familiar with the early videos on supervised learning where you can apply a standard supervised learning within maybe on your network or maybe something else in order to take it\'s input, an image like that and classify which alphabet or which 26 characters A to Z, or maybe we should have 36 characters if you have the numerical digits as well, the multi class classification problem where you take it\'s input and image contained a character and decide what is the character that appears in that image? So that was the photo OCR pipeline and how you can use ideas like sliding windows classifiers in order to put these different components to develop a photo OCR system. In the next few videos we keep on using the problem of photo OCR to explore somewhat interesting issues surrounding building an application like this.'",110,0,1
coursera,stanford_university,machine-learning,getting-lots-of-data-and-artificial-data,"b""I've seen over and over that one of the most reliable ways to get a high performance machine learning system is to take a low bias learning algorithm and to train it on a massive training set. But where did you get so much training data from? Turns out that the machine earnings there's a fascinating idea called artificial data synthesis, this doesn't apply to every single problem, and to apply to a specific problem, often takes some thought and innovation and insight. But if this idea applies to your machine, only problem, it can sometimes be a an easy way to get a huge training set to give to your learning algorithm. The idea of artificial data synthesis comprises of two variations, main the first is if we are essentially creating data from [xx], creating new data from scratch. And the second is if we already have it's small label training set and we somehow have amplify that training set or use a small training set to turn that into a larger training set and in this video we'll go over both those ideas. To talk about the artificial data synthesis idea, let's use the character portion of the photo OCR pipeline, we want to take it's input image and recognize what character it is. If we go out and collect a large label data set, here's what it is and what it look like. For this particular example, I've chosen a square aspect ratio. So we're taking square image patches. And the goal is to take an image patch and recognize the character in the middle of that image patch. And for the sake of simplicity, I'm going to treat these images as grey scale images, rather than color images. It turns out that using color doesn't seem to help that much for this particular problem. So given this image patch, we'd like to recognize that that's a T. Given this image patch, we'd like to recognize that it's an 'S'. Given that image patch we would like to recognize that as an 'I' and so on. So all of these, our examples of row images, how can we come up with a much larger training set? Modern computers often have a huge font library and if you use a word processing software, depending on what word processor you use, you might have all of these fonts and many, many more Already stored inside. And, in fact, if you go different websites, there are, again, huge, free font libraries on the internet we can download many, many different types of fonts, hundreds or perhaps thousands of different fonts. So if you want more training examples, one thing you can do is just take characters from different fonts and paste these characters against different random backgrounds. So you might take this ----  and paste that c against a random background. If you do that you now have a training example of an image of the character C. So after some amount of work, you know this, and it is a little bit of work to synthisize realistic looking data. But after some amount of work, you can get a synthetic training set like that. Every image shown on the right was actually a synthesized image. Where you take a font, maybe a random font downloaded off the web and you paste an image of one character or a few characters from that font against this other random background image. And then apply maybe a little blurring operators  -----of app finder, distortions that app finder, meaning just the sharing and scaling and little rotation operations and if you do that you get a synthetic training set, on what the one shown here. And this is work, grade, it is, it takes thought at work, in order to make the synthetic data look realistic, and if you do a sloppy job in terms of how you create the synthetic data then it actually won't work well. But if you look at the synthetic data looks remarkably similar to the real data. And so by using synthetic data you have essentially an unlimited supply of training examples for artificial training synthesis And so, if you use this source synthetic data, you have essentially unlimited supply of label data to create a improvised learning algorithm for the character recognition problem. So this is an example of artificial data synthesis where youre basically creating new data from scratch, you just generating brand new images from scratch. The other main approach to artificial data synthesis is where you take a examples that you currently have, that we take a real example, maybe from real image, and you create additional data, so as to amplify your training set. So here is an image of a compared to a from a real image, not a synthesized image, and I have overlayed this with the grid lines just for the purpose of illustration. Actually have these ----. So what you can do is then take this alphabet here, take this image and introduce artificial warpings[sp?] or artificial distortions into the image so they can take the image a and turn that into 16 new examples. So in this way you can take a small label training set and amplify your training set to suddenly get a lot more examples, all of it. Again, in order to do this for application, it does take thought and it does take insight to figure out what our reasonable sets of distortions, or whether these are ways that amplify and multiply your training set, and for the specific example of character recognition, introducing these warping seems like a natural choice, but for a different learning machine application, there may be different the distortions that might make more sense. Let me just show one example from the totally different domain of speech recognition. So the speech recognition, let's say you have audio clips and you want to learn from the audio clip to recognize what were the words spoken in that clip. So let's see how one labeled training example. So let's say you have one labeled training example, of someone saying a few specific words. So let's play that audio clip here. 0 -1-2-3-4-5. Alright, so someone counting from 0 to 5, and so you want to try to apply a learning algorithm to try to recognize the words said in that. So, how can we amplify the data set? Well, one thing we do is introduce additional audio distortions into the data set. So here I'm going to add background sounds to simulate a bad cell phone connection. When you hear beeping sounds, that's actually part of the audio track, that's nothing wrong with the speakers, I'm going to play this now. 0-1-2-3-4-5. Right, so you can listen to that sort of audio clip and recognize the sounds, that seems like another useful training example to have, here's another example, noisy background. Zero, one, two, three four five you know of cars driving past, people walking in the background, here's another one, so taking the original clean audio clip so taking the clean audio of someone saying 0 1 2 3 4 5 we can then automatically synthesize these additional training examples and thus amplify one training example into maybe four different training examples. So let me play this final example, as well. 0-1 3-4-5 So by taking just one labelled example, we have to go through the effort to collect just one labelled example fall of the 01205, and by synthesizing additional distortions, by introducing different background sounds, we've now multiplied this one example into many more examples. Much work by just automatically adding these different background sounds to the clean audio Just one word of warning about synthesizing data by introducing distortions: if you try to do this yourself, the distortions you introduce should be representative the source of noises, or distortions, that you might see in the test set. So, for the character recognition example, you know, the working things begin introduced are actually kind of reasonable, because an image A that looks like that, that's, could be an image that we could actually see in a test set.Reflect a fact And, you know, that image on the upper-right, that could be an image that we could imagine seeing. And for audio, well, we do wanna recognize speech, even against a bad self internal connection, against different types of background noise, and so for the audio, we're again synthesizing examples are actually representative of the sorts of examples that we want to classify, that we want to recognize correctly. In contrast, usually it does not help perhaps you actually a meaning as noise to your data. I'm not sure you can see this, but what we've done here is taken the image, and for each pixel, in each of these 4 images, has just added some random Gaussian noise to each pixel. To each pixel, is the pixel brightness, it would just add some, you know, maybe Gaussian random noise to each pixel. So it's just a totally meaningless noise, right? And so, unless you're expecting to see these sorts of pixel wise noise in your test set, this sort of purely random meaningless noise is less likely to be useful. But the process of artificial data synthesis it is you know a little bit of an art as well and sometimes you just have to try it and see if it works. But if you're trying to decide what sorts of distortions to add, you know, do think about what other meaningful distortions you might add that will cause you to generate additional training examples that are at least somewhat representative of the sorts of images you expect to see in your test sets. Finally, to wrap up this video, I just wanna say a couple of words, more about this idea of getting loss of data via artificial data synthesis. As always, before expending a lot of effort, you know, figuring out how to create artificial training examples, it's often a good practice is to make sure that you really have a low biased crossfire, and having a lot more training data will be of help. And standard way to do this is to plot the learning curves, and make sure that you only have a low as well, high variance falsifier. Or if you don't have a low bias falsifier, you know, one other thing that's worth trying is to keep increasing the number of features that your classifier has, increasing the number of hidden units in your network, saying, until you actually have a low bias falsifier, and only then, should you put the effort into creating a large, artificial training set, so what you really want to avoid is to, you know, spend a whole week or spend a few months figuring out how to get a great artificially synthesized data set. Only to realize afterward, that, you know, your learning algorithm, performance doesn't improve that much, even when you're given a huge training set. So that's about my usual advice about of a testing that you really can make use of a large training set before spending a lot of effort going out to get that large training set. Second is, when i'm working on machine learning problems, one question I often ask the team I'm working with, often ask my students, which is, how much work would it be to get 10 times as much date as we currently had. When I face a new machine learning application very often I will sit down with a team and ask exactly this question, I've asked this question over and over and over and I've been very surprised how often this answer has been that. You know, it's really not that hard, maybe a few days of work at most, to get ten times as much data as we currently have for a machine running application and very often if you can get ten times as much data there will be a way to make your algorithm do much better. So, you know, if you ever join the product team working on some machine learning application product this is a very good questions ask yourself ask the team don't be too surprised if after a few minutes of brainstorming if your team comes up with a way to get literally ten times this much data, in which case, I think you would be a hero to that team, because with 10 times as much data, I think you'll really get much better performance, just from learning from so much data. So there are several waysand that comprised both the ideas of generating data from scratch using random fonts and so on. As well as the second idea of taking an existing example and and introducing distortions that amplify to enlarge the training set A couple of other examples of ways to get a lot more data are to collect the data or to label them yourself. So one useful calculation that I often do is, you know, how many minutes, how many hours does it take to get a certain number of examples, so actually sit down and figure out, you know, suppose it takes me ten seconds to label one example then and, suppose that, for our application, currently we have 1000 labeled examples examples so ten times as much of that would be if n were equal to ten thousand. A second way to get a lot of data is to just collect the data and you label it yourself. So what I mean by this is I will often set down and do a calculation to figure out how much time, you know just like how many hours will it take, how many hours or how many days will it take for me or for someone else to just sit down and collect ten times as much data, as we have currently, by collecting the data ourselves and labeling them ourselves. So, for example, that, for our machine learning application, currently we have 1,000 examples, so M 1,000. That what we do is sit down and ask, how long does it take me really to collect and label one example. And sometimes maybe it will take you, you know ten seconds to label one new example, and so if I want 10 X as many examples, I'd do a calculation. If it takes me 10 seconds to get one training example. If I wanted to get 10 times as much data, then I need 10,000 examples. So I do the calculation, how long is it gonna take to label, to manually label 10,000 examples, if it takes me 10 seconds to label 1 example. So when you do this calculation, often I've seen many you would be surprised, you know, how little, or sometimes a few days at work, sometimes a small number of days of work, well I've seen many teams be very surprised that sometimes how little work it could be, to just get a lot more data, and let that be a way to give your learning app to give you a huge boost in performance, and necessarily, you know, sometimes when you've just managed to do this, you will be a hero and whatever product development, whatever team you're working on, because this can be a great way to get much better performance. Third and finally, one sometimes good way to get a lot of data is to use what's now called crowd sourcing. So today, there are a few websites or a few services that allow you to hire people on the web to, you know, fairly inexpensively label large training sets for you. So this idea of crowd sourcing, or crowd sourced data labeling, is something that has, is obviously, like an entire academic literature, has some of it's own complications and so on, pertaining to labeler reliability. Maybe, you know, hundreds of thousands of labelers, around the world, working fairly inexpensively to help label data for you, and that I've just had mentioned, there's this one alternative as well. And probably Amazon Mechanical Turk systems is probably the most popular crowd sourcing option right now. This is often quite a bit of work to get to work, if you want to get very high quality labels, but is sometimes an option worth considering as well. If you want to try to hire many people, fairly inexpensively on the web, our labels launch miles of data for you. So this video, we talked about the idea of artificial data synthesis of either creating new data from scratch, looking, using the ramming funds as an example, or by amplifying an existing training set, by taking existing label examples and introducing distortions to it, to sort of create extra label examples. And finally, one thing that I hope you remember from this video this idea of if you are facing a machine learning problem, it is often worth doing two things. One just a sanity check, with learning curves, that having more data would help. And second, assuming that that's the case, I will often seat down and ask yourself seriously: what would it take to get ten times as much creative data as you currently have, and not always, but sometimes, you may be surprised by how easy that turns out to be, maybe a few days, a few weeks at work, and that can be a great way to give your learning algorithm a huge boost in performance""",111,0,1
coursera,stanford_university,machine-learning,ceiling-analysis-what-part-of-the-pipeline-to-work-on-next,"b""In earlier videos,\nI've said over and over that, when you're developing a machine learning\nsystem, one of the most valuable resources is your time as the developer,\nin terms of picking what to work on next. Or, if you have a team of developers or a team of engineers working together\non a machine learning system. Again, one of the most valuable resources\nis the time of the engineers or the developers working on the system. And what you really want\nto avoid is that you or your colleagues your friends spend a lot\nof time working on some component. Only to realize after weeks or\nmonths of time spent, that all that worked just doesn't make a huge difference on\nthe performance of the final system. In this video what I'd like to do is\nsomething called ceiling analysis. When you're the team working on\nthe pipeline machine on your system, this can sometimes give you a very\nstrong signal, a very strong guidance on what parts of the pipeline might be\nthe best use of your time to work on. To talk about ceiling analysis I'm\ngoing to keep on using the example of the photo OCR pipeline. And see right here each of these boxes,\ntext detection, character segmentation, character recognition, each of these boxes can have even\na small engineering team working on it. Or maybe the entire system is\njust built by you, either way. But the question is where\nshould you allocate resources? Which of these boxes is most worth\nyour effort of trying to improve the performance of. In order to explain the idea\nof ceiling analysis, I'm going to keep using the example\nof our photo OCR pipeline. As I mentioned earlier, each of these\nboxes here, each of these machines and components could be the work of\na small team of engineers, or the whole system could be\nbuilt by just one person. But the question is, where should\nyou allocate scarce resources? That is, which of these components, which\none or two or maybe all three of these components is most worth your time,\nto try to improve the performance of. So here's the idea of ceiling analysis. As in the development process for\nother machine learning systems as well, in order to make decisions on what\nto do for developing the system is going to be very helpful to have a single\nrolled number evaluation metric for this learning system. So let's say we pick\ncharacter level accuracy. So if you're given a test set image,\nwhat is the fraction of alphabets or characters in a test image\nthat we recognize correctly? Or you can pick some other single road\nnumber evaluation that you could, if you want. But let's say for\nwhatever evaluation measure we pick, we find that the overall system\ncurrently has 72% accuracy. So in other words,\nwe have some set of test set images. And from each test set images,\nwe run it through text detection, then character segmentation,\nthen character recognition. And we find that on our test set the\noverall accuracy of the entire system was 72% on whatever metric you chose. Now here's the idea behind ceiling\nanalysis, which is that we're going to go through, let's say the first module of our\nmachinery pipeline, say text detection. And what we're going to do, is we're\ngoing to monkey around with the test set. We're gonna go to the test set. For every test example, which is going\nto provide it the correct text detection outputs, so in other words, we're going to\ngo to the test set and just manually tell the algorithm where the text is\nin each of the test examples. So in other words gonna simulate\nwhat happens if you have a text detection system with\na hundred percent accuracy, for the purpose of detecting text in an image. And really the way you do\nthat's pretty simple, right? Instead of letting your learning\nalgorhtim detect the text in the images. You wouldn't say go to the images and just manually label what is the location\nof the text in my test set image. And you would then let these correct or let these ground truth labels of where\nis the text be part of your test set. And just use these ground truth\nlabels as what you feed in to the next stage of the pipeline, so\nthe character segmentation pipeline. Okay?\nSo just to say that again. By putting a checkmark over here, what I mean is I'm going to go to my test\nset and just give it the correct answers. Give it the correct labels for\nthe text detection part of the pipeline. So that as if I have a perfect test\ndetection system on my test set. What we need to do then is run this\ndata through the rest of the pipeline. Through character segmentation and\ncharacter recognition. And then use the same\nevaluation metric as before, to measure what was the overall\naccuracy of the entire system. And with perfect text detection,\nhopefully the performance will go up. And in this example, it goes up by by 89%. And then we're gonna keep going, let's\ngot o the next stage of the pipeline, so character segmentation. So again, I'm gonna go to my test set,\nand now I'm going to give it the correct text detection output and give it\nthe correct character segmentation output. So go to the test set and manually label\nthe correct segmentations of the text into individual characters,\nand see how much that helps. And let's say it goes up to 90%\naccuracy for the overall system. Right? So as always the accuracy\nof the overall system. So is whatever the final output of\nthe character recognition system is. Whatever the final output\nof the overall pipeline, is going to measure the accuracy of that. And finally I'm going to build a character\nrecognition system and give that correct labels as well, and if I do that too then\nno surprise I should get 100% accuracy. Now the nice thing about having done this\nanalysis is, we can now understand what is the upside potential of improving\neach of these components? So we see that if we get\nperfect text detection, our performance went up from 72 to 89%. So that's a 17% performance gain. So this means that if we take our current\nsystem we spend a lot of time improving text detection, that means that we could potentially\nimprove our system's performance by 17%. It seems like it's well worth our while. Whereas in contrast, when going from text detection when we\ngave it perfect character segmentation, performance went up only by 1%, so\nthat's a more sobering message. It means that no matter how much time\nyou spend on character segmentation. Maybe the upside potential is going to be\npretty small, and maybe you do not want to have a large team of engineers\nworking on character segmentation. This sort of analysis shows that even\nwhen you give it the perfect character segmentation, you performance\ngoes up by only one percent. That really estimates what is the ceiling,\nor what is an upper bound on how much you can improve the performance of your system\nand working on one of these components. And finally, going from character, when we get better character recognition\nwith the forms went up by ten percent. So again you can decide is ten percent\nimprovement, how much is worth your while? This tells you that maybe with more effort\nspent on the last stage of the pipeline, you can improve the performance\nof the systems as well. Another way of thinking about this, is that by going through these sort\nof analysis you're trying to think about what is the upside potential of\nimproving each of these components. Or how much could you\npossibly gain if one of these components became\nabsolutely perfect? And this really places an upper bound\non the performance of that system. So the idea of ceiling analysis is pretty\nimportant, let me just answer this idea again but with a different example but\nmore complex one. Let's say that you want to do\nface recognition from images. You want to look at the picture and\nrecognize whether or not the person in this picture is\na particular friend of yours, and try to recognize the person\nShown in this image. This is a slightly artificial example, this isn't actually how face\nrecognition is done in practice. But we're going to set for an example,\nwhat a pipeline might look like to give you another example of how\na ceiling analysis process might look. So we have a camera image, and let's say\nthat we design a pipeline as follows, the first thing you wanna do is\npre-processing of the image. So let's take this image like we\nhave shown on the upper right, and let's say we want to\nremove the background. So do pre-processing and\nthe background disappears. Next we want to say detect\nthe face of the person, that's usually done on the learning So we'll run a sliding Windows crossfire\nto draw a box around a person's face. Having detected the face, it turns out\nthat if you want to recognize people, it turns out that the eyes\nis a highly useful cue. We actually are, in terms of recognizing\nyour friends the appearance of their eyes is actually one of the most\nimportant cues that you use. So lets run another crossfire to\ndetect the eyes of the person. So the segment of the eyes and then since this will give us useful\nfeatures to recognize the person. And then other parts of\nthe face of physical interest. Maybe segment of the nose,\nsegment of the mouth. And then having found the eyes, the nose,\nand the mouth, all of these give us useful features to maybe feed into\na logistic regression classifier. And there's a job with a cost priority,\nthey'd give us the overall label, to find the label for who we think\nis the identity of this person. So this is a kind of complicated pipeline,\nit's actually probably more complicated than you should be using if you actually\nwant to recognize people, but there's an illustrative example that's useful\nto think about for ceiling analysis. So how do you go through ceiling\nanalysis for this pipeline. Well se step through these\npieces one at a time. Let's say your overall\nsystem has 85% accuracy. The first thing I do is\ngo to my test set and manually give it the full\nbackground segmentation. So manually go to the test set. And use Photoshop or something to just\ntell it where's the background and just manually remove the graph background,\nso this is a ground true background, and see how much the accuracy changes. In this example the accuracy\ngoes up by 0.1%. So this is a strong sign that even if you\nhave perfect background segmentation, the form is, even with perfect\nbackground removal the performance or your system isn't going\nto go up that much. So it's maybe not worth a huge\neffort to work on pre-processing on background removal. Then quickly goes to test set give\nit the correct face detection images then again step though the eyes nose and mouth segmentation in some\norder just pick one order. Just give the correct\nlocation of the eyes. Correct location in noses,\ncorrect location in mouth, and then finally if I just give it the correct\noverall label I can get 100% accuracy. And so as I go through the system and\njust give more and more components, the correct labels in the test set, the\nperformance of the overall system goes up and you can look at how much the\nperformance went up on different steps. So from giving it\nthe perfect face detection, it looks like the overall performance\nof the system went up by 5.9%. So that's a pretty big jump. It means that maybe it's worth quite\na bit effort on better face detection. Went up 4% there, it went up 1% there. 1% there, and 3% there. So it looks like the components\nthat most work are while are, when I gave it perfect face\ndetection system went up by 5.9 performance when given perfect eyes\nsegmentation went to four percent. And then my final which is cost for\nwell there's another three percent, gap there maybe. And so this tells maybe whether the\ncomponents are most worthwhile working on. And by the way I want to tell\nyou a true cautionary story. The reason I put this is\nin this in preprocessing background removal is because I actually\nknow of a true story where there was a research team that actually literally\nhad to people spend about a year and a half, spend 18 months working\non better background removal. But actually I'm obscuring the details for\nobvious reasons, but there was a computer vision application where there's a team of\ntwo engineers that literally spent about a year and a half working on better\nbackground removal, actually worked out really complicated algorithms and\nended up publishing one research paper. But after all that work they found that\nit just did not make huge difference to the overall performance of the actual\napplication they were working on and if only someone were to do\nceiling analysis before hand maybe they could have realized. And one of them said to me afterward. If only you've did this sort of analysis\nlike this maybe they could have realized before their 18 months of work. That they should have spend their effort\nfocusing on some different component then literally spending 18 months\nworking on background removal. So to summarize, pipelines are pretty pervasive in\ncomplex machine learning applications. And when you're working on a big\nmachine learning application, your time as developer is so\nvaluable, so just don't waste your time working on something that\nultimately isn't going to matter. And in this video we'll talk about\nthis idea of ceiling analysis, which I've often found to be a very good\ntool for identifying the component of a video as you put focus on that\ncomponent and make a big difference. Will actually have a huge effect on the\noverall performance of your final system. So over the years working machine\nlearning, I've actually learned to not trust my own gut feeling\nabout what components to work on. So very often, I've work on machine\nlearning for a long time, but often I look at a machine learning problem, and\nI may have some gut feeling about oh, let's jump on that component and\njust spend all the time on that. But over the years, I've come to\neven trust my own gut feelings and learn not to trust gut feelings that much. And instead, if you have a sort of machine\nlearning problem where it's possible to structure things and do a ceiling\nanalysis, often there's a much better and much more reliable way for\ndeciding where to put a focused effort, to really improve the performance\nof some component. And be kind of reassured that,\nwhen you do that, it won't actually have a huge effect on the final\nperformance of the overall system.""",112,0,1
coursera,stanford_university,machine-learning,summary-and-thank-you,"b""Welcome to the final video of this Machine Learning class. We've been through a lot of different videos together. In this video I would like to just quickly summarize the main topics of this course and then say a few words at the end and that will wrap up the class. So what have we done? In this class we spent a lot of time talking about supervised learning algorithms like linear regression, logistic regression, neural networks, SVMs. for problems where you have labelled data and labelled examples like x(i), y(i) And we also spent quite a lot of time talking about unsupervised learning like K-means clustering, Principal Components Analysis for dimensionality reduction and Anomaly Detection algorithms for when you have only unlabelled data x(i) Although Anomaly Detection can also use some labelled data to evaluate the algorithm. We also spent some time talking about special applications or special topics like Recommender Systems and large scale machine learning systems including parallelized and rapid-use systems as well as some special applications like sliding windows object classification for computer vision. And finally we also spent a lot of time talking about different aspects of, sort of, advice on building a machine learning system. And this involved both trying to understand what is it that makes a machine learning algorithm work or not work. So we talked about things like bias and variance, and how regularization can help with some variance problems. And we also spent a little bit of time talking about this question of how to decide what to work on next. So, how to prioritize how you spend your time when you're developing a machine learning system. So we talked about evaluation of learning algorithms, evaluation metrics like  precision recall, F1 score as well as practical aspects of evaluation like the training, cross-validation and test sets. And we also spent a lot of time talking about debugging learning algorithms and making sure the learning algorithm is working. So we talked about diagnostics like learning curves and also talked about things like error analysis and ceiling analysis. And so all of these were different tools for helping you to decide what to do next and how to spend your valuable time when you're developing a machine learning system. And in addition to having the tools of machine learning at your disposal so knowing the tools of machine learning like supervised learning and unsupervised learning and so on, I hope that you now not only have the tools, but that you know how to apply these tools really well to build powerful machine learning systems. So, that's it. Those were the topics of this class and if you worked all the way through this course you should now consider yourself an expert in machine learning. As you know, machine learning is a technology that's having huge impact on science, technology and industry. And you're now well qualified to use these tools of machine learning to great effect. I hope that many of you in this class will find ways to use machine learning to build cool systems and cool applications and cool products. And I hope that you find ways to use machine learning not only to make <i>your</i> life better but maybe someday to use it to make many other people's life better as well. I also wanted to let you know that this class has been great fun for me to teach. So, thank you for that. And before wrapping up, there's just one last thing I wanted to say. Which is that: It was maybe not so long ago, that I was a student myself. And even today, you know, I still try to take different courses when I have time to try to learn new things. And so I know how time-consuming it is to learn this stuff. I know that you're probably a busy person with many, many other things going on in your life. And so the fact that you still found the time or took the time to watch these videos and, you know, many of these videos just went on for hours, right? And the fact many of you took the time to go through the review questions and that many of you took the time to work through the programming exercises. And these were long and complicate programming exercises. I wanted to say thank you for that. And I know that many of you have worked hard on this class and that many of you have put a lot of time into this class, that many of you have put a lot of yourselves into this class. So I hope that you also got a lot of out this class. And I wanted to say: Thank you very much for having been a student in this class.""",113,0,1
coursera,university_of_london,uol-machine-learning-for-all,introduction-computers-that-see,"b""You're standing at a crowded railway\nstation waiting for a friend. As you wait,\nhundreds of people pass you by. Each one looks different, but when your friend arrives you have no\nproblem picking her out of the crowd. Recognizing people's faces is something\nwe humans do effortlessly, but how would you program a computer\nto recognize a person? You could try to make a set of rules. For example, your friend has long\nblack hair and brown eyes, but that could describe literally\nbillions of people. What is it about her that\nyou actually recognize? Something about the shape of her nose or\nthe curve of her chin? But can you put it into words? The truth is that we can recognize people\nwithout ever really knowing how we do it. We cannot describe every detail\nof how we recognize someone. We just know how to do it. The trouble is that to program a computer, we need to break the task\ndown into its little details. That makes it very difficult or even impossible to program\na computer to recognize faces. Face recognition is an example of\na task that people find very easy, but that is very very hard for computers. These tasks are often called\nartificial intelligence or AI. You're now going to learn about the type\nof computing technique that can solve a lot of these AI problems and that's revolutionising what computers can\ndo that technique is machine learning.""",1,0,1
coursera,university_of_london,uol-machine-learning-for-all,artificial-intelligence,"b'Artificial intelligence\nis the area of Computer Science that\ntries to make machines that can replicate\nhuman intelligence to do tasks that humans can do, but if historically been\nvery difficult for machines. That doesn\'t necessarily\nmean things that we would call intelligent in humans. We\'d normally think that\nsomeone would need to be very intelligent to play chess\nto Grandmaster level. but a computer beat the world chess champion\nGarry Kasparov in the 1990s. On the other hand, tasks that almost any\nhealthy human could do effortlessly are still incredibly difficult\nchallenges for AI. Like picking up a glass of\nwater without spilling it, or having a conversation\nabout how your day was. In fact, having an ordinary\nconversation is one of the classic AI\nchallenges enshrined in the Turing test invented\nby Alan Turing. Turing was not only one of the original inventors\nof the computer and responsible for breaking the Nazi codes in World War II. He also invented the\nbasic ideas of AI. He asked himself,\n""How would it be possible to decide if a computer\nwas intelligent or not?"" His idea was based on a popular\nparty game at the time. A woman and a man leave the\nroom and the players can send questions to each of\nthem on pieces of paper. They have to guess based\nonly on the written answers, which is the man and\nwhich is the woman. Why not play this game but\ninstead of a man and a woman, you have to guess between\na human and a computer. Modern versions of the\ntest involved chatting to a computer or a human via\na text message interface, and trying to guess\nwhich is which. No computer has managed to fool most people over a\nlong period of time, but there are systems that fool people in short conversations. Although the Turing test is\npopular there are flaws, first the successful\nchatbots as they\'re called, often you simple tricks to fool people often showing\ntrue intelligence. Also, there are many\nchallenging areas of AI that aren\'t\ncovered in the test, like recognizing\nfaces, manipulating physical objects or\neven driving cars. In fact, AI is split\ninto a wide wide of sub-fields that address particular challenges\nlike computer vision, computers that can\nsee and understand. Natural language processing, computers that understand\nhuman language. Robotics and even\ncomputational creativity. Computers that can make\ncreative works of arts. Many of these sub-fields\nhave resulted in computer systems that can do some incredibly useful things. Robots that can run factories. The Google search engine, which is based on sophisticated natural\nlanguage processing, and computers that\ncan even diagnose diseases from looking\nat X-ray scans. We\'re likely to see a\nlot of major advances in this type of task-specific\nAI in the near future, but many AI researchers\nregard this as narrow AI. That it\'s good as individual\ntasks but does not have the general task independent intelligence that humans have. This artificial\ngeneral intelligence is still a long way\nfrom being achievable. So how does AI work? There have been many\ndifferent techniques used over the years, most with only limited success. Often they would work\non simple lab problems, but fail to scale up to the complexities\nof the real-world. Early systems were\nbased on logical rules. For example, expert\nsystems were type of AI system that tried to do things that expert\nhumans can do. For example,\ndiagnosing a disease. In this case, there might be a rule saying that\nif a patient has pain in the lower abdomen\nthey have appendicitis. One of the big problems is that these rules are rarely certain. There are many possible ways\ninto lower abdomen pain. Gradually, researchers\ndeveloped ways of dealing with this uncertainty like the mathematics of probability, and expert systems had reasonable success\nin several domains. But how would you create an expert system for\nrecognizing a face? What are the rules? We humans can do it but we\ndon\'t know how we do it. Building expert systems relied on experts knowing the rules\nthey use to make decisions, but very often, they\nare subconscious, we don\'t quite know\nhow we do things. This is one of the many reasons that most researchers started using a different approach\ncalled machine learning.'",2,0,1
coursera,university_of_london,uol-machine-learning-for-all,machine-learning,"b'How do we get the computer\nto understand images, to recognize faces, or tell the difference between\ncats and dogs? How can we program a computer to do it if we don\'t know\nhow to do it ourselves? The short answer is, we can\'t. But a new approach called\nMachine Learning is radically changing how we create software to\nsolve these problems. Instead of programming\na computer by telling it every detail\nof how to do a task, we teach it by giving it\nexamples of what to do. If we want the computer to tell the difference\nbetween cats and dogs, we can show it lots of pictures of cats and lots of pictures of dogs and it can learn\nto tell the difference. But wait, isn\'t that just magic? Have computers become so intelligent they can\nlearn just like us? Actually no, it\'s\nall just statistics. Machine Learning uses\nstatistical algorithms to learn from examples. We call these examples data and we say that the\ncomputer learns from data. These algorithms are\noften surprisingly simple but they can handle\na lot of example data. Simple algorithms can perform incredibly well if they\nhave enough examples. The most popular method of moment is deep neural networks, often called deep learning. They\'re cutting edge methods but they aren\'t particularly new, neural networks were\ninvented in the 1940s. The methods used in\ndeep learning are basically the same as\nthose used in the 1980s. So why if they suddenly\nstarted working a lot better? We have a lot more\ndata as well as the computer power\nto handle that data. Google\'s research director\nand AI pioneer, Peter Norvig, is quoted as saying, ""We don\'t have better algorithms, we just have more data."" Let\'s look in a bit more detail at how Machine Learning works. Machine Learning\nis about creating statistical programs\ncalled models. A model takes an input and\ngives you back an output. The input could be a picture and the output could be\nthe words cat or dog. The model is created based\non lots of example data. Each example includes both\nan input and an output. So it could be a picture\nof a cat together with the label cat or a picture\nof a dog with a labeled dog. A Machine Learning\nalgorithm takes the examples and uses\nthem to train the model. This means that they adapt the details of the\nmodel so that it maps the inputs in the example data to the corresponding outputs. So it maps the top image in the example data to cat\nand the bottom one to dog. Once the model has been trained, you can give it new input, a picture without a label, and it will tell you the output. The great thing\nabout this is that it\'s really generally applicable. The input is just a bunch\nof numbers but anything that\'s represented\non a computer is basically represented\nas a bunch of numbers. So the input can be almost anything you can\nrepresent in a computer, photographs, speech, music,\nweb pages, bank accounts, social media profiles, DNA\nsequences, legal decisions, disease symptoms, news stories, astronomical data, or\ncute kitten videos. Almost anything you do on a computer could have\nMachine Learning applied to it and the basic techniques are\npretty much the same. A couple of years ago,\nI decided that I should learn about natural\nlanguage processing, the competing techniques for understanding human\nspeech in writing. I thought I\'d have to learn a lot of new and difficult techniques, but the first chapter of the book I read was about how\nto represent words as input to Machine Learning\nand the rest of the book was just the same Machine\nLearning techniques I\'ve been using for years\nin other domains. The outputs can also be\nlots of different things. In the most common type\nof Machine Learning, the output is one of a number of categories called classes. For example, cat, dog, or rabbit or for medical cases it could be\nthe name of the disease or for face recognition it could be the name of the person\nwho is recognized. There are many\ndifferent tasks that boil down to putting\nthings into categories, so classification is common. All the examples we will use in this course will\nbe classification. But there were lots of\nother possible outputs. If the output is a number or several numbers like how\nserious a disease is, how emotional some music is, or how cute a kitten is\nwe call it regression. The output can even\nbe new examples. Machine Learning can\ncreate new images, music or synthetic speech in which case we call it\na generative model. Whenever we train a model on examples of input and output, we call it supervised learning. But sometimes, we don\'t know\nwhat the right output is. This is very common in science, we might have a lot\nof DNA sequences and we want to know what\ndifferent types exist. In this case, we might give the learning\nalgorithm a set of inputs on their own and get it to figure\nout the categories, this is called\nunsupervised learning. A final type of learning\nworks differently. Rather than getting examples, reinforcement learning, works much more like\ntraining a dog. The algorithm does things and gets rewards for\ndoing good things like a dog getting a treat and gets punishments for\ndoing the wrong thing. This is great for playing games. The model gets\nrewards for winning games and punishment\nfor loosing them. AlphaGo, the software that build the world champion\nat the game go, used reinforcement\nlearning to learn which strategies\nwere good based on whether it won or lost games. So there are many\ndifferent approaches to Machine Learning but all of these different\ntypes of learning use statistical algorithms and data. In this course, you\'ll\nlearn about supervised classification but many of the basic ideas apply to all\ntypes of Machine Learning.'",3,0,1
coursera,university_of_london,uol-machine-learning-for-all,machine-learning-algorithms,"b""How does machine learning actually work? It can sound a bit like magic,\nyou give the computer some examples and it magically learns how\nto do the right thing. But it isn't Magic, machine learning is\nbased on algorithms that are normally quite stupid, but can do some incredible\nthings if you give them enough data. Most machine learning algorithms\nare based on statistics and could be quite mathematical and complex. Part of the philosophy of this course is\nthat you don't need to understand all the details of the algorithm\nto do machine learning. But in this lecture, I'd like to give\nyou a sense of how some algorithms work. So it doesn't seem too mysterious to you. For example, you have lots of pictures\nof cats and lots of pictures of dogs. You're given a new picture and\nhave to decide if it's a cat or a dog. How can you do it? One simple way of doing this is to look\nthrough all the original examples and find the one that's most similar to\nthe new picture and use its class. It's a very simple method but\ncan actually work quite well. In fact, it's an established algorithm\nwith its own name nearest neighbor. It's called that because you're\nclassifying a picture based on the nearest example,\nwhich we call its neighbor. It works as an algorithm,\nbut it can be improved. For example, there are times when a dog picture\nmight look a lot like a cat picture. In that case,\nthe nearest neighbor will be wrong. We can never completely\neliminate errors like that. But one way we can reduce them is\nto use more than one neighbor. We could use the three\nclosest examples and choose the class that most of them have. We normally name the number of\nneighbors with the letter K and call the algorithm K Nearest Neighbors or\nKNN. One Nearest Neighbor is\nhardly ever used but K Nearest Neighbors is actually a very\ncommonly used machine learning algorithm. Another issue with nearest neighbor\nis what we mean by near or similar. The algorithm needs a mathematical\ndefinition of similarity that will give a number rating how\nsimilar two items are. For example, for pictures, you might\nmeasure how different each pixel is and add up all the results to get a score. But as we'll see in future videos, using\nindividual pixels often doesn't work well. A lot of the work of using nearest\nneighbors is about having a good measure of similarity which internally means\nhaving a good representation of pictures or whatever we are trying to classify. This is something we'll look at\nin the video on data features. In fact, the machine learning software\nyou'll use in this course is based on K Nearest Neighbors together with a really\nsophisticated measure of similarity. Another problem with nearest\nneighbors is that it can be slow, you have search through every single\nexample to find the most similar. One solution is to select only\na small number of important examples to compare with and throw the rest away. Comparing with the small number of\nimportant examples together with a very sophisticated similarity measure and\na bunch of other clever stuff is the basis of a method called\nSupport Vector Machines. Which is state of the art when\nI started machine learning and is still very important. Another approach is to not use\nthe original examples directly, but to use them to create a mathematical\nfunction to do the classification. This function is called a Model. Let's look at a simple example,\ndogs are bigger than cats so we can try to classify based on size, assuming we can calculate size from\nan image which is admittedly pretty hard. We could classify all animals\nabove a certain size as dogs and below that size as cats. This is a very simple mathematical\nfunction that can do classification, comparing size with a certain\nvalue that we call the threshold. How do we know what to use as a threshold? We can do it based on the data. We choose the value that gives us\nthe smallest number of incorrect classifications. This is an example of\na process called optimization. Optimization means choosing the numbers\nthat we use in our models so that they give us the best\nresults on the training data. In this case, we're choosing the threshold the size\nabove which we classify animals as dogs. Most machine learning algorithms\nthat use some form of optimization, classifying cats and dogs based\nonly on size won't work very well. It'll do okay for most dogs, but fail for\ntiny chihuahuas little puppies and fat cats. But we can do a lot better if we combine\ntogether other features, for example, we could use ear shape and nose length. One method called Decision Trees\ncombines many simple decisions on individual features. The algorithm performs one\ndecision after another, choosing features to use based\non the previous decision. It might first make\na decision based on size and then split up the small\nanimals based on ear shape. The features to use,\nthe order in which they used, and the threshold values are all learned\nfrom data using optimization. Other algorithms like neural\nnetworks combined many features together into complex\nmathematical functions and only make one decision based in\nthe output of that function. The details of the function are also\nlearned from data using optimization. So most machine learning algorithms are\nbased on combining together many features of the data into mathematical\nfunctions called models. The details of these models\nare chosen automatically, so they give the best results on the training\ndata, a process called optimization. The details of the algorithms vary a lot,\nbut having this basic understanding of how they work in general will\ngive you a good foundation for understanding how to do machine learning.""",4,0,1
coursera,university_of_london,uol-machine-learning-for-all,interview-with-machine-learning-experts,"b""Hello, you've been\nlearning a lot about machine learning and you've\ngot the basic principles. But today I'd like to broaden\nout a bit of your learning by inviting a panel of\nexperts from Goldsmiths, each of whom does some unique approaches\nto Machine Learning. So let's start by\nintroducing yourselves. Could you each tell our learners who you are and a little bit\nabout what you do. Sure. I'll start. I'm\nRebecca Fiebrink, I'm reader here at Goldsmiths and a lot of my research\nand teaching at Goldsmiths centers\naround creative uses of technology and especially creative uses of\nMachine Learning. My name is Larisa Saldatova. I'm also reader in Goldsmiths. I've been involved to\nMachine Learning Projects for two decades. I also lead the Online\nData Science Program here in Goldsmiths. My name is Jamie Ward and I'm a lecturer here\nat Goldsmiths. I've researched\nwearable computing like human activity recognition using sensors on those\nwearable computers. We use machine learning to\ntry and figure out what it is people are doing or who they're engaging\nwith and so forth. Okay. Great, thank\nyou. We've been learning about how\nmachine learning works, how it's based on data and the different types of machine learning problems you can have, classification problems,\nregression problems. Could you tell us each\nof you a little bit about machine learning problem that you've been involved in, what kind of data\nyou were using in, what you were trying to do and whether it was a classification, regression or something else. So I work with\nbiomedical applications and many of them include\nthe drug design. So your data about the chemical compounds and if they show some\nactivity or not. The pharmaceutical companies are using millions of compounds, so they screen it in\nparallel so they're are very expensive simply\nbecause we don't have this millions to spent on chemicals so we\nhave to be clever. So they use great interest in machine learning techniques, it called active\nMachine Learning. When system can infer what the chemicals will bring maximum information\nto train your model. So it tells you, so this 100 and then you go and buy only\nthat instead of millions. Yes, it works quite well. So the machine\nlearning algorithms is actually telling you\nwhat data to gather? Yes, it tells you not only\nto gather but it was also directing us what to buy\nand how to save money. Yes. I'll go next. So for about\n10 years I've been working with musicians and artists who are really interested in\nusing supervised learning. So both classification\nand regression, to build things like new\nmusical instruments. So an example might\nbe to move around using sensors worn on the body or I've got an example of one of the sensors that might be used. This as a Leap Motion which gets information about where\nyour hand is in space, and then you can\nbuild a classifier to classify whether\nsomebody is doing one action or another and if you want to build a\nmusic system from that, your classifier could\ntrigger the playback of one sound sample when\nyou're doing one thing and another sound sample when\nyou're doing another thing. So there's all sorts of different sensors that you could use. You could also get data from microphones, or from cameras, or from other data sources and of course use that to\ncontrol sound synthesis, use it to control live visuals or lighting in a performance, or even use it to understand\nsomething about say how a dancer is moving or how a musician is playing\ntheir instrument. So what's the benefits\nover just using the traditional knobs\nand buttons interface? Well, there's a lot of benefits. So the most obvious benefit to somebody who's begun to\nstudy machine learning, I think is that machine learning can just make it easier to deal with data that might\nbe complicated or in situations where you\nmight have a lot of data. So for instance with\nsomething like a Leap Motion, if I'm even just taking the\nXYZ position of each of my fingertips over this device, if I want to use that\nto control maybe 10 parameters of a visualization, or animation, or sound, suddenly I have a really\ncomplicated programming problem to do and Machine\nLearning makes it quite easy to instead of me sitting down and\nwriting that code, I can give a bunch\nof training data expressing how I would like\nthat relationship to look. Give that data to Learning Algorithm which\nthen builds me a model. Of course sometimes I have really complex data\nsets coming from especially video or\naudio or other sensors where it's just\nimpossible for someone, even an expert\nprogrammer who really knows a lot about\nsignal processing, it's impossible for them to build an accurate model at all\nwithout machine learning. Machine learning allows us to then do things we just\ncouldn't do before. Okay great, thanks. Jamie. Well, my work with the\nhuman activity recognition tries to solve a lot\nof different problems. So we tried pick them\nout one at a time. For example, one of the areas I was interested\nin looking at was, what information\ndoes the movement of your eyes giveaway?\nSo how you look. So rather than figuring out\nwhat you're looking at, figure out what it is you're doing depending on\nhow your eyes move. So if you're reading a book and are you reading some text, your eyes will jump around in a very particular pattern and we could use little\nsensors to detect that. You could use cameras. But I'm interested in something that you wear on your body. So I've got a gadget\nto show as well. There's these spectacles that pick up the electrical signals. There's commercial\nspectacles to pick up the electrical signals that are produced as your eye moves, and we can take those\nsignals and analyze them using machine learning algorithms to figure out how\nyour eyes are moving. Has it made a big\njump to the left or to the right or vice versa. So that would be a\nclassification problem. Can you classify in large and small saccades of the eye movement and then\nwe can feed that into another classifier to\ntry and figure out is the person reading or are they having a conversation\nwith someone or are they watching a video. So that's one of the areas\nthat I've been working on, wearable sensing and\nMachine Learning. What I found really interesting\nabout all of this is that a lot of subtle\nhuman behavior, we do it subconsciously. We're not really aware\nof what we're doing it and that means it will be incredibly hard to try and program like computer\nprogram to do this. Is that why, one of the reasons you're using Machine Learning? Yeah, absolutely. I mean\none of the other things is the oldest non-variable\nbehavior that we give off. You see little movements and things that we're not\nalways conscious of. If you take a computer program on a behavioral\npsychologists years to watch what happens and codify these movements\nand interactions. But with sensing and with\nmachine learning we can do all that just using\nthe data itself. So the Machine Learning makes that process and\nawful lot easier. We can make models of\nhuman interaction, human behavior without having to code them directly,\nwe can learn them. Great, and it was really great to hear all your\ndifferent projects because it shows how diverse\nMachine Learning can be. How many different\ntypes of data but also how many different kinds\nof problems it can solve.""",5,0,1
coursera,university_of_london,uol-machine-learning-for-all,summary,"b""Well done,\nyou've done your first machine learning. You've learned the basics of\nwhat machine learning is, how machines can learn to\ndo tasks from example data. You've seen a little about\nhow it's possible for a relatively simple computer algorithm\nto learn complex tasks from data. And you've also learned a bit about\nthe kinds of tasks that machine learning algorithms can do,\nmapping inputs to outputs. But most importantly you've actually\ntried machine learning yourself. You've trained a model\nby giving it examples. It's one thing to hear about machine\nlearning and the amazing things it can do but it's really hard to understand\nit without trying it for yourself. By doing it yourself, you should have\ngot a sense of how it works, and just as importantly,\nthe ways in which it doesn't work. Machine learning is a really exciting\ntechnology that's revolutionizing how computers work and what they can do. You've taken your first steps in\nbeing part of that revolution and taking your part in the future\nof computer science.""",6,0,1
coursera,university_of_london,uol-machine-learning-for-all,the-bit,"b""A computer is a machine that can represent many, many different types of information. But at their most basic level, these types of information are all represented in terms of one thing. The most fundamental unit of digital information, the bit. A bit is something that can only have one of two states at any one time. One or Zero, On or off, black or white, true, false, yes, no, high-voltage, low-voltage, right, wrong, read, unread, locked, unlocked, win or lose. A bit is very, very simple. But it's also very powerful, because it can represent many things. You might have heard people describe bit as one or zero. But those are only one of the things that a bit can represent. Everything on the list I just described as well as many other things, can be and is represented as a bit on a computer. A bit can be one or zero when it's dealing with numbers, black or white for pictures, or locked or unlocked for security features. Some people might say that a bit is really high or low voltage, because that's how it's represented in physical memory. But the same bit of information might be represented differently when it's copied to a hard disk or DVD or transmitted over optic fibers. A bit is an abstract representation that can represent any duality, any binary distinction between two things. Like anything on a computer, it's this abstraction that gives it its power. It isn't tied to any particular application. It can be used to represent almost anything. But isn't it still a bit too simple. Yes, a bit can represent any two opposites, but there were a lot of things in life that don't just come down to true or false, yes or no. How can bits represent landscape paintings, symphonies, population demographics, and social networks? We can represent many things with bits because we have a lot of them. And I mean a lot. Computer memory is typically measured in gigabytes. A byte is eight bits, and a gigabyte is a billion bytes. So, that's eight followed by nine zeros. Modern hard disks can be measured in terabytes, which would be eight trillion bytes. That's a lot of ones and zeros, or trues and falses. It's combining these bits together that allows us to represent many complex things. A single bit can represent black or white. But by putting together lots of black and white pixels, you can make a picture. As you'll find out in the rest of this topic, by combining a lot of bits together you can make numbers, letters, sounds, pictures, videos, and everything else that you can store on a computer. If you understand how bits work together, then you will have got to grips with a really fundamental part of how computers work.""",7,0,1
coursera,university_of_london,uol-machine-learning-for-all,bytes-and-numbers,"b""Bits are great because they are very simple representation that can be used for many different types of data. But if you want to represent anything more complex, you need combine bits together. On a modern computer, you almost never deal with individual bits. It's way too inefficient to try to access data at that level of single bits. Memory's always addressed in larger chunks. One very commonly used chunk of memory is a byte. A byte is eight bits. It's the standard measure of memory. We measure files in terms of kilobytes, megabytes, gigabytes, and terabytes. In fact, even a single byte is too small to access directly on a modern computer. In a typical modern computer, the smallest element you can access is 64 bits or eight bytes. That means, if we want a variable to represent just true or false, we still have to use 64 bits to store it. That sounds like it's very inefficient, but actually accessing data in 64 bit chunks speeds up the whole computer. So, it does turn out to be faster. Having said that, a byte is still a standard measure. It's easier to get our heads around so let's think about bytes. If you represent a bit as a one or a zero, a bite looks like this. A byte can represent many. In fact, 256 different patterns of ones and zeros called bit patterns. Each pattern can represent something different. For example, this pattern represents letter A. This pattern represents letter G. But any pattern could represent many different types of data. So, this pattern can also represent the number 71 or dark gray. Let's start by looking at numbers. Numbers are really useful because you can use them to represent many other things. In fact, I'll talk about representing everything else in terms of numbers. Some things like letters are represented in a way that you could think about in terms of bit patterns or numbers, but I just find numbers easier to read in bit patterns. This is an example of using one abstraction bit patterns to build a high level one numbers, that's easy to work with for humans at least, and could be used to build other abstractions such as letters, images, and sounds. How do we represent numbers? Let's start by looking at how we represent numbers in the decimal system. As an example, let's look at the number 3,168. The digit on the far right, eight is multiplied by one. The next digit, six is multiplied by 10. The one is multiplied by 10 times 10 or a 100. The digit on the left, three is multiplied by 10 times 10 times 10 or thousands. So, each place in a number represents a number 10 times larger than the previous one. Binary numbers work the same way, except instead of multiplying by 10, we use two. The right most number is multiplied by one. The next right most by two. Then two times two, four, two time two times two, eight. So, the binary number 1101 is starting from the right one times one, plus zero times two, plus one times four, plus one times eight that makes one plus zero plus four plus eight which is the decimal number 13. So, you can use binary notation to represent decimal numbers as a pattern of bits. I won't go into a lot of detail about binary since you'll learn about it in other courses. The important thing is that you can represent numbers as bits. A single byte can represent numbers up to 255. But most computers represent numbers as either 32 bits or 64 which is either over four billion or over quintillion. What I've just described only represents whole positive numbers but binary can also be used to represent negative or fractional numbers. You can read more about that in the textbook if you're interested. Numbers are the fundamental building block of everything on a computer. So, they'll also be your building block for understanding other data representations.""",8,0,1
coursera,university_of_london,uol-machine-learning-for-all,other-types-of-data,"b""With numbers, we can create other types of data. One of the most common types of data using a computer is written text. Text can be converted into a sequence of numbers by using a code where each letter corresponds to a number. For example, A is 65 and G is 71. You could just as easily say that each letter corresponds to a bit pattern. A is 01000001, but I find it easier to think in numbers. The computer doesn't carry the weight. The American Standard Code for Information Interchange abbreviated to ASCII is a standard for representing texts that includes all of the letters in the English alphabet plus numbers and special characters in seven bits. That's great if you're writing in English. But what if you speak in another language? An extended eight-bit version of ASCII includes the accents for European languages like French or German. But that still doesn't help you if you want to represent Arabic, Bengali or Mandarin on a computer. Since that excludes the vast majority of the world's population, ASCII is being replaced by a new standard called Unicode which can represent almost any writing system and, importantly, is extensible so other writing systems can be added. It's an unfortunate legacy of the Western bias in computing technology that there's still a lot of computer systems that use ASCII but that's gradually fading. A very large proportion of the data represented on a computer is a combination of text and numbers. For example, a web page is mostly text with some numbers used for formatting, most spreadsheets and numbers with some text. A corporate database will have texts for things like employee names and numbers for salaries and payroll numbers. So, text and numbers can get us a long way. But what about other things? What about images and sounds? We've seen how we can make pictures out of black and white squares represented as bits. We call each square a pixel, a picture element. A modern picture is typically made out of millions of pixels. For example, an image that's a 1,024 pixels across by 1,024 pixels down would be over a million pixels in total, but it's still relatively low-resolution. Of course, we don't just want black and white pixels. The first improvement is to have varying shades of gray or brightness between black and white. We could do that by using numbers instead of a single bit. Because computer screens create colors using light, high numbers values represent bright colors, more light, and low number values represent dark, less light. Brightness is typically represented by eight bits. So, shades between black and white will be a number between 0 and 255. What about color? As we saw earlier, to create color, we use three numbers, one for each primary color, red, green and blue often called RGB. By combining them, you can create any color that is visible to humans. We often add a fourth number to allow for transparent images. This number normally called alpha represents opacity, which is the opposite of transparency. So, an alpha of 0 means a completely transparent pixel and 255 is completely opaque, not transparent. Once we have images, we can create videos by having sequences of images. Each obstruction allows us to create new obstructions. Numbers lets us create images which lets us create videos. Again, this is a concept we've already discussed in this course. Sound is also represented in terms of numbers. A sound is actually a series of microscopic changes in air pressure that is picked up by our ears. A microphone can record these changes in air pressure and convert them into voltages. To record audio in digital form, we can record the air pressure change as a number. To get realistic sound, we have to do this very fast, typically, over 44,000 times a second. One of these individual recordings is called a sample and a whole sequence of samples becomes a sound file. So, our very simple bits can be used to build up very complex data. A single number is tiny, only 32 or 64 bits. But images and audio can include millions of numbers. Since a video file is a long sequence of images, it can be very large, often several gigabytes. Our large memory chips and hard disks can hold what seems like huge amounts of numbers, but with data like video, they can often fill up. So, it's very important to be aware of how data is being represented as it can directly affect the performance of your computer. You've now seen how bits as simple components can be built into complex data. This will help you understand existing data representations. But it's also vital to one of the most important jobs that you will do as a computer scientist: designing your own data representations.""",9,0,1
coursera,university_of_london,uol-machine-learning-for-all,introduction-to-data-features,"b""Hello. You started learning about Machine Learning and you've\ngot the basics of it. One of the most important things you've already learned is that Machine Learning\nis all about data. So to understand\nMachine Learning, you have to understand how\nthat data is represented. That's what we're going\nto look at this week. How computers\nrepresent data and how that data representation can affect how well\nMachine Learning works""",10,0,1
coursera,university_of_london,uol-machine-learning-for-all,data-features,"b""How does a computer see an image? We saw that a machine\nlearning model takes an input and\ngives an output. We've talked about the\ninput being an image, but we've also seen\nthat the input is always just a\nbunch of numbers. That's powerful because it\nmeans that we can apply machine learning to\nanything that we can represent as numbers. But it does mean\nthat we have to find a good way of representing\nthings as numbers, including representing\nimages as numbers. We've also seen how we normally represent images on a computer. An image is a grid of pixels and each pixel is represented as numbers that represent colors. So that could be a good\ninput to machine learning, but there are problems with it. Small changes to an image\nthat we wouldn't even recognize as and real change can have a big impact\non pixel values. A small change in lighting, can have big effects that we don't recognize because we're used to seeing objects in the different\nlighting conditions. Also shifting the whole image even a couple of\npixels to the right, can completely change the\nvalue of a particular pixel, even though the image\nlooks the same to us. This means that raw pixel values can be a bad representation of an image from\nmachine learning. We need a better way of\nrepresenting images as numbers. In machine learning,\nwe use the name features for the numbers we\nuse to represent things. Pixels are an example of\na feature for images. They're what we\ncall a low-level or raw data feature that represent the image as it is but individual pixels don't carry much meaningful\ninformation. If instead of using\nimages of cats and dogs, we took measurements of\nthings like nose length, weight and fur color, these will be higher level more meaningful features which might work better for\nmachine learning. The features we choose can\nhave a big effect on learning. It'll be possible to learn something using certain features, but it might be\nimpossible to learn the same thing using\nother features. If all we have is fur color, it would be very hard to learn the difference between\ncats and dogs. Most domains have many\ndifferent possible features. For example, a sound signal is a low-level feature for both\nmusic and human language, but both have high-level\nmeaningful features like musical notes\nor written words. Machine learning\nis likely to work better with these\nhigh level features, but there are also many\nsituations where we'd like to work directly with\nthe raw data like images. Luckily, it's possible to\nautomatically calculate some features that are\nmore meaningful than pixels from the real pixel data. For example, edges\nbetween patches of color are often a good\nfeature and they can be calculated using the kind\nof filter that you might be familiar with from a photo editing software like Photoshop. Some applications have even\nmore specialized features. Many face recognition\nsystems have special code to recognize the eyes or the\nedges of the mouth. So an important part\nof the process of machine learning is\nfeature extraction, extracting more meaningful\nfeatures from the raw data. This generally means writing some custom code to\ncalculate these features, but as we'll see, it's now increasingly possible to learn the features themselves.""",11,0,1
coursera,university_of_london,uol-machine-learning-for-all,neural-networks,"b'How do your image recognition\nmodels work so well if pixels are such a bad set of features with\nmachine learning? It\'s because they have a really good feature extraction that can calculate really\ngood features from the original pixels. In fact, these features\nare learned from data. How does that work?\nLet\'s have a look. Features are just numbers. Imagine we have two\nfeatures for pets, for example, tail length\nand ear pointiness. These are two numbers\nand we can create a new feature by combining the two original\nfeatures together. How do you combine two numbers together to make a new number? There are lots of\nways of doing it but a really simple way is\nto add them together. So we have a new feature which is tail length plus ear pointiness. This isn\'t necessarily\na very good feature. For example, ear pointiness\nmight be much more important for telling if a pet is a cat or a dog than tail length, but they both contribute\nequally to the sum. We can make ear pointiness more important by multiplying it by a big number and multiplying tail length\nby a small number. In this new version, they both still have an effect but ear pointiness\nis more important. The two numbers we multiply\nby are called, weights. We can actually learn a\ngood feature of this type by using optimization to\ncalculate the best weights. Now, this is a really simple\nexample of a feature, so you may think that a state of the art\nmachine learning system would need something\nmuch more complex. Well, actually no,\ndeep neural networks, the cutting edge machine\nlearning method, that had solved many\ncomplex AI challenges, works almost exactly like this. How can something so simple\nsolve such hard problems? The basic answer is that these really simple\ncalculations can scale up well when it\ncomes to massive data. The first way this\nhappens is that you don\'t start with just\ntwo basic features. You can have a lot more, may be hundreds or even thousands of numbers that you add together. Secondly, you can take the newly calculated features and calculate new features from them. The sum we just saw\nis called a neuron. It is one unit of neural network, but you can have hundreds of\nneurons and the output of each neuron is fed into our inputs of hundreds\nof other neurons. The word, ""deep, "" in deep learning means\nthat you have many, many layers of neurons\nfeeding into other neurons. This means that even though the basic calculations\nare very simple, a deep neural network can\nlearn very complex features. There\'s one important detail that neural network experts\nwould catch me on: simply feeding the output of one\nneuron into another is actually equivalent\nto simple neurons with different weights. So it doesn\'t allow you to calculate any more\ncomplex features. You need to transform\nthe output by a mathematical function\ncalled, a non-linearity. There are many types\nof non-linearities, some of them quite complex\nbut they don\'t have to be. One very common\nnon-linearity is just to take any output that\'s\nnegative and set it to zero. So the simplest\ntype of neuron just multiplies its input\nfeatures by weights, adds them up, and if the output is negative\nsets is to zero. This simple network can learn very complex features by\nchoosing good weights. I want to be clear here:\nThis is a simple neuron but that doesn\'t mean that real neural networks have to\nuse more complex neurons. The one I\'ve described is\nabsolutely state-of-the-art and is used in almost all the best neural\nnetwork systems. Other types are used\nfor other purposes or types of data but that\ndoesn\'t mean they\'re better. Of course, this is a very\nsimple description of a neural network and I have\nleft out lots of details. In particular, the\nlearning algorithms, the optimizations used to calculate the weights\ncan be very complex. But the key thing I want you to understand is that a lot of learning algorithms are based on quite simple mathematics. But they work because\nthese calculations can scale up to thousands\nof features in neurons. The fact that neural\nnetworks can be very large and complex helps\nthem work effectively, but it also means that it can be hard for us to understand\nwhat they\'re doing. It\'s very difficult to interpret the many simple\ncalculations and get a big picture of how neural\nnetworks make decisions. This is a problem with many\nmachine learning algorithms because they\'re very\ncomplex and they learned, not designed by people, it\'s hard to know how they work. This can make it hard to test and debug models that don\'t work. It also raises issues when machine learning\nalgorithms make important decisions without us understanding why they\nmake those decisions. Something we\'ll find\nout more about later. Since we\'re working with images, I want to talk about another type of neuron that works well for images called, a\nConvolutional Neural. When I first talked about\nfeature extraction, I talked about how a good way of calculating features\nfrom images is to use a filter of the sort\nyou might have in a photo editing application\nlike Photoshop. For example, edge detection and blowing make good features. A Convolutional\nNeuron is basically a Photoshop filter that\ntransforms an image. But unlike filters\nin an application, the details of filter can\nbe learned by optimization. The mathematics is quite\ncomplicated but you don\'t need to know the math to use a Convolutional Network. Because it\'s a neural network, the output of one\nfilter can be fed into other filters to create\ncomplex new features, which are then\nthemselves fed into other filters to create\neven more complex features. This is the basic\ntechnology that allows machine learning to\nrecognize images so well.'",12,0,1
coursera,university_of_london,uol-machine-learning-for-all,interview-data-features,"b""So we've been learning about how important\nit is to look at the representation of our data in machine learning, and what features to use as\npart of machine learning. And in this course, you must have\nbeen looking at images and pixels but there's really many, many different types of data that require\nmany different types of features. So I'd like to talk to our panel of\nexperts about the kind of features they use. So starting with you Jamie,\nwhat kind of data and features do you use in your projects? >> Pretty much all of my\nproject are a time series, so that's where you've got your data, it's not just that it's a pixel in a\nparticular location with a certain color. It's a changing variable over time. So capturing that temporal\ninformation is quite a challenge and to build a feature on\nthat as a challenge to. One of the things you might want\nto look for in temporal data. So you can imagine for example, the\nsignals your heart makes, you see the same when you see an electrocardiogram there's\nthis distinctive electrical signal. Well, that's a time series. And one way to analyze that would\nbe to have a fixed window of time, say like a few seconds or a second,\nand you slide it over the data, and you create some analysis on that. And one of my favorite features as\n[UNKNOWN, useful favorite features that I find, is we want to know something\nabout how fast the signal is moving. Are they moving up and down a lot\nare they just staying fairly flat. And one of the things you can do is you\ncan just take the mean of this window, and say, okay, what's the average level here? Or if there's a lot of movement we\ncould do something like the standard deviation to find out how\nmuch it's moving up and down. But you can go even further and\nfigure out what the frequency responses. So what specific frequencies,\nhow fast are those signals moving. And a really cheap and easy way to do\nthat, which is my favorite feature, is where we do a zero crossing count. So if you've got a signal on time,\nevery time it crosses the zero line, so the middle line, every time it crosses\nthat we count, we just add add up. And that allows us to approximate\nfrequency using a very simple algorithm. >> Yeah, and\ntime series is a really sort of rich and powerful form of data that you can see\neverywhere whether it's in sound data or the kind of data you're talking about. Or I work a lot with human movement,\nwhich is very sort of time series based or sort of financial data as well,\nthe stock market data is all about time. So that's not something we talked about\nin the course of its really I'm part of machine learning. >> Yes, and\nmany of the same algorithms can be used. So they're transferable. Once you just start getting\nyour head around that we're also taking this\nvariable of time into account. But yes, it's a very interesting and\nvery useful topic. >> What about you Louisa,\nwhat kind of data do you work with? >> No interact design you work\nmostly with chemical structures, but we had fun very interesting project,\nit's about meta learning. There is a famous theory in machine\nlearning, no free lunch theorem. So there is no a single machine\nlearning algorithms that will work best in all circumstances and\nthe it's very true for drug designs. So we analyze the different studies\nagainst different drug targets using different machine learning algorithms,\nbut also using different representations. Because you can represent chemical\nstructures by many different ways. And we learnt, so\nit's learning about learning. We try to learn what representations for\nwhat drug targets using what machine learning working best and\nit was very interesting. So for some situations you have\nquite the should data set. So this will be your features\ndescription of this data sets. How many data points if the sufficiently\nindependent, if there is no noise. But also description of machine\nlearning algorithm itself, as their robust against noise. Do they have parameters and\nalso description of drug targets. So we put it all together as features. So we had like combined data sets. They did self machine learning\nalgorithms and also drug targets. And by the way One of the best\nthe performance boost around the forest. So just if you don't\nknow what to apply and what characteristics and\nit is a good starting point. >> Okay, I mean that's a lot of\nreally interesting stuff there. I mean, I think it's really it's a really\nimportant point that that often you can get some data and you can have very\ndifferent representations of it. And and that's something that comes up\na lot in machine learning isn't it and choosing which representation to\nuse is really, really important. But also that sounds like a really\namazing project to not only use machine learning but apply machine\nlearning to machine learning itself, which you know gives a really\ninteresting results and the fact that you're never going\nto find a perfect algorithm. But there may be some, but like random\nforests which we haven't talked about. What's quite interesting algorithm,\nbut one that works generally quite well in at least the\nkind of problems you I've been working on. As you said, there's no free lunch. There's no one single algorithm\nthat's guaranteed work everywhere. And I think the students who are working\non machine learning projects, there should remember,\nthey will never find the perfect solution. So they just need to find something with. [LAUGH]\n>> Rebecca? >> Yeah, so the a lot of the projects\nthat I work on and that I help students to work on user a wide variety\nof sensors or audio, or video. And I think echoing some of the things\nthat Jamie and Louisa both said, there's there's not a perfect feature\nrepresentation for any given sensor or even necessarily for any given task. And there's a lot of experimentation\nthat happens in trying to figure out what are good features for a task. And I can come back to my my Leap Motion\nexamples that I mentioned earlier. So this device itself will find\na skeleton of your hand in space and you can pretty easily get\ninformation about the XYZ out of each finger tip, for instance. And so I just did a project with this\nwith a bunch of music therapists and music teachers who were using leap\nto build musical instruments for people with disabilities. So they were using classification and\nregression as people move their hand in space to select\nwhich sound sample is played or to change sound in\na continuous way over time. And we ended up with a couple\ndifferent feature representations. One of them just uses information,\nnot so much about your hand position. But just where's your palm in space, and\nthat gives you an instrument that you can think about playing by selecting\ndifferent sounds in different places. Another feature extractor we built uses\nthe distance between these two fingers. So you can imagine squeezing a sound, and the squeezing motion changes\nwhat sound you're making. But certainly in other projects\npeople have really good use out of more complex features including\ntreating this data as Time series data. So one of my favorite students\nprojects from a workshop is a little robot called hi bot, and\nyou control hi bot by waving at it. And when it sees you waving it\ntakes its little cardboard arm and waves back at you. And they're in order to understand\nwhether somebody's waving or not, you can't just look at what position\nyour hand or fingers are at the moment. You have to look at how\nthey're moving over time or even are they moving over time? So in that case we can take a time window\nand look at the finger positions or the palm position over\nthe last few milliseconds. And then look at, what is the difference\nbetween the maybe maximum left and maximum right position or what is\nthe standard deviation within that window. It turns out that's great for\nnoticing whether somebody's waving or not. >> That reminds me of, I would listen to a\npodcast about an AR experienced augmented reality experience where\nyou interact with waving. And the developer said, you just wouldn't believe how\nmany different ways people wave. >> [LAUGH]\n>> They wave like and like that and this. And I think that shows for them the\nimportance of gathering a lot of data and be able to test, and data isn't\nalways what do you expect it to be. For sure, and again as Jamie mentioned earlier when he\nwas talking about some of his projects. Weaving is something that we tend\nto do without thinking about it. We can't always articulate how we wave or even remember whether we're waving\nthe same way someone else or not. So using machine learning to\ncapture a wave from data is just so much more of a sane way to build a project\nthan trying to get a programmer and somebody who is a PhD in waving or\nsomething like that. >> [LAUGH]\n>> I might lead back quickly to Jamie, because we've got this idea\nof a no free lunch and that there isn't a perfect\nfeature representation. Is that something that sort of comes\nup a lot with you that you need lots of different feature\nrepresentations of different tasks. >> Absolutely I mean,\nit's totally what you're seeing. There are so many different ways to wave. For example, and so\nmany ways to do everything. And you couldn't have one\nalgorithm that sorts at all. And part of the problem with that as I was\nlooking at it as engineer saying, yes, this is what the problem is. I'll break up at that feature seems\nappropriate and it makes perfect sense for a very small number of situations. What you're doing is you're\noverfitting the problem. So we need to try and\nlook at a broader and think of things that we've not thought of. So machine learning is\npretty good at that. We throw it lots of data,\nlots of random noisy examples and it might make a better system than\nwe are able to do on by ourselves. So the variety of features\nthat you might use, the choice of features might be something\nthat we could get to the algorithm to do. Rather than me deciding\nwhere you can mean and variance we can actually have the the\nfeatures can be selected themselves using machine learning which\nis a big area of work. Yeah, which is a lot of what\nthe deep learning is doing. >> Pretty much, yes. >> Okay, well great. Thank you everybody. That's that's a great explanation of\nthe power features in the importance of features and their diversity\nfeatures in machine learning. And if our learners go further and\nmachine learning, this is definitely a topic that will be\nvital in the practice of machine learning.""",13,0,1
coursera,university_of_london,uol-machine-learning-for-all,introduction-to-machine-learning-in-practice,"b""Welcome back. You've learned quite a lot about machine\nlearning already, a lot of the technical details. This week, we're going\nto look a bit more at the practicalities of\ndoing machine learning. One of the most important\nthings you need to do in machine learning is\nmake sure it works, and to do that you've\ngot to test it. So we're going to look at how machine learning experts\ntests their projects. Then, we're going to\nzoom out and look at the big picture of\nmachine learning, how machine learning will\naffect our society and future, the great benefits we can\nget from machine learning, but also some of the\ndangers and how we can avoid those dangers\nof machine learning.""",14,0,1
coursera,university_of_london,uol-machine-learning-for-all,testing,"b""How do we know if machine\nlearning has worked? You probably already have discovered\nthe results are often not what you expect or want. In fact, even when a machine\nlearning model works well, it's almost never completely perfect. It will still make mistakes on occasion. Often, we humans can't do tasks like\nrecognizing faces perfectly, so how could we expect computers to do so? But that makes it quite hard to work out\nif a system is working acceptably because you'll never get it to be 100% correct. A good start is to see how much of\nthe data it classifies correctly. Once your algorithm is finished learning,\nyou can test it by classifying each item of the training data and\nsee what percentage it gets right. But it can be hard to\ndetermine a good result. It can depend on your application. Classifying cat images correctly\n99% of the time is really good, but a self-driving car that fails to see\npedestrians even 1% of the time could be fatal. Sometimes it's worth looking at\nhow well humans do in a task. If people can only classify an emotion\nin the face correctly 70% of the time, you probably can't expect machine\nlearning to do much better. You might also want to favor\ncertain types of error. In a medical test, it's probably better\nto be cautious and erroneously recognize the disease in a healthy patient than\nfail to spot it in an ill patient. Classifying the training set well is good,\nbut you really want your model\nto do well on new data. Machine learning works by\noptimizing the model so that it performs well\non the training data. That means it can learn all the details of\nthe training data and do very well on it, but it probably won't do as well on new\ndata that was not in the training set. This is why machine learning practitioners\nkeep some data just for testing. This test data is not used\nduring the training phase, so it's a fairer test than\nusing the training data. The percentage of correct classifications\non the test set will give a much better idea of how well your model\nwill work in practice. You should typically split your data\ninto a training set and a test set. The training set should be bigger so\nthat you have enough data for your algorithm to learn effectively. A good split might be 4/5 training and\n1/5 testing. Testing is a really important\npart of machine learning. Professionals often use complex testing\nmethods with multiple data sets, but splitting your data into train and\ntest sets is a really good start.""",15,0,1
coursera,university_of_london,uol-machine-learning-for-all,problems-with-machine-learning,"b""Did all the datasets work? If not, why did some of them fail? There are a lot of reasons why\ncertain data sets do not work well for machine learning. The basic reason is that\nthe machine learning algorithm is actually doing its job. It's learning what's in the training data\nset, but that isn't always what we wanted to learn and it doesn't always transfer\nover to the testing set or the real world. This is often because it learns aspects of\nthe training set that are irrelevant to the task. There was a classic example of a system\nthat was trying to recognize tanks and did very well on the training set but\ncompletely fails in the real world. The developers realized that they had\ntaken all of the pictures of the tanks on a cloudy day. The system learned the lighting\nconditions, not the tanks themselves. There were some technical\nsolutions to this type of problem, very complex models can learn every\nlittle detail of the training set and therefore lots of irrelevant details. That's why most machine learning\npractitioners favored using models that are as simple as possible while\nstill being able to do the job. When it comes to doing\nmachine learning though, often the best thing you can do is\nmake sure you have a good data set. When collecting a data set, you have to\ncheck whether there's anything about how you are collecting the data that\nwould not be true of the real world. Are all the pictures against the same\nbackground, in the same lighting conditions, is the person you're trying to\nrecognize always wearing a hat or glasses? Do they have their hair tied back? Are they always wearing the same makeup? The more you can vary any of these\nthings in your data set, the better. Try to capture images under as\nmany conditions as possible. Make sure that your training and test\nsets are as representative as possible of the data you will be getting\nin the real world, so that your model isn't surprised by\na very different type of image. In short, do everything you can to make\nsure your algorithm is learning what you want it to learn and can't cheat by using\nirrelevant features to make decisions. What if once you've tested your model,\nit doesn't work as you'd expected? This is when you should really\nstart looking at the examples that aren't classified correctly. Can you see any patterns in\nthe mistakes the model is making? Maybe you haven't got enough training\nexamples that are similar to a particular example in your testing set. Maybe certain items in one class look\nsimilar to training examples in another class. Most of the time you can fix problems by\nadding new training examples that show the learning algorithm\nthe difference between two classes. For example, if you don't have any\ngray dogs in your training set and lots of gray cats,\nit might think that this is a cat. It should be quite easy\nto fix the problem. Go and find some pictures of gray dogs. Sometimes two classes might just\nbe too hard to tell apart and you might need to rethink\nthe problem a bit. Maybe limiting the set of classes or\nmaking sure your images are taken under more controlled conditions,\nwhich is fine as long as your real world examples that you use later will\nalso be in controlled conditions. Getting a working machine learning\nmodel is not an exact science. Even the world's greatest experts spend a\nlot of time trying to improve their models through trial and error. The important thing is to spend\na lot of time testing the model and when it goes wrong, look carefully\nat the data that's misclassified and do your best to improve the training set.""",16,0,1
coursera,university_of_london,uol-machine-learning-for-all,applications-of-machine-learning,"b""The reason a lot\nof people are now excited about machine\nlearning is that it's having massive success\noutside of a lab in real world applications\nthat we use all the time. In this course, we've looked\nmostly at image recognition, a technology that's starting to be used to search for images. The iPhone's photos\napp allows you to search for your photos\nfor particular people, a technology that relies on face recognition using\nmachine learning. You can actually help\ntrain the system yourself by labeling photos\nwith the correct names. But machine learning\nisn't just about images, it can be applied to many\nother types of data. One of the most successful\nuses of machine learning concerns language both\nwritten text and speech. The Google search engine\nrelies very heavily on machine learning on the text of web pages to give\nyou good results. Machine learning has also enabled the current generation of\nautomatic translation software, which is now very good\nbecause it's been trained on data sets of the same text translated into two\nor more languages. The proceedings of the\nCanadian Parliament, which are available in\nboth English and French, are an early example\nof a dataset that was used to train\ntranslation software. One of the most\nimpressive uses of machine learning on language\nhas been speech recognition. Of the sort you can see an automatic assistance like\nAmazon Alexa or Apple Siri. The fact that a\ncomputer can understand your question and come up\nwith the correct answer is pretty astounding\nand it's all down to machine learning on large\namounts of recorded voice data. You're also likely to encounter machine learning if you\ndo online shopping. The product recommendation\nthat sites like Amazon give you are chosen\nusing machine learning. If Amazon's recommendation\nseem uncannily accurate, it's because it's\nlearned from it's hundreds of millions\nof customers, and that's one of\nthe characteristics of modern machine learning. You've learned in\nthis course that machine learning works better\nthe more data you have. So the companies\nthat can use it most effectively are those\nwith the most data. Like Amazon with its\nmillions of customers, Facebook with its\nbillions of users, and Google with essentially\nthe entire Internet. The vast amounts of data we're producing on the Internet\nare likely to feel new amazing breakthroughs that apply machine learning\nto new areas. Some of the ones that seem possible in the next\nfew years include automatic medical diagnosis\nfrom radiology scans, and computers that can\nwrite news articles. Something that's\nalready been done for sports and financial news. Self-driving cars are another machine\nlearning innovation. Something that would\nhave seemed like science fiction even\na few years ago, but they had a full\ntrial near us in London a few months before\nfilming this video. One of the hardest things\nabout self-driving cars is recognizing what's on\nthe road around the car. Whether it's other cars, bicycles, pedestrians\nor road signs. This means that a large part of self-driving car technology\nconcerns image recognition. Even though it seems like\namazingly advanced technology, it's based on the same types of machine learning techniques that you've been using\nthroughout this course.""",17,0,1
coursera,university_of_london,uol-machine-learning-for-all,dangers-of-machine-learning,"b""Although machine learning is an exciting\ntechnology that has a lot of potential to make our lives better, we should\nalso look at the potential dangers. A lot of people worried\nabout the potential for artificial intelligence\nto be too successful. The computers will become more intelligent\nthan humans and will take over the world. This view is well expressed by Nick\nBostrom in his book, Superintelligence. Like many thinkers in this area,\nhe thinks there's a potential for an intelligence explosion. In which an AI, that is more intelligent\nthan humans, will invent a more intelligent AI, which in turn will invent\nan even more intelligent AI and so on, until we have a vastly super intelligent\ncomputer that will take over the world. Or possibly, in a matter of minutes. I personally think that although super\nhuman artificial intelligence is possible eventually, this scenario is unlikely. Having worked in many areas\nrelated to AI for many years, I realized that it's very very hard. Creating human level intelligence and\nbeyond, we'll take a lot of work and these many conceptual breakthroughs. Simply having more memory or processing\npower which is what an AI is likely to bring, will not mean these\nproblems can be solved instantly. But that doesn't mean we shouldn't worry\nabout machine learning technology. Pedro Domingos, machine learning professor\nand author of The Master Algorithm. One of the best books on machine learning\nfor the general public, puts it nicely. People worry that computers will get\ntoo smart and take over the world, but the real problem is\nthat they're too stupid and have already taken over the world. Machine learning is now being used\nin lots of very important roles and we should worry about the possible\nnegative implications. Even if we're mostly worried\nabout superintelligence, I think that the best way of dealing with\nits dangers is to deal with the dangers of the AI we currently have. That way, our defenses against the dangers\nof AI will evolve together with the AI itself, and will be more prepared if and\nwhen it does become super intelligence. One of the most obvious effects of\nmachine learning and AI is on jobs. If machine Learning Systems can do things\nthat previously only humans could do, there's a risk that our jobs\nwe taken over by machines. There's already evidence that this is\nhappening as described by Martin Ford in his book, Rise of the Robots. This can affect any kind\nof job from taxi drivers being replaced by self-driving cars,\nto doctors and lawyers being replaced by automatic\ndiagnosis or legal case search. How we deal with this situation is\nmore of a political challenge than the technological one. We could end up in a situation of mass\nunemployment and poverty but we could also end up in the science fiction utopia in\nwhich we all have to work much less and benefit from all the work done for\nus by the machines. Whichever happens is down to the choices\nwe make about how we want our society to work. And those are very much political choices. Job automation is about machine\nlearning working well, but there were lots of problems due to\nmachine learning not working as we want. A big issue is bias in\nmachine learning models. We tend to think of computers\nas objective and unbiased but researchers like\nKate Crawford have shown that machine learning models can show\nthe same sort of bias that humans have. For example, machine Learning Systems used\nby banks can disproportionately refuse loans to certain ethnic groups. This is often called algorithmic bias, but\nthe bias is not really in the algorithms. Machine learning algorithms\ndo have one major bias. They aim to reproduce what\nthey see in the data. That isn't really a bias. It's what the algorithm is supposed to do. The trouble is that a lot of data\nfrom the real world is biased. Because the real world itself has bias. A machine learning algorithm might\nassociate the word surgeon with men, but nurse with women. This might accurately reflect\ncurrent data because a lot of text out there include male surgeons and\nfemale nurses, but that doesn't mean that's what\nwe want the system to learn. They're increasing number of female\nsurgeons and male nurses and we want more women in medicine. The same goes for police work. If in a certain country, there's an ethnic\nor religious bias and police arrests, this will be reflected in the data. But we want to eliminate\nbias not reinforce it. This means we have to be very\ncareful about the data sets we use. I've said before that it's important\nthat our data sets are representative. But this is even more important if\nthe data is unrepresentative in a way that perpetuates existing social biases. It's harder than it seems. For me, working in London, it's very easy to gather data that's\nfairly representative of the UK. But if I want a machine\nLearning System for use globally, my data might be very\nunrepresentative of other countries. One really important aspect of data that\nwe've already learned about is features. It's not only important which\nexamples of data we use but the features we include\nabout in each example. One way of trying to eliminate bias is to\nignore features such as race or gender. That sounds like a good idea. A machine learning algorithm can truly\nbe biased by race if it doesn't know people's ways. Unfortunately, it isn't that simple\nbecause other seemingly innocuous data features might be associated with race or\ngender. For example,\nclothes shopping habits can reveal gender. Postcodes might reveal race, if people from certain ethnic groups tend\nto live in different parts of a city. Once you start thinking about it, you realize there aren't many features\nthat don't show bias in some way. Biased systems can be particularly\ndangerous if they're making important decisions about us. I've mentioned machine Learning Systems\nbeing used to inform police work. But there are many others, machine\nLearning Systems now influence whether we get an insurance,\na bank loan, or even a job. Machine learning systems are being used\nin certain US school districts to decide which school teachers are under\nperforming and which should be fired. Cathy O'Neil has a phrase for these types of potentially biased machine\nLearning Systems in life changing roles. Weapons of math destruction. The dangers are enhanced by the fact that\nmany machine learning methods like neural networks are very complex and\nhard to interpret. This means it's very hard to know why\na model has made a certain decision. If we don't know why a decision was made, there's no way to appeal against it or\nto make the system work better. If people trust a model without checking\nhow it works because they can't, we end up with important decisions being made\nabout people that no one understands and that no one can know whether\nthey were right or wrong. So it's really important that we\nthink carefully about the future of machine learning. It's a very powerful technology, but we have to make sure we\nuse it in the right way. And that's where you come in. If machine learning takes over human jobs, we have to work out\nwhat we can do instead. And that is a conversation\nyou can be part of. I think one very important job in\nthe future will be teaching the machine learning models that do the work. That's why learning about machine\nlearning is very important. It might be a key skill you need for\nyour next job. And you're now more ready to be involved\nin the conversation about the future of work. Making sure that machine learning models\nare not biased means making sure the data is representative. I believe that one of the best ways of\ndoing this is making sure that the people who do machine learning are as\nrepresentative as possible. If only a few people involved\nin machine learning, they will inevitably bring their biases to\nthe system whether they mean to or not. The're more different kinds of people\ninvolved in collecting data sets and training models the likely they\nare to find and correct biases. This is part of the reason\nI'm teaching this course, because I want all of you to\nbe part of machine learning. Each and every one of you will have\na different background of perspective, which means that you'll bring a different\npoint of view to machine learning. Each and every one of you has an important\ncontribution to make to machine learning and I want you all to\nmake that contribution. Let's all work together to make sure that\nmachine learning makes this world a better place, not a worse one.""",18,0,1
coursera,university_of_london,uol-machine-learning-for-all,interview-benefits-and-dangers-of-machine-learning,"b""So in this course, we've been learning about\nwhat machine Learning is and the technical\ndetails of what it does. But really, technology should be about making\npeople's lives better. I'd like to ask our\npanel of experts, how can machine learning\nimprove our lives? What are the big benefits that machine learning can\nbring us, Rebecca? Well, I've spent a lot\nof the last 10 years as a researcher building\ntools for other people to apply machine\nlearning to problems that might benefit them. So this is something that\nI think about a lot. Certainly, the range of applications where machine learning\nis useful is huge. I tend to focus mainly on applications that have to\ndo with accessibility, building systems for people with disabilities and/or\nsystems for creativity, enabling new types of music\nor art or interaction. In those domains,\nmachine learning brings some really tangible\nexciting benefits. So first of all, there's a lot of\ndata that people in those domains may want\nto take advantage of that is just really hard or impossible to do\nwithout machine learning. So if you have data with\nmany features, for instance, if you're dealing with audio\nor video or certain sensors, it can just be cumbersome\nor impossible to make sense of that data even if\nyou're an expert programmer. So machine learning\nallows people to build models of that data and then use those models to either say something useful about\nwhat's happening in the world around them or to drive computer systems that respond to the world\nin certain ways. So this could mean making new musical instruments that respond to people's gestures. It could mean making responsive\nart installations that look at what people\nare doing in a room and move robotic\nsculptures in response. It can mean making new kinds of video games that respond\nto maybe how somebody is moving with\ncontrollers or even how the social interaction unfolds between different\ncharacters in a game. So all these things\nare exciting but might just be hard\nwithout machine learning. But there are other\nbenefits as well, particularly in the space\nof creativity and design. So for one thing, building things with machine\nlearning is often much more accessible to more\npeople than programming. So if you can come up\nwith some examples of a thing that a person might do and labels that describe that action or computer responses that you might want to be\ntriggered by that action. If you can come up with\nthat dataset and use an appropriate\nmachine learning tool to build a model of\nthat relationship, then suddenly, you don't\nhave to be a programmer. So this enables people who\naren't programmers to actually build quite sophisticated\nsystems themselves. Sometimes this means designers in an organization can\nbuild prototypes rather than relying\non a tech team. Also, means that kids\ncan build things for themselves and for\nothers. So that's exciting. Then the third thing\nthat I'll add is that machine learning gives\nus an alternative way to express embodied interactions or expressive interactions\nto a computer. So it can be really\nhard for us to even as people speaking\nour native language describe how a\ndancer is moving or describe how we play an instrument or how\nwe ride a bicycle. But it's quite natural for\nus to demonstrate that. So by using machine learning, we can make it much\nmore effective for people to build\ncomputer systems that understand something about\nhow humans are acting as people with bodies that do all sorts of complicated\nthings in the world. In those glides, I think there's machine learning has\nthis reputation of being this very technical subject\nfor just for engineers. But actually, you were right. Then what we're trying to do in this course as well is to show that it can be a technology that everybody can use with their own embodied\nphysical skills. Larisa, what about you? What\ndo you see these benefits? I see as the main benefit\nof machine learning in supporting humans to process this huge amount of\ninformation we are facing. So we no longer can read all\narticles that are published. We no longer can analyze a\nsensor data that are coming. That is why here there are so many applications where you rely on computers\nto process it. So there is no denial that computers are better\nin many things. So they can do\nthings in parallel, in multidimensional, with\nthousands of huge features. So humans just cannot do it. It's already\nmulti-billion industry. The old prognosis that it\nwill be only expanding. So what we are witnesses\nnow, it's very profound. The nature of our\njobs is changing. The nature is how we're interacting with the\nworld is changing. Yes, it is exciting. It's great opportunity\nto know more about it. But also, if we can\ndirect which way it goes. So that would be my answer. Well, I guess two answers to that that reflect in both of what you've said\nthere, both of you. From a slightly smaller scale, my own research in\nwearable computing. I mean, a small goal\nthere is to be able to build computers that\nhelp us do daily things. Every day, daily things. That can be an accessibility. If you are physically\nunable to do some of these things or you need\nsome assistance with that, the computer can help you. With it being able to\nunderstand your movements, your actions, this\nembodied connection. If computers able to sense and understand that, say understand, is able to recognize that, use machine learning\nto then analyze it and then act on that to help us. I think there's a whole\nrange of applications that are very interesting. One of the areas I work\nin is with autism. Where social behavior between say all these autistic children is thought to have certain\nissues, certain difficulties. What we found using\nwearable sensing was that there were social\nbehaviors happening the non-autistic people\ncouldn't see but the autistic people themselves\nwere able to engage with. We were able to find that\nusing machine learning and sensing but not as\ncasual observers. So there's a lot of very\ninteresting useful areas in that topic that drive me. But on a grander\nscale, a major scale. I mean, there isn't an area of life for machine\nlearning doesn't have a profound influence and impact for the positive\nand the negative. Just to pick one example I think that diagnosis of\nserious illness. For example in\ncancer diagnosis has just taken a massively\nforward when you're able to train an algorithm to support cancerous\ncells so much quicker, so much more efficient, so much easier than a physician having to look at lots and lots of scans. So I think there are\nso many areas where there's really genuine positive impacts of machine learning. Thanks Jamie. You also\nmentioned some of the negative. I mean, there's a\nlot of anxiety about machine learning and\nAI at the moments. What do you think the potential dangers\nof this technology? As much as the benefits, there are negatives and probably more and we have\nto be aware of them. I think as engineers and\ncomputer scientists, we really need to be aware of them because there are things we can influence\nand have a say in. One of the big things in any\ndiscipline is assumptions. If you make assumptions\nabout things, you're destined to\nfailure but we need to make assumptions otherwise\nwe'll never progress. With machine learning,\nthere's a lot of assumptions. We've even made\nsome of them today. I'll make them all the\ntime. Where we're like, oh we don't know how to choose which features or which\nalgorithms we're going to use. We'll just throw lots of\nmachine learning at it, learn from the data, and that will find\nthe best solution. But there's a big assumption. Because the minute you take the human out of the loop and just leave it all to the\nmachine learning and have no human in there, no brain in there then we\nstart opening ourselves up to all sorts of biases in the data and lots of manipulation of the data that can then influence\nthe algorithms. You can certainly look\ninto these instances where an advanced machine learning\nalgorithms have started expressing racist decisions based on taking data from the internet which in\nitself was was racist. But for me the biggest issue at moment with machine learning, which we really need to\nthink about as one about, comes back to this idea of\nefficiency and energy usage. There's a trend in\nmachine learning to solve every problem\nusing deep learning. Let's get some deep\nlearning on that, let's go assume massive\nneural networks and have lots and lots of computing power to solve the problem. Sometimes that solution could be just a simple linear regression. We have just spent lots and lots of energy trying\nto solve that problem. Now, that seems fine for a bunch of computer\nscientists in a lab. But if it's go half the world using machine\nlearning algorithms, then we start to have\nsignificant impact on our energy infrastructure. A recent study shown that some natural language\nprocessing algorithms, just to train the model used as much energy as several\ntransatlantic flights. Before to already for example, certain computing\ntechnologies already use more energy than\nsome countries. So we're on the verge of machine learning becoming\none of the biggest users of vital natural resources on the planet and we really need\nto do something about that. So that's quite a\ncritical thing I think. Which is engineers and scientists\nwe need to think about. Thank you. Larisa, your\nthoughts on the dangers? I think like within a\ntechnology it's all about how you use it\nlike electricity, huge benefits but if you don't use it properly\nyou can kill yourself. So this is the same. You have to use this powerful\ntechnology responsible. Unfortunately, people\nwho are working, developing these\ntechnologies, they are focused too much on\ntechnology per se. I can see how it can happen, it happened to me. It's so beautiful, so logical, so organized, so powerful, and you think that\nsomeone else will take care how it is used,\nall ethical aspects. Unfortunately, legislation,\nregulation, people who's supposed to do it or they don't understand fully\nwhat is components and non-specialist or it's\nso lag behind. So yes. The power unleashed, and\nit's sometimes wild. So we need to think about it. Not one of hot topics for discussion its\nhow data is gathered, that people didn't volunteer\nto the needs as data or data can reflect some\nracist point of views. So how to stop it. How to control it. So there are many dangers. Another hot topic is, will all this machine learning\nalgorithms take jobs? It's very important to analyze how it will change\nthe structure of job market. Our everyday life, our\nsocial interactions. I think what's happening\nnow is extremes, it's so wonderful, it\nwill solve all problems or it's awful it\nwill kills us all. The reality is somewhere\nin between and we all can make a difference. So yes, the thing with\ntechnologies is there are good and bad and we can make\nall the difference. Rebecca, where is the reality. I think I agree with everything that Jamie\nand Larisa raised. I think I'm not going to repeat their points but I have\na few thoughts to add. So another one of the challenges that I think about a\nlot when I'm trying to help people who aren't me use machine learning\neffectively whether they're students or whether\nthey're professionals in some domain is it\ncan be really hard to connect these\ndecision-making algorithms for instance to\nthe human contexts in which they're\nmeant to be used. So in the human-computer\ninteraction community sometimes the term for this as you know is human-centered\nmachine learning. You and I worked\non a journal issue around this theme and I\nthink there are people in domains from the arts to medicine trying to grapple\nwith these questions about, okay, an algorithm is being\nused to do something like medical diagnosis for\ninstance but it can be very, very hard to understand why it's making certain predictions. The solutions to\nthat aren't clear. There's even some research\nsuggesting that well, if you take a\ndecision-making algorithm and you construct\nit so that it gives an explanation to the\nend users so that the user has some rationale for why that decision was made. People can be more likely to\ntrust a bad decision because they don't question the rationale of that decision in the same way. So there's a whole\nbunch of questions there about how we\nreally designed these systems from the\nbeginning to fit into human processes in\nways that give us the desired outcome\nthat don't make us either completely reliant on questionable processes or remove the benefits of\npotentially integrating that data-driven approach to decision-making into\ndifferent applications. So that's one thing that\nI think about a lot. Another thing that I think about a lot in the creative context is the ethics around\nwhere data comes from. So there are a lot of really fantastic\ninteresting art and music projects happening\nright now where if you can train a generative machine\nlearning algorithm not to do necessarily classification\nor regression but something that for instance, you could train on a set\nof thousands of songs and use that model that gets built of those songs\nto generate new songs, is that really yours? Do you own those new songs? What about all the people who composed the songs\nthat you trained on? Should you be crediting\nthem in some way? Should you be compensating them? This technology is\nreally taken off without much regard\nto how we ethically approach that and\nthe norm right now is that people just take\nwhatever data they can get and don't credit\nor compensate people and that seems\nvery problematic to me. So there's certainly discussions happening around\nboth the ethics and the possible technical\nsolutions to addressing that but\nwe're ways away from having any consensus about\nwhat that looks like. The last thing that\nI want to quickly mention is that there are some really interesting\nchallenges around the use of machine learning in human contexts that don't\neven necessarily stem from the fact that we're\nusing intelligent algorithms but rather the propensity\nfor people to either, as you mentioned totally trust are totally mistrust technology. One of the books that I\nrecommend to students often it's a book called Weapons\nof Math Destruction by Cathy O'Neil. In this book she talks about\nhow we as humans have used algorithms to support or replace human decision-making and some of the consequences, some of the really scary\nupsetting consequences that that can bring\nwhether or not those are machine learning algorithms\nare just something that a programming firm\nput together I think. I always encourage people to be skeptical about\nwhether we're using algorithms to do\nthings that might be better off done by\npeople or might be better off done\nin collaboration between people and machines. Of course, the first step to getting people to be\nskeptical about that is also getting people to realize that machine\nlearning has all of these potential pitfalls and\ncomplexities and it's not magic and it's certainly\nnot always going to be right and we have to figure\nout how to deal with that. Thank you. I mean,\nthat's absolutely right. To bring back to\nyour original point, I think we have a responsibility to try and make sure that we're getting the benefits\nof machine learning, the world gets the benefits\nof machine learning without so much of the dangers. I think the idea of\nexplainable machine learning, machine learning that can\nexplain why it's doing it. So we don't have to trust it. So we don't have to take it\nthat there's this sort of our core is really beneficial. I almost want to draw attention to Rebecca's first\npoint which is that the importance of looking\nat the human context of machine learning and the many\ndifferent kinds of people, the many different kinds\nof academic backgrounds or social backgrounds\nthat can contribute to understanding machine\nlearning and making sure machine learning is doing\nthe right thing for people and it is\nnot just something for engineers to look it as a small technical problem but\nit's something to look at the social context of everybody who interacts\nwith technology. That's why I really\nwant to thank you, the learners for\ntaking this course, for taking a course where you're maybe not coming\nin with a lot of computer science background but where you can start to take part in the conversation about\nwhat machine learning is. What it's used for and how\nit can benefit humanity.""",19,0,1
coursera,university_of_london,uol-machine-learning-for-all,introduction-collecting-your-own-dataset,"b""Now you're ready to do your own Machine\nLearning projects. Machine Learning is about using example data to train a\ncomputer to do a task. That means to do a\nMachine Learning project you need to collect data\nand train the model. You need to think\nabout what problem you want to solve in\nMachine Learning, and what data you could collect to help\nsolve that problem. You need to make sure your\ndata accurately reflects the real world and it's enough for the Algorithm\nto learn a working model. Any computing project you do is unlikely to work first time. So you'll need to test it\nand improve your model by collecting more data\nor finding the problem. Collecting data,\ntraining, and testing are all important parts of the\ncraft of Machine Learning. Something you can only learn\nby doing it in practice. So let's get started.""",20,0,1
coursera,university_of_london,uol-machine-learning-for-all,collecting-a-dataset,"b""You're now going to do your\nown Machine Learning Project. Collecting data to try to\nmodel solve a problem. The first thing you need to do is decide what problem\nyou're going to tackle. A Machine Learning model\nmaps inputs to outputs. In this project the\ninputs will be images, but they can be of\nanything you choose. We've been using\nclassification which means the outputs are one of a\nsmall set of categories. We can use these categories\nto do other things like playing sounds\nor showing images. For example playing meow\nwhen we recognize a cat. But the model itself is just\nputting inputs into classes. So you need to decide\nwhat the classes are and which examples\nfit in each class. You also need to make\nsure that there's enough information in the inputs to work out their classes. It can't be based on any contextual information which a human might know but it's\nnot in the picture itself. One important aspect of this for most Machine Learning is\nthe features you're using. If the features do not contain the information needed\nto perform the task, the learning algorithms\nwill not work. For this project you don't have much control\nover the features. You're using images, but bear in mind that some features are\nhard to detect in images, like the size of an object. Just because something is photographed close\nup and takes up a large part of the\nimage doesn't mean the object itself is big. Once you've defined\nyour problem you need to gather your datasets. This means finding\nimages in each class. You need a good number of\nimages for each class. Modern machine learning\nproblems can require thousands or even millions\nof examples for each class. But because we're using a really good feature extractor that has already been trained\non millions of images, 30-40 examples per class\nis probably enough. More importantly, your examples should be\nas varied as possible. They should represent\nthe whole range of the types of inputs you\nwould get in the real world. In particular, you should\nmake sure there aren't any extraneous elements that are associated with the class. Like all items in the\nclass being taken on the same background or in the\nsame lighting conditions. Once you've trained your model, you also need to test it to\nmake sure that it works. This means you should\nalso have a test dataset. The test dataset\nshould also be as close as possible for the data you expect\nin the real world. The best way to create a test dataset is to gather\nslightly larger set for training and then randomly choose some of your examples\nto be your test data. Make sure you don't use the same examples in\nyour train and test set, otherwise it's not a fair test. Once you have your\ndata you can use it to train the model using our\nMachine Learning plugin. You can then try out the model on the test set and see how\nmany examples it gets right. If it works okay, that's good. But if it doesn't, you need to think about\nhow to improve it. Look carefully at\nthe examples that have been classified incorrectly. Try to imagine what\ntraining examples would help your algorithm\nclassify them correctly. When you think it's working, gather more test data and\nsee if it works on that. Getting Machine Learning to\nwork can be a lot of effort. But if you spend enough time testing and gathering new data, you should be able\nto get a good model. Sometimes however,\nit will be simply impossible to get it working\nfor the data you've chosen. In that case you might need\nto rethink your problem. Sometimes you can come up with a set of classes that are just as useful in practice but\nmuch easier to recognize. Maybe you'll need\nyour images to be taken under controlled\nlighting conditions, or you need to exclude certain difficult examples\non the border line. Now is your chance to do\nsome real Machine Learning, to be involved in one of the most cutting edge\nareas of computer science. Go out, collect your data, and get training. Good luck.""",21,0,1
coursera,university_of_london,uol-machine-learning-for-all,interview-advice-for-your-first-machine-learning-project,"b'As you\'re starting to\nwork on your projects, I\'m sure you\'re looking for some advice and some help\nabout how to go about it. So I thought I\'d ask our panel of experts\nwho\'ve all had many, many years of experience\nworking machine learning, also teaching machine learning about if they have\nany advice for you. So should we start\nwith you, Larisa? Our students are beginning\ntheir first machine learning project creating\nsome image classifiers. What would your\nadvice be to them? My advice would be\nget your data first. You cannot do machine learning\nif you don\'t have data. If you think you have data, maybe still not good enough data. So you need enough, you need of suitable quality. So do you have it in place? You just cannot be sure that\nyour project will succeed, and especially if you use some data set\navailable somewhere, just make sure you\ndownload it, you check it, you fully understand\nwhat is inside, how it was gathered. So make sure there are no\nbiases there or minimize it. Focus first of all on your\ndata then you can go further. By the way, it\'s quite well-known that data gets\naround data processes, and will take about 80 percent of your time of all projects. So this is the key. Jamie, does that? Yeah, absolutely all of that, 85 percent, 90 percent even. You spent a lot of\ntime cleaning data. Image is one thing certainly, but time series stuff it\'s like the cleaning of the data is\njust absolutely everything, it takes use so much\ntime and effort. I guess what would\ntie with that is if you are creating features, plot them, visualize them, have an indication\nthat you can visually see what your data looks like because that really\nmassively helps things, and particularly if you\'re\ndoing classification, you should be able to\nplot your features in such a way where you can\nclearly see a deference. Because if you can\nsee a difference, there\'s a good chance the\nalgorithm should be able to distinguish the\ndifferent classes. But definitely plot\neverything beforehand. Clean it, plot it. Be sure\nof what you\'re doing, and have an expectation of\nwhat you will at the end. Try and have an idea of what you expect to get so that\nwhen you get the result, you\'ve got something\nto compare against. From working for you\nfor quite awhile, I\'ve realized that you really\nput a lot of importance on understanding the data before\nyou do anything with it. Yeah. We can be blind\nabout it and throw it into a bag deep learning\nalgorithm and hope for the best, but it\'s quite unsatisfactory. You want to understand\nsomething, I think sometimes. I would add to that\nthe data is key. One of the good rules of thumb to be thinking about\nis if you anticipate some kind of variation\nthat you want to be able to handle with\nyour classifier in the real-world\nefforts being trained, make sure that variation is\nthere in your training data. Seems obvious but often times\npeople forget about this. But also, I think\ntaking what you know about evaluation metrics and really stepping back\nand thinking about, ""Okay, does\ncross-validation accuracy match up with what I really\nwant out of this classifier?"" for example. Sometimes\nthat is true. Sometimes the accuracy\nof every class is as important as the\naccuracy of every other class, and you have about the same numbers of each\nclasses in your training data, in which case, great,\nthat\'s pretty easy. But if you said, you know what, it\'s really important to me that these particular classes never get confused with each other. But if data like this\nis an edge case, and as a person I might have a hard time really assessing what class\nthey should go in, I don\'t care so much about what the accuracy\nlooks like on those. That can be good to\ntake into account when you\'re really doing that\nevaluation in the end, whether it\'s cross-validation on your training data or looking at accuracy and how it\'s broken\ndown in the test data. Because in the real-world, often we have lots of\ndifferent priorities, and it\'s not just about getting every type of example as\naccurate as everything else. There might be all sorts of other complicated stuff going on. The other thing that\nI would stress, which I talk to my\nstudents about a lot is that no matter how much you know about machine\nlearning algorithms, no matter how much you\nknow about your data set, no matter how much you\nknow about your features, if you have a PhD in\nmachine learning, you still may not have the\nintuition to know exactly how to build that\nmachine learning system right the first time. So experimentation is key. Learning about the\ndata, not just once, but trying different things\nand continuing to try to inspect what\'s happening at all stages of the machine\nlearning pipeline, it\'s a core part of applied\nmachine learning practice. As I mentioned, even\npeople with PhDs in machine learning do this, and they get really\ngood at doing it. So don\'t worry if\nyou don\'t know what exactly to do because many other people\nmight not know either. Exposing the world often\ndon\'t know what they do. Can I just add to\nthat? If you see that you get like 99 percent\naccuracy or 99.9 percent accuracy and\nis looking really good, there\'s a\nproblem somewhere. Very, very rarely as a result. Good advice. I\'ve heard that from those time. It feels good then you\ngot to be skeptical. Maybe we can look\nat the other side. Obviously never happens to us, but what happens when\nyou gather the data, you train the algorithm and it doesn\'t do what\nyou want to do, you don\'t get any accuracy? Any advice for our learners\nin that situation? Alcohol. You should try\ndifferent algorithms. You should try analyze\nyour pipelines. So maybe you\'re\noverlooking something. Just imagine the situation\nif humans are trying to do if it doesn\'t work at all. So maybe there are problems, what you\'re trying to do\njust really will not work. So you need to rethink the\nwhole design. It does happen. But it\'s better to plan\nfrom the beginning. So what you are doing, try to estimate is it\nlikely to work or not. Even if 99 percent accuracy, you still need to compare it with performance of\nother algorithm. If it\'s completely random, then yes, something is\nwrong at some stage. So try to find it out, see what you can change. Jamie, does this\ncombat your obsession about understanding\nthe data as well? Absolutely. If for example when you were doing\nyour exploration of your data and you can see\nsome clear differences visually in representation\nof your features, in visualization in\nyour features and your algorithm was not\nable to pick them up, I think there\'s still hope, you can go back\nfind new features, find new ways of representing it. Maybe you\'re making\nsome mistakes. Do some iterative applications\nof different algorithms and different features\nbecause there\'s still hope. But if you could never see any difference from the outset, then perhaps the problem isn\'t\nsolvable using that data. Go back and rethink\nthe whole problem perhaps, record again. Path to success is\nlittered with failures. It\'s just one of those things. I\'ve had a couple of times\nwhere you know there is an original difference but when you actually look at\nthe features you are using, you can\'t see that\ndifference, it disappears. So I think there\'s issues of understanding the feature representation,\nunderstanding the data, and maybe there\'s just something wrong in that whole pipeline, the features, the\nalgorithm can\'t spot it. Any other advice? Advice that I give to my\nstudent who are doing project is try to make a clear plan. It sounds boring but it\'s so important that you\'re quite clear where you are in your plan. When you actually have to\nfinish cleaning your data and start experimenting and\nif you\'re not there yet, then think what can you speed up. Because if you don\'t\nmanage your plan, there is huge, huge danger if you will\nnot finish on time. So most of projects fail or were not as successful as they could be because\nthey run out of time. Know when to stop\nbecause if you\'re at 97 percent accuracy and it looks like it\'s\naccurate and genuine, you don\'t need to keep\nplugging away, you\'re okay. Look at the context\nof the problem. Machine learning\nis never perfect. It\'s never perfect. Well, thanks lots to all of you. Thanks a lot for that\nfantastic advice, and I\'m sure our learners\nwill really appreciate. I\'m sure you\'ll join me in\nsaying a big good luck to our learners both in\ntheir current projects, and hopefully their future lies working with\nmachine learning.'",22,0,1
coursera,university_of_london,uol-machine-learning-for-all,summary,"b""Congratulations. You've completed your first\nMachine Learning project. You've gathered the dataset, trained your model,\nand tested it. You've learned a bit more about the amazing things that\nMachine Learning can do, and you've probably\nlearned about the ways Machine Learning could go\nwrong and what makes it hard. You've had your first\nstarts in one of the most exciting emerging\nareas of computer science, one that's likely to shape the future of\ncomputing technology. What you do next is up to you. Maybe, you simply see\nMachine Learning as part of your broader career\nin computer science. You might not need to go into so much depth in\nMachine Learning but collecting data and\ntraining models is still likely be part\nof what you do, or maybe your experience with Machine Learning or\nsimply help you better understand the various\ncomputer systems that you interact with and use\nMachine Learning models, everything from shopping\nrecommendations to medical diagnosis. In any case, you've taken your first steps into the\nfuture of computer science.""",23,0,1
